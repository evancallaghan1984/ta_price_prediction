{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce9d5af-0fda-41db-a2fc-22a5d7e0f586",
   "metadata": {},
   "source": [
    "# Capstone Project\n",
    "* Student Name: Evan Callaghan\n",
    "* Student Pace: Part Time\n",
    "* Instructor Name: Mark Barbour\n",
    "# Forecasting Future Price Using Technical Analysis, Price Action, and Supervised Machine Learning\n",
    "Warren Buffet is one of, if not the, greatest investor of all time. Anyone who has purchased, withheld from, or attempted to day-trade a particular stock with the undeniable self-certainty of its' impending movement knows how devastatingly unforgiving and unpredictable market forces can be. Studies show that [more than 97% of day traders are unsuccessful and lose money over time](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3423101), with just 1.1% of traders earning more than minimum wage and only 0.5% of traders exceeding the salary of entry level job. Even highly intelligent fund managers equiped with a wealth of valuable data and resources struggle to keep up with the returns of the S&P 500. In fact, only [15% of funds in the IA North American sector were able to beat it over the last 10 years.](https://portfolio-adviser.com/the-15-of-us-funds-that-beat-the-sp-500-over-the-past-decade/) \n",
    "\n",
    "In light of the statistically large number of failed traders and underperformers, it lends even more credit to the exceptional success of Buffet's career. How is it that he able to consistantly and reliably outsmart the [unfathomable randomness of Brownian Motion](https://en.wikipedia.org/wiki/Geometric_Brownian_motion?utm_source=chatgpt.com) and predict the direction of his stocks with enough accuracy as to outperform the vast majority of the investment world year over year. If not for the comprehensive investment philosphy and thoroughly described research methods outlined in the letters he sends to shareholderes of Berkshire Hathaway every year, his ability to predict the future would almost seem supernatural. Barring undisclosed superpowers, we can safely assume Buffet has simply built his own highly effective price prediction model using a carefully selected basket of features encompassing historical price action, technical analysis, financial metrics, market sentiment, economic data, human psychology, and more. \n",
    "\n",
    "While we may not possess as much knowledge or experience as the worlds greatest investor, we can begin constructing a model to predict future price by experimenting with features utilized by accomplished long term investors, profitable traders in both intra-day and swing trading, and data-driven quantitative analysts. Technical Analysis (TA) indicators, which are lauded by day traders and can often be found integrated into the trading strategies of a variety investment professionals, are statistical tools used to analyze market trends, aid in decision making, and forecast probabilistic movements. Simple Moving Averages (SMA), Volume-Weighted Average Price (VWAP), and Relative Strength Index (RSI) are examples of commonly used TA indicators which are calculated using price or volume or both. These tools are 'lagging indicators', meaning they rely on historical data to generate signals and are reflective of past price movements and trends, rather than predicting future price action. We use them to find levels of support and resistance, historically repeated patterns, and confirmation signals to enhance our other trades. While the efficacy of TA indicators is up for debate, with some academics seeing them as [\"voodoo finance\"](https://www.brunel.ac.uk/economics-and-finance/research/pdf/2301-Jan-GMC-Technical-Analysis.pdf) and others saying [the profitability of technical analysis is illusory](https://www.sciencedirect.com/science/article/abs/pii/S0169207013000964), a few studies have shown significant positive returns for common technical analysis trading rules like [this study on the chinese stock market](https://web.archive.org/web/20081204112239/http://findarticles.com/p/articles/mi_qa5466/is_200704/ai_n21292807/pg_1?tag=artBody;col1). Their predictive powers may be up for debate, but what is not up for debate is how many traders/swing traders use them as part of their strategy. All in all, testing out the potential forecasting strength of TA indicators is well worth the research. \n",
    "\n",
    "In order to begin the process of building a system of algorithmic trading strategies using machine learning, we will calculate commonly used TA indicators and use them to predict stock price 1 week (5 trading days) in the future. This will give us a great baseline to see how useful analyzing TA indicators are to future price prediction, and which TA indicators are most correlated with future price. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b635c3-9842-408e-9f61-0c982d9c49a2",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "The target of our predictive model is daily closing price('Close'), which will be pulled from the 'yfinance' python library, along with daily open price('Open'), daily high price ('Daily_High'), daily low price ('Daily_Low'), and daily volume ('Volume'). In order to give the model enough information to find patterns, we will use about 10 years of trading data from 2015-01-01 to 2025-03-05. \n",
    "\n",
    "Before pulling the data, we use a CSV file downloaded from nasdaq.com which contains a list of the top 5000 stocks by market cap. Because of the limited computational power we have, we will only be using the top 25 stocks by marketcap. Once we import that CSV file, we will filter out all stocks which aren't common shares and remove any ticker names that are not available on Yahoo Finance. Upon doing this, we can pull in the 10 years of data from Yahoo Finance using our top 25 stock tickers by market cap and organize them into a dataframe for data manipulation. \n",
    "\n",
    "Once we have our top 25 stocks in a dataframe, each containing 10 years of daily price data, we'll begin calculating our TA indicators using the daily closing price. Below is a list of the TA indicators we will be calculating and including in our model:\n",
    "* Simple Moving Average (SMA)\n",
    "  * An average of past prices over a set period of time, smoothing out price fluctuations.\n",
    "* Exponential Moving Average (EMA)\n",
    "  * A weighted moving average that gives more importance to recent prices for faster trend detection.\n",
    "* Relative Strengh Index (RSI)\n",
    "  * A momentum oscillator that measures the speed and change of price movements to identify overbought and oversold conditions. \n",
    "* Moving Average Convergence Divergence (MACD)\n",
    "  * A trend-following indicator that tracks the relationship between two moving averages to signal potential buy/sell opportunities.\n",
    "* Stochastic Oscillator\n",
    "  * A momentum indicator comparing a securitiy's closing price to its' price range over a given period to gauge trend strength. \n",
    "* Volume-Weighted Average Price (VWAP)\n",
    "  * A benchmark indicator that calculates the average price of a security based on both price and volume.\n",
    "* Bollinger Bands\n",
    "  * A volatility indicator that consists of a moving average with upper and lower bands expanding or contracting based on market volatility. \n",
    "* Average True Range (ATR)\n",
    "  * A measure of market volatility that tracks the average range between high and low prices over a set period. \n",
    "* On-Balance Volume (OBV)\n",
    "  * A volume-based momentum indicator that adds or subtracts volume based on price movements to assess buying and selling pressure.\n",
    "* Fibonacci Levels\n",
    "  * Key price levels derived from the Fibonacci sequence, used to identify potential support and resistance zones. \n",
    "* Momentum\n",
    "  * A trend indicator that measures the speed of price movement to identify the strength of a trend.\n",
    "* Quantiles\n",
    "  * Statistical thresholds that divide the dataset into equal parts including the median, upper quantile, and lower quantile.\n",
    " \n",
    "On top of this, moving averages will be calculated for all of these indicators and the daily price action that we pulled from yfinance (daily open price, daily high price, daily low price, and daily volume) with the exception of our target varaible (daily closing price). Each of these will get a 3day, 5day, and 7day moving average that will be used as an individual feature. In addition to the moving averages, the top 35 features most important to predicting the target variable will be getting 'lags', which basically give the model access to recent data. Since the model will only be looking at one row at a time, we need lags in order to give the model access to what the feature values were yesterday, the day before that, etc. The top 35 most important features will receive the following lags: 1, 2, 3, 4, and 5. Once these lags are calculated, we will then find the most important features and filter by the top 60. After this, we will complete our preprocessed data by including 1, 2, 3, 4, and 5-day lags for our target variable (daily closing price). We need to do this in order to give our model access to what the previous daily closing prices were in order to enhance our predictive capability. On top of this, we made sure to only include data that would be available to a person trading with the same information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3522a0-72d2-46a9-89cf-e8d818ea580c",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088ed28-0f49-4f28-aedb-b9540a919e44",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "All python libraries used throughout the project will be imported here, in order to prevent redundancy and maintain cleaner, more organized code. This ensures are dependencies are loaded at the start, making it easier to manage imports and avoid potential issues with mising modules in different sections of the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a01d735-f842-4107-ad8c-7a9fa0b20c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import warnings # Import warnings for Warning Supression \n",
    "from pandas.errors import PerformanceWarning # Handle Pandas performance Warning\n",
    "import numpy as np # Numpy for numerical operations \n",
    "import pandas as pd # Data manipulation and analysis\n",
    "import yfinance as yf # Fetch financial data from Yahoo Finance\n",
    "import xgboost as xgb # The ML algorithm we are using to model\n",
    "\n",
    "# Model evaluation metrics \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson # Autocorrelation test\n",
    "import matplotlib.pyplot as plt # Data Visualization\n",
    "import seaborn as sns # Statistical data visualization\n",
    "from datetime import datetime # Date and Time calculations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b49cbc-7b80-4c4e-a602-818ec4b237d3",
   "metadata": {},
   "source": [
    "### Warning Supression\n",
    "In some cases, python libraries may generate warnings that do not effect the correctness of the code, but clutter the output. To maintain clean and readable code, we suppress these unnecessary warnings, while leaving actual critical warnings the ability to appear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c83b9-37ed-4b1b-89ab-35bc8e4a9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Warning Supression to ignore errors related to updating software\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ignore PerformanceWarning to avoid inefficiency error \n",
    "warnings.filterwarnings('ignore', category=PerformanceWarning)\n",
    "\n",
    "# Suppress warnings from yfinance\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='yfinance')\n",
    "\n",
    "# Suppress specific warnings from yfinance\n",
    "warnings.filterwarnings(\"ignore\", message=\".*no price data found.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8eb97-ad7d-4e4d-a570-85cde91272c3",
   "metadata": {},
   "source": [
    "### Load Stock CSV file into DataFrame\n",
    "Load the CSV file from nasdaq.com into a DataFrame, only keep the necessary columns 'Symbol', 'Name', and 'Market Cap', remove the '$' and ',' from the 'Market Cap' column so that we can sort it in descending order, filter to only import the top 100 stocks by market cap, and reset index for readability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e2975-7d47-4313-93a9-26bcc74988d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = 'https://raw.githubusercontent.com/evancallaghan1984/ta_price_prediction/refs/heads/main/data/nasdaq_screener_stock_list.csv' \n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Filter DataFrame to only show the columns 'Symbol', 'Name', and 'Market Cap'\n",
    "df = df[['Symbol', 'Name', 'Market Cap']]\n",
    "\n",
    "# Convert 'Market Cap' to numeric if it's not already\n",
    "# Remove commas, dollar signs, and replace these symbols with empty spaces\n",
    "df['Market Cap'] = df['Market Cap'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "\n",
    "# Sort the DataFrame by Market Cap in descending order\n",
    "df_sorted = df.sort_values(by='Market Cap', ascending=False).head(100)                                                                        \n",
    "\n",
    "# Reset the index of the DataFrame and drop the old index\n",
    "df_sorted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update the index to start from 1 instead of 0\n",
    "df_sorted.index = df_sorted.index + 1\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df_sorted.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7033a6-ca08-4220-b381-443c378a80fd",
   "metadata": {},
   "source": [
    "### Remove All Stocks Except Common Shares\n",
    "For our model, we remove all stocks except common shares to ensure the dataset focuses solely on publicly traded equity. Other stock classifications, such as global notes and capital stock, may represent different ownership structures or financial instruments that do not align with the analysis of common stock performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fe5b5-1215-423f-9acf-fbcbae90e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no leading or trailing whitespaces in the 'Name' column\n",
    "df_sorted['Name'] = df_sorted['Name'].str.strip()\n",
    "\n",
    "# List of terms to filter out\n",
    "terms_to_drop = [\"Capital Stock\", \"Depository Shares\", \"Global Notes\", \"ADS\", \n",
    "                 \"Registry Shares\", \"Depositary Shares\"\n",
    "]\n",
    "\n",
    "# Create a regex pattern to match any of the terms\n",
    "# //b ensures that the match occues only at the start or end of a word\n",
    "# pipe '|' ensures that if any of the terms in 'terms_to_drop' are seen, \n",
    "# there is a match\n",
    "pattern = '|'.join([f\"\\\\b{term}\\\\b\" for term in terms_to_drop])\n",
    "\n",
    "# Apply filtering based on the updated pattern\n",
    "df_filtered = df_sorted[~df_sorted['Name'].str.contains(pattern, case=False, \n",
    "                                                        na=False)\n",
    "]\n",
    "# Reset the index of the DataFrame and drop the old index\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update the index to start from 1 instead of 0\n",
    "df_filtered.index = df_filtered.index + 1\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b4a72-0d9a-4914-a86a-0686589c1474",
   "metadata": {},
   "source": [
    "### Remove Invalid Tickers\n",
    "Some stock tickers in the list from nasdaq.com may be not available on Yahoo Finance for a variety of reasons, so to avoid errors in the future, we will cross-examine the list of stocks from nasdaq.com with the stocks available on Yahoo Finance. If the stocks are not available on Yahoo Finance, they will will be removed from our stock ticker list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b196f-e2c2-425b-a8ff-4d083ede941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stock tickers not found on yahoo finance\n",
    "# Convert df_filtered['Symbol'] to a list\n",
    "stock_list = df_filtered['Symbol'].head(50).astype(str).tolist()\n",
    "\n",
    "# Function to check if a stock ticker is valid\n",
    "def is_valid_ticker(ticker):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        hist = stock.history(period=\"1d\")  # Fetch 1 day of historical data\n",
    "        return not hist.empty  # Valid if data is not empty\n",
    "    except Exception as e:\n",
    "        return False  # If there's an error, consider it invalid\n",
    "\n",
    "\n",
    "# Filter out invalid tickers\n",
    "valid_tickers = [ticker for ticker in stock_list if is_valid_ticker(ticker)]\n",
    "\n",
    "# Keep only valid tickers in df_filtered\n",
    "df_filtered = df_filtered[df_filtered['Symbol'].isin(valid_tickers)]\n",
    "\n",
    "# Reset the index of the DataFrame and drop the old index\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"All invalid tickers have been successfully removed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f9fb2-83b9-43af-8fcc-dc2a8bc2dd65",
   "metadata": {},
   "source": [
    "### Download Historical Data from Yahoo Finance\n",
    "Download historical price data of the top 25 stocks in our nasdaq.com list from 2015-01-01 to 2025-03-05. Get the daily close, daily open, daily high, daily low, and volume. Use 'auto_adjust=True' to make sure that the stock prices are adjusted for stock splits and dividends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd2a1f3-01d5-4061-ae0e-3e31563c03cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import 10 years of daily historial price data from Yahoo Finance\n",
    "\n",
    "# Convert df_filtered['Symbol'] to a list of top 25 stocks by marketcap\n",
    "stock_list = df_filtered['Symbol'].head(25).astype(str).tolist() \n",
    "\n",
    "# Download data for all stocks\n",
    "data = yf.download(stock_list, start=\"2015-01-01\", end=\"2025-03-05\", auto_adjust=True, actions=False)[['Close', 'Open', 'High', 'Low', 'Volume']]\n",
    "\n",
    "# Reshape the data: Convert tickers from columns to rows\n",
    "daily_stock_price = data.stack().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "daily_stock_price.columns = ['Date', 'Symbol', 'Close', 'Open', 'Daily_High', 'Daily_Low', 'Volume']\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "daily_stock_price.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d647228-c1e2-4a86-9bb4-36ec7dc34479",
   "metadata": {},
   "source": [
    "### Re-order the DataFrame for proper Time Series Analysis\n",
    "To make sure our data is set up chronologically correct for modeling and grouped by stock ticker, we need to sort our historical data by the stock ticker, then make sure to have the ticker repeat on every single line for the duration of it's price history. This way when training the data, the model will know that when the stock ticker changes, that means the date column will restart and a new modeling framework should take place. In addition to this, make sure that the 'Date' column is in datetime format, so that we can filter by date when modeling and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca506bfb-eb1c-41c5-ab57-d489a41020ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now group by 'Symbol' and reset the index for the entire dataframe\n",
    "daily_stock_price = daily_stock_price.groupby('Symbol').apply(lambda x: x.reset_index(drop=True))\n",
    "\n",
    "# Fill down the 'Symbol' column (so it repeats for each stock's data)\n",
    "daily_stock_price['Symbol'] = daily_stock_price['Symbol'].fillna(method='ffill')\n",
    "\n",
    "# Reset the index again and drop the existing 'Symbol' index\n",
    "daily_stock_price = daily_stock_price.reset_index(drop=True)\n",
    "\n",
    "# Make sure Date column is in datetime datatype format\n",
    "daily_stock_price['Date'] = pd.to_datetime(daily_stock_price['Date'], errors='coerce')\n",
    "\n",
    "# Display the first few rows\n",
    "daily_stock_price.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b27d50-211b-4b2a-9ac2-0acb817a7441",
   "metadata": {},
   "source": [
    "### Initialize Window, Calculate TA indicators, and Create Moving Averages\n",
    "Now we initialize a window for our moving average calculations, calculate all of our TA indicators using our daily price data, and create moving averages for daily price data as well as TA indicators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2082ea6-ef78-42ad-b0d6-e2ad214b4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy  of the daily_stock_price DataFrame and rename it \n",
    "stock_data_1_week = daily_stock_price.copy()\n",
    "\n",
    "# Define list of window sizes for calculating moving averages\n",
    "one_week_window = [3, 5, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec794f6-5505-48dd-9378-3d4864cd3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create moving averages for Daily Volume, Daily High Price,\n",
    "# Daily Low Price, and Daily Open Price\n",
    "# Group by stock symbol and apply a rolling window of all sizes in one_week_window\n",
    "# For each daily stock metric in each stock symbol\n",
    "# Shift by 1 day to prevent future data leakage\n",
    "\n",
    "# Daily Volume MA\n",
    "for col in stock_data_1_week.columns: \n",
    "    for window in one_week_window: \n",
    "          \n",
    "        stock_data_1_week[f'Volume_{window}day_avg'] = stock_data_1_week.groupby(\n",
    "            #\n",
    "            'Symbol')['Volume'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "\n",
    "# Daily High Price MA\n",
    "for col in stock_data_1_week.columns: \n",
    "    for window in one_week_window: \n",
    "        \n",
    "        stock_data_1_week[f'Daily_High_{window}day_avg'] = stock_data_1_week.groupby(\n",
    "        \n",
    "            'Symbol')['Daily_High'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "        )   \n",
    "\n",
    "# Daily Low Price MA\n",
    "for col in stock_data_1_week.columns:\n",
    "    for window in one_week_window:\n",
    "        \n",
    "        stock_data_1_week[f'Daily_Low_{window}day_avg'] = stock_data_1_week.groupby(\n",
    "             # Apply a rolling mean to the Daily Low of each stock symbol.\n",
    "            'Symbol')['Daily_Low'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "        )   \n",
    "\n",
    "# Daily Open Price MA\n",
    "for col in stock_data_1_week.columns: \n",
    "    for window in one_week_window: \n",
    "        \n",
    "        stock_data_1_week[f'Open_{window}day_avg'] = stock_data_1_week.groupby(\n",
    "            \n",
    "            'Symbol')['Open'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ec939-45ec-42c5-960f-5abfcf5bc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Moving Average (EMA)\n",
    "# Calculate EMA for window sizes in one_week_window\n",
    "# Apply an Exponential Weighted Moving Average to the \n",
    "# 'Close' price while grouping by 'Symbol' \n",
    "# Store result in a new column for each window size\n",
    "\n",
    "for col in stock_data_1_week.columns:\n",
    "    for window in one_week_window:\n",
    "        stock_data_1_week[f'EMA_{window}'] = stock_data_1_week.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.shift(1).ewm(span=window, adjust=False).mean()\n",
    "        )\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909fe0e-16fa-46c0-b6ca-39b48978bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Moving Average (SMA)\n",
    "# Calculate SMA for window sizes in one_week_window\n",
    "# Apply a rolling window to the 'Close' price while grouping by 'Symbol'. \n",
    "# Store result in a new column for each window\n",
    "\n",
    "for col in stock_data_1_week.columns:\n",
    "    for window in one_week_window:\n",
    "        stock_data_1_week[f'SMA_{window}'] = stock_data_1_week.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0301330-a6b3-4916-bdf1-4f7c376f34a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RSI\n",
    "\n",
    "# Define a function to calculate RSI\n",
    "def calculate_rsi(df, window):\n",
    "    \n",
    "    # Calculate difference in 'Close' price from the previous day\n",
    "    delta = df['Close'].diff()\n",
    "\n",
    "    # Separate gains and losses\n",
    "    gain = delta.where(delta > 0, 0) # Keep gains, replace losses with 0\n",
    "    loss = -delta.where(delta < 0, 0) # Keep losses as positive values, replace gains with 0\n",
    "\n",
    "    # Calculate the rolling average of gains and losses over a given window\n",
    "    avg_gain = gain.rolling(window=window).mean()\n",
    "    avg_loss = loss.rolling(window=window).mean()\n",
    "\n",
    "    # Calculate Relative Strength (RS)\n",
    "    # RS = Ratio of avg gains to avg losses\n",
    "    rs = avg_gain / avg_loss\n",
    "\n",
    "    # Calculate Relative Strength Index (RSI)\n",
    "    # RSI = 100 - (100 / (1 + RS))\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Add RSI value as new column in our dataframe\n",
    "    df['RSI'] = rsi\n",
    "\n",
    "    return rsi\n",
    "\n",
    "\n",
    "# Most common RSI calculation is using a 14day MA\n",
    "# Add to DataFrame\n",
    "stock_data_1_week['RSI'] = calculate_rsi(stock_data_1_week, 14).shift(1)\n",
    "\n",
    "# Iterate over the columns in our stock_data_1_week DataFrame\n",
    "# Iterate over window sizes in one_week_window\n",
    "# Group by 'Symbol' and apply the calculate_rsi function to our DataFrame\n",
    "# Store RSI value in new column for each window size\n",
    "# Shift by 1 to prevent future leakage\n",
    "for col in stock_data_1_week.columns:\n",
    "    for window in one_week_window:\n",
    "        stock_data_1_week[f'RSI_{window}'] = stock_data_1_week.groupby('Symbol', group_keys=False).apply(\n",
    "        lambda x: calculate_rsi(x, window=window).shift(1)\n",
    "        ).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9627f-e119-4714-93e1-a5c0c073f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to calculate the MACD (Moving Average Convergence Divergence) \n",
    "# Compute 12day and 26day Exponential Moving Averages(EMAs)\n",
    "# Compute MACD line as difference between 12day and 26day EMA\n",
    "# Compute the Signal Line as the 9day EMA of the MACD\n",
    "# Drop unneeded 12day EMA and 26day EMA\n",
    "\n",
    "\n",
    "def calculate_macd(df):\n",
    "    \n",
    "    df['EMA_12_MACD'] = df.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.ewm(span=12, adjust=False).mean()\n",
    "    )\n",
    "    df['EMA_26_MACD'] = df.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.ewm(span=26, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    df['MACD'] = df['EMA_12_MACD'] - df['EMA_26_MACD']\n",
    "    df['Signal_Line'] = df.groupby('Symbol')['MACD'].transform(\n",
    "        lambda x: x.ewm(span=9, adjust=False).mean()\n",
    "    )\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['Signal_Line']\n",
    "\n",
    "    df.drop(columns = ['EMA_12_MACD', 'EMA_26_MACD'], inplace=True)\n",
    "\n",
    "# Create funcion that calculates rolling averages of the MACD indicator for a \n",
    "# given window size \n",
    "# Apply a rolling mean to the MACD, Signal Line, and MACD Histogram\n",
    "# Create a new column for each rolling window size\n",
    "# Shift values by 1 to prevent future data leakage\n",
    "  \n",
    "\n",
    "def calculate_rolling_macd(df, window):\n",
    "    # Apply rolling average to the MACD, Signal Line, and MACD Histogram with a shift to avoid future data leakage\n",
    "    df[f'MACD_rolling_{window}'] = df.groupby('Symbol')['MACD'].transform(\n",
    "        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    df[f'Signal_rolling_{window}'] = df.groupby('Symbol')['Signal_Line'].transform(\n",
    "        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    df[f'MACD_Histogram_rolling_{window}'] = df.groupby('Symbol')['MACD_Histogram'].transform(\n",
    "        lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "  \n",
    "# Compute MACD and Signal Line for each stock symbol\n",
    "calculate_macd(stock_data_1_week)\n",
    "\n",
    "# Apply calculate_rolling_macd() function to each window size in one_week_window\n",
    "for col in stock_data_1_week.columns:\n",
    "    for window in one_week_window:\n",
    "        calculate_rolling_macd(stock_data_1_week, window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7cfae-162e-4e84-86f1-f42434127568",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_1_week.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe0cbc-984c-4585-b08d-ec460754adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to calculate Stochastic Oscillator\n",
    "# Compute lowest low and highest over over a given window\n",
    "# Compute %K as the relative position of the 'Close' price within the high range\n",
    "# %D is the 3day moving average of %K\n",
    "# Drop lowest low and highest high columns\n",
    "\n",
    "def calculate_stoch_oscillator(df, windows):\n",
    "        \n",
    "    df['Stoch_Lowest_Low'] = df.groupby('Symbol')['Daily_Low'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "    )\n",
    "    \n",
    "    df['Stoch_Highest_High'] = df.groupby('Symbol')['Daily_High'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "    )\n",
    "    \n",
    "    df[f'%K_{window}'] = ((df['Close'] - df['Stoch_Lowest_Low']) / (\n",
    "        df['Stoch_Highest_High'] - df['Stoch_Lowest_Low'])) * 100\n",
    "\n",
    "    df[f'%D_{window}'] = df.groupby('Symbol')[f'%K_{window}'].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\n",
    "    )\n",
    "\n",
    "    df.drop(columns=['Stoch_Lowest_Low', 'Stoch_Highest_High'], inplace=True)\n",
    "\n",
    "# Create function to calculate the 14day standard stochastic oscillator Indicator\n",
    "# Use a window of 14 to get %K\n",
    "# Get 3day average of %K to calculate %D\n",
    "# Drop intermediate columns\n",
    "\n",
    "def standard_stoch_oscillator(df):\n",
    "    df['Stoch_Lowest_Low'] = df.groupby('Symbol')['Daily_Low'].transform(\n",
    "        lambda x: x.rolling(window=14, min_periods=1).min()\n",
    "    )\n",
    "    \n",
    "    df['Stoch_Highest_High'] = df.groupby('Symbol')['Daily_High'].transform(\n",
    "        lambda x: x.rolling(window=14, min_periods=1).max()\n",
    "    )\n",
    "    \n",
    "    df['%K'] = ((df['Close'] - df['Stoch_Lowest_Low']) / (df['Stoch_Highest_High'] - df['Stoch_Lowest_Low'])) * 100\n",
    "\n",
    "    df['%D'] = df.groupby('Symbol')['%K'].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "    df.drop(columns=['Stoch_Lowest_Low', 'Stoch_Highest_High'], inplace=True)\n",
    "  \n",
    "\n",
    "# Compute standard 14day Stachastic Oscillator for each stock symbol\n",
    "standard_stoch_oscillator(stock_data_1_week)\n",
    "\n",
    "# Apply calculate_stoch_oscillator function to each window size\n",
    "# in one week window and using stock_data_1_week DataFrame\n",
    "for col in stock_data_1_week.columns:  \n",
    "    for window in one_week_window:\n",
    "        calculate_stoch_oscillator(stock_data_1_week, window)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2efea-5d80-47cf-bca5-f389b95b7f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create function to calculate Volume Weighted Average Price (VWAP) per stock symbol\n",
    "# Calculate Cumulative Price Volume and Cumulative Volume\n",
    "# Compute VWAP as the ratio of the cumulative price-volume to cumulative volume\n",
    "\n",
    "# Calculate Volume Weighted Average Price (VWAP) per symbol\n",
    "def calculate_vwap(df):\n",
    "    # Ensure 'Close' and 'Volume' are numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "    df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')\n",
    "\n",
    "    # Calculate cumulative price-volume product for VWAP\n",
    "    # Represents total value of traded stocks\n",
    "    df['Cumulative_Price_Volume'] = df.groupby('Symbol')['Close'].transform(\n",
    "    lambda x: (x * df.loc[x.index, 'Volume']).cumsum()\n",
    "    )\n",
    "    # Calculate cumulative volume for VWAP\n",
    "    df['Cumulative_Volume'] = df.groupby('Symbol')['Volume'].transform(\n",
    "    lambda x: x.cumsum()\n",
    "    )\n",
    "    # Calculate VWAP as the ratio of cumulative sums for each group (symbol)\n",
    "    df['VWAP'] = df['Cumulative_Price_Volume'] / df['Cumulative_Volume']\n",
    "\n",
    "\n",
    "# Add VWAP and VWAP window averages to dataframe\n",
    "# Shift rolling mean up by one to avoid future data leakage\n",
    "# Calculate VWAP using calculate_vwap function on the stock_data_1_week DataFrame\n",
    "# Iterate over columns, group by 'Symbol', and apply a rolling \n",
    "# window for each window size in one_week_window\n",
    "calculate_vwap(stock_data_1_week)\n",
    "for col in stock_data_1_week.columns:\n",
    "\n",
    "    for window in one_week_window:\n",
    "\n",
    "        stock_data_1_week[f'VWAP_{window}'] = stock_data_1_week.groupby('Symbol')['VWAP'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "\n",
    "        \n",
    "# Drop intermediate columns for VWAP calculations\n",
    "stock_data_1_week.drop(columns=['Cumulative_Price_Volume', 'Cumulative_Volume'], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ae3d0-575c-470e-b424-859cf1ddd12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_1_week.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865196bf-9b2c-4235-bc9e-c4b8285048cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Bollinger Bands for each stock ticker\n",
    "# Calculate rolling averages and standard deviation of 'Close' column\n",
    "# and add as columns in the given df\n",
    "# Rolling average of 'Close' is the 'Middle Band'\n",
    "# Calculate Upper and Lower Bollinger Bands using Middle Band and Standard Deviation\n",
    "# and add as columns in the given df\n",
    "# Upper Band = Middle Band + (Standard Deviation * 2)\n",
    "# Lower Band = Middle Band - (Standard Deviation * 2)\n",
    "\n",
    "def calculate_bollinger_bands(df):\n",
    "    # Ensure 'Close' is numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "\n",
    "    df['Middle_Band'] = df.groupby('Symbol')['Close'].transform(\n",
    "    lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "    )\n",
    "    \n",
    "    df['Std_Dev'] = df.groupby('Symbol')['Close'].transform(\n",
    "    lambda x: x.rolling(window=window, min_periods=1).std().shift(1)\n",
    "    )\n",
    "    \n",
    "    df['Upper_Band'] = df['Middle_Band'] + (df['Std_Dev'] * 2)\n",
    "    df['Lower_Band'] = df['Middle_Band'] - (df['Std_Dev'] * 2)\n",
    "\n",
    "\n",
    "# Create function to calculate bollinger bands for multiple window sizes in a DataFrame\n",
    "# Apply calculate_bollinger_bands function to each window size and add to column\n",
    "\n",
    "def calculate_bollinger_band_windows(df, windows):\n",
    "    # Ensure 'Close' is numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "\n",
    "    for window in windows:\n",
    "        df[f'Middle_Band_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "\n",
    "        df[f'Std_Dev_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std().shift(1)\n",
    "        )\n",
    "\n",
    "        df[f'Upper_Band_{window}'] = df[f'Middle_Band_{window}'] + (df[f'Std_Dev_{window}'] * 2)\n",
    "        df[f'Lower_Band_{window}'] = df[f'Middle_Band_{window}'] - (df[f'Std_Dev_{window}'] * 2)\n",
    "\n",
    "\n",
    "# Apply calculate_bollinger_bands to stock_data_1_week DataFrame\n",
    "calculate_bollinger_bands(stock_data_1_week)   \n",
    "\n",
    "# Loop through all columns in stock_data_1_week and \n",
    "# calculate Bollinger Bands for each window in one_week_window\n",
    "for col in stock_data_1_week.columns:\n",
    "    calculate_bollinger_band_windows(stock_data_1_week, one_week_window)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bf089-8654-49a4-8623-61c4b5709900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Average True Range (ATR)\n",
    "\n",
    "# Function to calculate True Range (TR)\n",
    "def calculate_true_range(df):\n",
    "    # Convert relevant columns to numeric (if not already numeric)\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "    df['Daily_High'] = pd.to_numeric(df['Daily_High'], errors='coerce')\n",
    "    df['Daily_Low'] = pd.to_numeric(df['Daily_Low'], errors='coerce')\n",
    "\n",
    "    # Ensure previous close is calculated per stock symbol to prevent cross-stock contamination\n",
    "    df['ATR_Prev_Close'] = df.groupby('Symbol')['Close'].shift(1)\n",
    "\n",
    "    # Daily High Price - Daily Low Price\n",
    "    df['ATR_High_Low'] = df['Daily_High'] - df['Daily_Low']  \n",
    "\n",
    "    # Daily High - Prev Close\n",
    "    df['ATR_High_Close'] = (df['Daily_High'] - df['ATR_Prev_Close']).abs()  \n",
    "\n",
    "    # Daily Low - Prev Close\n",
    "    df['ATR_Low_Close'] = (df['Daily_Low'] - df['ATR_Prev_Close']).abs()  \n",
    "\n",
    "    # True Range is the max of the three\n",
    "    df['ATR'] = df[['ATR_High_Low', 'ATR_High_Close', 'ATR_Low_Close']].max(axis=1)\n",
    "\n",
    "    # Drop Intermediate columns\n",
    "    df.drop(columns=['ATR_Prev_Close', 'ATR_High_Low', 'ATR_High_Close', 'ATR_Low_Close'], inplace=True)\n",
    "    \n",
    "\n",
    "# Apply calculate_true_range to the stock_data_1_week DataFrame\n",
    "calculate_true_range(stock_data_1_week)\n",
    "\n",
    "# Grouped by 'Symbol', Add ATR rolling mean for each window size \n",
    "# in one_week_window\n",
    "# Shift by 1 to prevent future data leakage\n",
    "for col in stock_data_1_week.columns:\n",
    "    for window in one_week_window:\n",
    "        stock_data_1_week[f'ATR_{window}'] = stock_data_1_week.groupby('Symbol')['ATR'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5a9d3-dc4d-42c4-b15c-b50611289d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_1_week.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a4085-12e8-4e7d-97f1-93e45e193eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create function to calculate Fibonacci levels for a given window\n",
    "# Group by 'Symbol' and calculate rolling max and min for each window \n",
    "# Shift(1) to avoid future data leakage\n",
    "# For each Fibonacci level in the provided 'levels' list, calculate the retracement level\n",
    "# This is done by subtracting the Fibonacci level multiplied by the difference \n",
    "# between high and low from the high value\n",
    "# Add columns for each Fib Level and subsequent rolling window sizes\n",
    "\n",
    "def fib_retracement(df, windows, levels):\n",
    "    for window in windows:\n",
    "\n",
    "        df[f'Fib_{window}_High_Max'] = df.groupby('Symbol')['Daily_High'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).max()\n",
    "        )\n",
    "        df[f'Fib_{window}_Low_Min'] = df.groupby('Symbol')['Daily_Low'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window, min_periods=1).min()\n",
    "        )\n",
    "    \n",
    "        # Calculate Fibonacci retracement levels for each level\n",
    "        for level in fib_levels:\n",
    "            df[f'{window}_day_Fib_{level*100:.1f}%'] = df[f'Fib_{window}_High_Max'] - (\n",
    "                level * (df[f'Fib_{window}_High_Max'] - df[f'Fib_{window}_Low_Min']))\n",
    "\n",
    "        # Drop Intermediate Columns\n",
    "        df.drop(columns = [f'Fib_{window}_High_Max', f'Fib_{window}_Low_Min'], inplace=True)\n",
    "    \n",
    "\n",
    "# Define Fibonacci levels\n",
    "fib_levels = [0.236, 0.382, 0.500, 0.618, 0.786, 1.000, 1.618, 2.618, 4.236]\n",
    "\n",
    "# Apply fib_retracement function using stock_data_1_week, \n",
    "# one_week_window, and fib levels defined above\n",
    "fib_retracement(stock_data_1_week, one_week_window, fib_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95450a29-d142-4d5b-b665-dc4add35c92b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create function to calculate OBV (On-Balance Volume)\n",
    "# OBV uses volume flow to predict changes in stock price\n",
    "# Group by 'Symbol' calculate difference between current days'\n",
    "# closing price and previous days closing price.\n",
    "# If price increased since yesterday, OBV increases by Volume\n",
    "# If price decreased, OBV decreases by Volume \n",
    "# cumsum() makes it cumulative over time to keep track\n",
    "# Shift(1) to prevent future data leakage \n",
    "# Fill initial columns with 0\n",
    "\n",
    "def calculate_obv(df):\n",
    "    df['OBV'] = df.groupby('Symbol').apply(\n",
    "        lambda group: (np.sign(group['Close'].diff()) * group['Volume']).cumsum()\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    df['OBV'] = df['OBV'].shift(1)\n",
    "    \n",
    "    df['OBV'] = df['OBV'].fillna(0) \n",
    "\n",
    "    return df\n",
    "\n",
    "# Create function to calculate rolling OBV for a dataframe with specified window sizes\n",
    "# Fill NaN values in first few rows with 0\n",
    "def calculate_rolling_obv(df, windows):\n",
    "    df[f'OBV_{window}day_avg'] = df.groupby('Symbol')['OBV'].transform(\n",
    "        lambda x: x.rolling(window=window).mean()\n",
    "    )\n",
    "\n",
    "    df[f'OBV_{window}day_avg'] = df[f'OBV_{window}day_avg'].fillna(0)\n",
    "    \n",
    "\n",
    "# Apply OBV calculation to stock_data_1_week dataframe\n",
    "calculate_obv(stock_data_1_week)\n",
    "\n",
    "# Apply calculate_rolling_obv using stock_data_1_week\n",
    "# and window sizes in one_week_window and add columns\n",
    "# to DataFrame\n",
    "for col in stock_data_1_week.columns:\n",
    "    for window in one_week_window:\n",
    "        calculate_rolling_obv(stock_data_1_week, window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4c2dbd-9a33-4318-9a7c-83b48b31d43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stock_data_1_week.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5aa41-1b2d-4f7f-9400-17ff9b4bdfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Function to calculate Momentum Features\n",
    "# Calculate momentum for a DataFrame with specified rolling window sizes\n",
    "# Group by 'Symbol'\n",
    "# Momentum is calculated by subtracting the 'Close' price of 'window' days ago from\n",
    "# the current 'Close' price\n",
    "# x.shift(window) shifts the 'Close' column by 'window' periods\n",
    "# Fill the intial NaN rows with 0\n",
    "\n",
    "def calculate_momentum(df, windows):\n",
    "    for window in windows:\n",
    "        df[f'Momentum_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x - x.shift(window)\n",
    "        ).fillna(0)\n",
    "\n",
    "# Use calculate_momentum function on stock_data_1_week \n",
    "# and one_week_window for window sizes\n",
    "calculate_momentum(stock_data_1_week, one_week_window)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e088481-5272-455e-bcf4-8892fb8a4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to calculate Quantile-Based Features\n",
    "# and add as new columns to DataFrame\n",
    "# Group by 'Symbol' for each calculation\n",
    "# Calculate median, quantile(0.25), and quantile(0.75) and apply given \n",
    "# rolling window sizes to each quantile calculation for each window\n",
    "# Fill initial NaN columns with 0\n",
    "\n",
    "def calculate_quantiles(df, windows):\n",
    "\n",
    "    for window in windows:\n",
    "    \n",
    "        df[f'Rolling_Median_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window).median()\n",
    "        ).fillna(0)\n",
    "    \n",
    "        # Rolling 25th Quantile\n",
    "        df[f'Rolling_Quantile_25_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window).quantile(0.25)\n",
    "        ).fillna(0)\n",
    "    \n",
    "        # Rolling 75th Quantile\n",
    "        df[f'Rolling_Quantile_75_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window).quantile(0.75)\n",
    "        ).fillna(0)\n",
    "\n",
    "# Apply calculate_quantiles function to stock_data_1_week using window sizes in one_week_window\n",
    "calculate_quantiles(stock_data_1_week, one_week_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d2bb2-74be-4999-bf5f-22eabf608bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_1_week.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb3139-137a-4bdf-8fd5-ca92fcd5bdd2",
   "metadata": {},
   "source": [
    "### Filter Important Features to Lag\n",
    "With 10 years of historical data, 12 TA indicators, and at least 3 moving averages for each feature, the number of features can add up fast and too many features will add unnecessary noise to our model. In order to filter our features down, we use the baseline version of our XGBoost ML model in order to get a list of the top 35 features contributing most to the model's predictions. These top 35 features will have 'lags' added to them. \n",
    "Lag is just shifting a feature backward in time a specified number of steps. Each feature will have 1day, 2day, 3day, 4day, and 5day lags, which are different from moving averages because they are just the feature shifted back 1, 2, 3, 4, and 5 days, as a way to give the model recent data. Lags are important in time series analysis because when we train the model on the training data, the model iterates through each row and only looks at the data available in that row to analyze and create patterns to predict the target. With lags, each row contains the feature value today, yesterday, the day before that, and so on, which gives the model historical data to use for analysis. \n",
    "Once we add lags to the top 35 features, we add the lagged features to our DataFrame and then run it through the same baseline model again to get feature importance. This time, we filter by top 60 and only keep the top 60 features contributing most to the model's prediction. We then add lags for our target variable ('Close'), which leaves us with 60 filtered important features, 2 descriptive columns ('Date', and 'Symbol'), our target variable, and the lags for our target variable, which leaves us with 68 total columns in our DataFrame. This is the finalized preprocesssed DataFrame we will use to train and test our model with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441efe9-d0a4-44c4-84e5-8d29a6f147b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the baseline XGBoost model, print out top 35 contributing features\n",
    "\n",
    "stock_data_important_feat = stock_data_1_week.copy()\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "stock_data_important_feat = stock_data_important_feat.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Train on all data before 2023-01-24\n",
    "stock_data_train = stock_data_important_feat[stock_data_important_feat['Date'] <= '2023-01-24']\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "stock_data_train['Close_Target'] = stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "stock_data_train = stock_data_train.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "stock_data_train[numeric_cols_train] = stock_data_train[numeric_cols_train].fillna(stock_data_train[numeric_cols_train].median())\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = stock_data_train['Close_Target']\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan, # Ensure missing values are handled correctly\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "\n",
    "# Sort by importance and select top 35\n",
    "top_35_important_features = feature_importance.nlargest(35).index.tolist()\n",
    "print(\"Top 35 Important Features:\", top_35_important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86347a64-a950-4aa2-aa90-fb888c7a675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize features_to_lag as the top 35 important contributing features\n",
    "features_to_lag = top_35_important_features\n",
    "\n",
    "# Creating lag features for each column\n",
    "lags = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create function to  create lag features for eachof the given features\n",
    "# Loop through each feature in top_35_important_features\n",
    "# Check if column exists, if it doesn't exist, create lagging column\n",
    "# for each lagged feature. \n",
    "# Each feature in top_35_important_features gets 5 lags\n",
    "# If column not found in DataFrame, skip\n",
    "\n",
    "def calculate_lag(df, features, lags):\n",
    "    # Apply lags to the dataframe\n",
    "    for col in features:\n",
    "        if col in df.columns:\n",
    "            for lag in lags:\n",
    "                if f'{col}_lag_{lag}' not in df.columns:\n",
    "                    df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "            # else:\n",
    "                # print(f\"Column '{col}' not found in dataframe, skipping.\")\n",
    "\n",
    "\n",
    "# Apply calculate_lag function using stock_data_1_week, features_to_lag, \n",
    "# and all 5 lags for each feature\n",
    "calculate_lag(stock_data_1_week, features_to_lag, lags)\n",
    "    \n",
    "stock_data_1_week.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4bee9-0d51-4518-92a2-ceec12487333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the addition of our lag features, we use the XGBoost baseline model\n",
    "# to sort by the top 60 most important features\n",
    "stock_data_important_feat = stock_data_1_week.copy()\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "stock_data_important_feat = stock_data_important_feat.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Train on all data before 2023-01-24\n",
    "stock_data_train = stock_data_important_feat[stock_data_important_feat['Date'] <= '2023-01-24']\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "stock_data_train['Close_Target'] = stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "stock_data_train = stock_data_train.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "stock_data_train[numeric_cols_train] = stock_data_train[numeric_cols_train].fillna(stock_data_train[numeric_cols_train].median())\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = stock_data_train['Close_Target']\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan, # Ensure missing values are handled correctly\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "\n",
    "# Sort by importance and select top 60\n",
    "top_60_important_features = feature_importance.nlargest(60).index.tolist()\n",
    "print(\"Top 60 Important Features:\", top_60_important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505c50b-fd12-49a2-aba8-8e97493e984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter stock_data_1_week dataframe to only the top 60 important features,\n",
    "# descriptive columns, and the target features. \n",
    "# All other columns are removed\n",
    "\n",
    "descriptive_features = ['Symbol', 'Date']\n",
    "\n",
    "target_feature = ['Close']\n",
    "\n",
    "filtered_features = descriptive_features + target_feature + top_60_important_features\n",
    "\n",
    "stock_data_1_week = stock_data_1_week[filtered_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c509d22-9405-4b4e-b48b-aa490f54ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_1_week.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d80bf-cec7-46d6-b799-e600664bb121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize features_to_lag as target_feature\n",
    "features_to_lag = target_feature\n",
    "\n",
    "# Lags for each feature\n",
    "lags = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Apply calculate_lag) function to dataframe, features_to_lag,\n",
    "# which is now just the target varible, and our list of 5 lags\n",
    "calculate_lag(stock_data_1_week, features_to_lag, lags)\n",
    "    \n",
    "stock_data_1_week.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ccdee-b1c6-4db9-88a1-c1713148bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_1_week.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e727d-282c-48d9-b6b0-71199928d0e0",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "The model we will be using XGBoost (Extreme Gradient Boosting). XGBoost is a powerful machine learning algorithm based on gradient boosting and is optimized for speed and performance. It's great for structured data because of it's ability to handle missing values, prevent overfitting through regularlization, process large datasets, and handle unscaled data. XGBoost is used often in time series stock analysis for its' ability to capture complex patterns and identify nonlinear dependencies and interactions between features. This is especially useful since stock prices are influenced by a large number of factors. \n",
    "Below are some scientific and academic research papers on the usefulness of XGBoost for capturing complex patterns in time series data:\n",
    "* [High High Accuracy Stock Trend Prediction using XGBoost Model by Alex Chen](https://osf.io/hmj53/?utm_source=chatgpt.com)\n",
    "* [Stock Price Prediction Method Basedon XGboost Algorithm by Yifan Zhang](https://www.researchgate.net/publication/366442499_Stock_Price_Prediction_Method_Based_on_XGboost_Algorithm)\n",
    "* [Stock Market Prediction: XGBoost and LSTM Comparative Analysis by Saddam Hossain and Gagninder Kaur](https://ieeexplore.ieee.org/document/10574794?utm_source=chatgpt.com)\n",
    "* [Forecasting method of stock market volatility in time series data based on mixed model of ARIMA and XGBoost by Yan Wang and Yuankai Guo](https://ieeexplore.ieee.org/abstract/document/9058617)\n",
    "* [Stock Market Performance Analytics Using XGBoost by Nisar Hussain, Amna Qasim, Zia-ud-din Akhtar, Ayesha Qasim, Gull Mehak, Luciana del Socorro Espindola Ulibarri, Olga Kolesnikova and Alexander Gelbukh ](https://link.springer.com/chapter/10.1007/978-3-031-47765-2_1)\n",
    "\n",
    "Our XGBoost model is structured specifically for time series analysis. Instead of using the normal 'train_test_split', we manually split our data into training and testing sets, as 'train_test_split' would have randomly split data off for training and that would not work for temporal data. Our training data is all data for each stock from 2015-01-01 up to 2023-01-24 and all testing data is about the last two years of our data, from 2023-01-31 to the end of our data (2025-03-05). A copy of our target , 'Close', is copied and shifted back 5 trading days, so that the model can train on the value it is trying to predict with the feature values 5 trading days before. A default set of hyperparameters is used to create the model and upon fitting the model, performance metrics and a list of the top most important features are then printed out for evaluation. \n",
    "\n",
    "In total, 6 models are created and 3 hyperparameters are tuned. With the tuning of each hyperparameter, the metrics of the orignal model and tuned model will be evaluated based on metric performance. Evaluation of the best model will be based on:\n",
    "* Performance metrics\n",
    "  * R-Squared\n",
    "    * Represents the proportion of variance in the dependent (target) variable that is explained by the model. It is on a scale of 0-1 and the closer the value is to 1, the better performing the model is. \n",
    "  * Mean Squared Error\n",
    "    * Measures the average squared differences between the predicted and actual values. Larger values mean higher prediction errors. Sensitive to large values and can show if the model has high outliers. The lower the value, the better. \n",
    "  *  Mean Absolute Error\n",
    "     * Calculates the average of the absolute differences between predicted and actual values. Useful because it is in the same units as our model, giving us a realistic typical error size that our model is off by. The lower the value, the better.\n",
    "  *  Root Mean Squared Error\n",
    "     * Like Mean Absolute Error, Root Mean Squared Error is useful for evaluation because the value is in the same units as our predicted values. It differs because it is the square root of the average of the squared differences in predicted and actual values, which penalizes larger errors more heavily than smaller errors. This metric is sensitive to large values and can show if outliers are distorting the model. The lower the value, the better.\n",
    "  *  Mean Absolute Percentage Error (MAPE)\n",
    "     * Measures the absolute percentage difference between predicted and actual values. The value is given in percentage terms. The lower the value, the better. \n",
    "* Feature Diversity\n",
    "  * Making sure one feature is not responsible for explaining the majority of the model's predictive power.\n",
    " \n",
    "Given that we are analyzing time series market data, absolute metrics (Mean Absolute Error, Mean Absolute Percentage Error, and Median Absolute Error) will be prioritized over squared metrics (Mean Squared Error, Root Mean Squared Error) when evaluating the predictive power of the model. The reason for this is our goal is predicting future market behavior and absolute metrics directly measure how far off the model's predictions are. Squared metrics are sensitive to large errors, which is useful if your goal is to minimize errors, that is irrelevant to us because our goal is predictive power. In markets, sudden shifts can happen that cause extreme outliers. Squared metrics give disproportionately large penalties for these outliers, which can shift the model focus and cause it to overfit to rare/random events. Absolute metrics are less sensitive to these large outliers. On top of this, absolute metrics are in the same unit as the target variable, making them easier to interpret and relate to real-world outcomes. The performance of squared metrics will only be evaluated if two of the models have very similar absolute metrics and we need to choose one to move forward with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147decf-b6b4-41c1-b7b0-83139680756d",
   "metadata": {},
   "source": [
    "### Model 1: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e3735-0369-486e-8f4d-3394e79d96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Baseline Model\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "stock_data_1_week = stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until January 24 2023 for training \n",
    "stock_data_train_1_week_baseline = stock_data_1_week[stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from January 31 2023 onwards for testing\n",
    "stock_data_test_1_week_baseline = stock_data_1_week[stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "stock_data_train_1_week_baseline['Close_Target'] = stock_data_train_1_week_baseline.groupby('Symbol')['Close'].shift(-5)\n",
    "stock_data_test_1_week_baseline['Close_Target'] = stock_data_test_1_week_baseline.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "stock_data_train_1_week_baseline = stock_data_train_1_week_baseline.dropna(subset=['Close_Target'])\n",
    "stock_data_test_1_week_baseline = stock_data_test_1_week_baseline.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "stock_data_train_1_week_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "stock_data_test_1_week_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Filter DataFrame to numeric columns in order to fill NaN values\n",
    "numeric_cols_train = stock_data_train_1_week_baseline.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = stock_data_test_1_week_baseline.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Fill NaN values with the median of each numeric column\n",
    "stock_data_train_1_week_baseline[numeric_cols_train] = stock_data_train_1_week_baseline[numeric_cols_train].fillna(stock_data_train_1_week_baseline[numeric_cols_train].median())\n",
    "stock_data_test_1_week_baseline[numeric_cols_test] = stock_data_test_1_week_baseline[numeric_cols_test].fillna(stock_data_test_1_week_baseline[numeric_cols_test].median())\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_baseline = stock_data_train_1_week_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_baseline = stock_data_train_1_week_baseline['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_baseline = stock_data_test_1_week_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_baseline = stock_data_test_1_week_baseline['Close_Target']\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_week = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  \n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan,  # Ensure missing values are handled correctly\n",
    "    colsample_bytree = 1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_week.fit(X_train_1_week_baseline, y_train_1_week_baseline)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_1_week_baseline = model_baseline_1_week.predict(X_test_1_week_baseline)\n",
    "\n",
    "# Print upon completion of training\n",
    "print(\"Training for Model 1: Baseline Model complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63f8dd-0c9a-4c3b-8b80-e54c451605d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate performance metrics on Model 1 Baseline test data\n",
    "\n",
    "# Mean Absolute Error = average absolute difference between actual values and predicted values\n",
    "# Mean Squared Error = average squared difference between actual values and predicted values\n",
    "# Root Mean Squared Error = Square root of Mean Squared Error\n",
    "# R_Squared = Measures how well model explains variance in the target variable\n",
    "# Median Absolute Error = Median of Absolute differences between actual avlues and predicted values\n",
    "# Mean Absolute Percentage Error (MAPE) = Expresses Percentage error as a percentage of actual values\n",
    "# Durbin-Watson (DW) = Detects autocorrelation in residuals\n",
    "\n",
    "mae_1_week_baseline = mean_absolute_error(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "mse_1_week_baseline = mean_squared_error(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "rmse_1_week_baseline = np.sqrt(mse_1_week_baseline)  # Root Mean Squared Error\n",
    "r2_1_week_baseline = r2_score(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "medae_1_week_baseline = median_absolute_error(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "mape_1_week_baseline = np.mean(np.abs((y_test_1_week_baseline - y_pred_1_week_baseline) / y_test_1_week_baseline)) * 100\n",
    "dw_stat_1_week_baseline = durbin_watson(y_test_1_week_baseline - y_pred_1_week_baseline)\n",
    "\n",
    "# Print out the metrics for test data\n",
    "print(f'Mean Absolute Error on test data: {mae_1_week_baseline}')\n",
    "print(f'Mean Squared Error on test data: {mse_1_week_baseline}')\n",
    "print(f'Root Mean Squared Error on test data: {rmse_1_week_baseline}')\n",
    "print(f'R-squared on test data: {r2_1_week_baseline}')\n",
    "print(f'Median Absolute Error on test data: {medae_1_week_baseline}')\n",
    "print(f'MAPE on test data: {mape_1_week_baseline:.2f}%')\n",
    "print(f'Durbin-Watson Statistic on test data: {dw_stat_1_week_baseline}')\n",
    "\n",
    "\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_baseline = dict(zip(X_train_1_week_baseline.columns, model_baseline_1_week.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_baseline = sorted(feature_importance_1_week_baseline.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_baseline:\n",
    "    if int(importance*100) >= 1:\n",
    "        print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59e45c-672e-4aba-a2c2-f4ee6703f7be",
   "metadata": {},
   "source": [
    "### Model Evaluation: Model 1 Baseline Model\n",
    "\n",
    "| Performance Metrics |Model 1|\n",
    "|-|-|\n",
    "| R-Squared| 0.86957 | \n",
    "| Mape| 4.86% | \n",
    "| Root Mean Squared Error| 69.847 | \n",
    "| Mean Squared Error| 4878.631 | \n",
    "| Mean Abolute Error| 21.563 | \n",
    "| Median Absolute Error | 4.026 | \n",
    "\n",
    "\n",
    "|Feature|Model 1 Feature Diversity|\n",
    "|-|-|\n",
    "|Daily_Low|53.74%|\n",
    "|Daily_High|22.92%|\n",
    "|5_day_Fib_50.0%|4.23%|\n",
    "|3_day_Fib_50.0%|3.97%|\n",
    "\n",
    "\n",
    "Baseline Model Performance Metrics on Test Set\n",
    "* R-Squared: 0.8696\n",
    "  * 86.96% of the variance in the target variance (daily closing price) is captured by the model, which is a strong fit.\n",
    "\n",
    "* MAPE: 4.86%\n",
    "  * On average, the model's predictions are off by 4.86% from the actual stock price.\n",
    "\n",
    "* Mean Absolute Error: 21.56\n",
    "  * On average, when giving equal weight to all errors, our model's predicted values deviate from the actual values by about $21.56\n",
    " \n",
    "* Median Absolute Error: 4.026\n",
    "    * The median of the absolute differences between my actual values and predicted values is 4.026.\n",
    "\n",
    "For the model metrics, comparison evaluations cannot be performed until we create another model and compare the results. However, the lack of feature diversity in the current model should be addressed first. With only two features contributing the vast majority of the model's predictions (55% and 22%, respectively), its important to ensure that the model isnt overly reliant on just a few features. To increase feature diversity, I decided to adjust the 'colsample_bytree' hyperparameter from 1 to 0.5. 'colsample_bytree' controls the fraction of features to be randomly sampled for each tree during the model's training process. By changing 'colsample_bytree' from 1 to 0.5, a random subset of features  (50% each subset) will be sampled for each tree during the model's training process. This should increase feature diversity and improve generalization, while maintaining relativley similar performance metrics. \n",
    "\n",
    "#### Next Steps: Enhance feature diversity without sacrificing performance by adjusting colsample_bytree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1cb207-d731-4a44-892e-790afaeaaade",
   "metadata": {},
   "source": [
    "### Model 2: colsample_bytree = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe951d27-8685-451f-877e-dbcaf15a8918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: colsample_bytree = 0.5\n",
    "# attempt to reduce feature dominance, increase feature diversity, and maintain metric performance\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "stock_data_1_week = stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until January 24 2023 for training\n",
    "stock_data_train_1_week_bytree = stock_data_1_week[stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from January 31 2023 onwards for testing\n",
    "stock_data_test_1_week_bytree = stock_data_1_week[stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "stock_data_train_1_week_bytree['Close_Target'] = stock_data_train_1_week_bytree.groupby('Symbol')['Close'].shift(-5)\n",
    "stock_data_test_1_week_bytree['Close_Target'] = stock_data_test_1_week_bytree.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "stock_data_train_1_week_bytree = stock_data_train_1_week_bytree.dropna(subset=['Close_Target'])\n",
    "stock_data_test_1_week_bytree = stock_data_test_1_week_bytree.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "stock_data_train_1_week_bytree.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "stock_data_test_1_week_bytree.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Filter DataFrame to numeric columns in order to fill NaN values\n",
    "numeric_cols_train = stock_data_train_1_week_bytree.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = stock_data_test_1_week_bytree.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Fill NaN values with the median of each numeric column\n",
    "stock_data_train_1_week_bytree[numeric_cols_train] = stock_data_train_1_week_bytree[numeric_cols_train].fillna(stock_data_train_1_week_bytree[numeric_cols_train].median())\n",
    "stock_data_test_1_week_bytree[numeric_cols_test] = stock_data_test_1_week_bytree[numeric_cols_test].fillna(stock_data_test_1_week_bytree[numeric_cols_test].median())\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_bytree = stock_data_train_1_week_bytree.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_bytree = stock_data_train_1_week_bytree['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_bytree = stock_data_test_1_week_bytree.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_bytree = stock_data_test_1_week_bytree['Close_Target']\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_week_bytree = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan,  # Ensure missing values are handled correctly\n",
    "    colsample_bytree = 0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_week_bytree.fit(X_train_1_week_bytree, y_train_1_week_bytree)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_1_week_bytree = model_1_week_bytree.predict(X_test_1_week_bytree)\n",
    "\n",
    "# Print upon completion of training\n",
    "print(\"Training for Model 2 is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353cef8-3a04-44da-a123-0c0704c4b72d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate performance metrics on Model 2 test data\n",
    "\n",
    "# Mean Absolute Error = average absolute difference between actual values and predicted values\n",
    "# Mean Squared Error = average squared difference between actual values and predicted values\n",
    "# Root Mean Squared Error = Square root of Mean Squared Error\n",
    "# R_Squared = Measures how well model explains variance in the target variable\n",
    "# Median Absolute Error = Median of Absolute differences between actual avlues and predicted values\n",
    "# Mean Absolute Percentage Error (MAPE) = Expresses Percentage error as a percentage of actual values\n",
    "# Durbin-Watson (DW) = Detects autocorrelation in residuals\n",
    "\n",
    "mse_1_week_bytree = mean_squared_error(y_test_1_week_bytree, y_pred_1_week_bytree)\n",
    "mae_1_week_bytree = mean_absolute_error(y_test_1_week_bytree, y_pred_1_week_bytree)\n",
    "rmse_1_week_bytree = np.sqrt(mse_1_week_bytree)  # Root Mean Squared Error\n",
    "r2_1_week_bytree = r2_score(y_test_1_week_bytree, y_pred_1_week_bytree)\n",
    "medae_1_week_bytree = median_absolute_error(y_test_1_week_bytree, y_pred_1_week_bytree)\n",
    "mape_1_week_bytree = np.mean(np.abs((y_test_1_week_bytree - y_pred_1_week_bytree) / y_test_1_week_bytree)) * 100\n",
    "dw_stat_1_week_bytree = durbin_watson(y_test_1_week_bytree - y_pred_1_week_bytree)\n",
    "\n",
    "# Print out the metrics for test data\n",
    "print(f'Mean Squared Error on test data: {mse_1_week_bytree}')\n",
    "print(f'Mean Absolute Error on test data: {mae_1_week_bytree}')\n",
    "print(f'Root Mean Squared Error on test data: {rmse_1_week_bytree}')\n",
    "print(f'R-squared on test data: {r2_1_week_bytree}')\n",
    "print(f'Median Absolute Error on test data: {medae_1_week_bytree}')\n",
    "print(f'MAPE on test data: {mape_1_week_bytree:.2f}%')\n",
    "print(f'Durbin-Watson Statistic on test data: {dw_stat_1_week_bytree}')\n",
    "\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_bytree = dict(zip(X_train_1_week_bytree.columns, model_1_week_bytree.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_bytree = sorted(feature_importance_1_week_bytree.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_bytree:\n",
    "    if int(importance*100) >= 1:\n",
    "        print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752fa08e-55c7-4f61-9ff1-70c8f76a6fff",
   "metadata": {},
   "source": [
    "### Model Evaluation: Baseline Model vs. Model 2\n",
    "\n",
    "\n",
    "| Performance Metric|Model 1|Model 2| Top Performing Model\n",
    "|-|-|-|-|\n",
    "| R-Squared| 0.86957 | 0.86956| Model 1\n",
    "| Mape| 4.86% | 4.84% | Model 2\n",
    "| Root Mean Squared Error| 69.847 | 69.852| Model 1\n",
    "| Mean Squared Error| 4878.634 | 4879.256 | Model 1\n",
    "| Mean Abolute Error| 21.563 | 21.521| Model 2\n",
    "| Median Absolute Error | 4.026 | 3.983 | Model 2\n",
    "\n",
    "\n",
    "|Feature|Model 1 Feature Diversity|\n",
    "|-|-|\n",
    "|Daily_Low|53.75%|\n",
    "|Daily_High|22.92%|\n",
    "|5_day_Fib_50.0%|4.23%|\n",
    "|3_day_Fib_50.0%|3.97%|\n",
    "\n",
    "\n",
    "|Feature|Model 2 Feature Diversity|\n",
    "|-|-|\n",
    "|Daily_Low|33.13%|\n",
    "|Daily_High|28.05%|\n",
    "|Open|13.37%|\n",
    "|3_day_Fib_78.6%|6.89%|\n",
    "|Rolling_Quantile_25_3|3.21%|\n",
    "\n",
    "Let's first compare our feature diversity, which was the goal is changing colsample_bytree from 1 to 0.5 in model 2. As we can see, we were successful in increasing feature diversity. Daily_Low dropped from 53.75% to 33.13%, and we now have two extra features over 5%. Before we solidify this as a success, let's compare our performance metrics. Model 2 performed best with all of our absolute metrics: MAPE, Mean Absolute Value, and Median Absolute Value. For the squared metrics, model 1 technically had better values, but most of them were insignificant. Since model 2 positivley increased feature diversity and absolute performance metrics improved, model 2 is a better performer and we are going to solidfy the hyperparameter 'colsample_bytree' as 0.5 moving forward. \n",
    "\n",
    "The next hyperparameter we are going to adjust is 'max_depth', which controls the maximum depth of a decision tree. It helps determine how many levels a tree can grow before stopping. A larger 'max_depth' allows the tree to keep splitting until it fully classifies the data, potentially leading to overfitting. A smaller 'max_depth' limits how many splits can be made, forcing the tree to generalize more, but could lead to underfitting the data. Our default 'max_depth' is 5 and we are going to try 'max_depth' values of 3 and 7. \n",
    "\n",
    "#### Next Steps: Solidify colsample_bytree as 0.5 moving forward and try adjusting max_depth hyperparameter to the values 3 and 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0f871-b168-4586-9270-73b47201044d",
   "metadata": {},
   "source": [
    "### Model 3: max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7096f4a-a0e0-4840-9beb-9215a415f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: max_depth = 3\n",
    "# colsample_bytree = 0.5\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "stock_data_1_week = stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until January 24 2023 for training\n",
    "stock_data_train_1_week_md_3 = stock_data_1_week [stock_data_1_week ['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from January 31 2023 for testing\n",
    "stock_data_test_1_week_md_3 = stock_data_1_week [stock_data_1_week ['Date'] > '2023-01-31']\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "stock_data_train_1_week_md_3['Close_Target'] = stock_data_train_1_week_md_3.groupby('Symbol')['Close'].shift(-5)\n",
    "stock_data_test_1_week_md_3['Close_Target'] = stock_data_test_1_week_md_3.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "stock_data_train_1_week_md_3 = stock_data_train_1_week_md_3.dropna(subset=['Close_Target'])\n",
    "stock_data_test_1_week_md_3 = stock_data_test_1_week_md_3.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "stock_data_train_1_week_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "stock_data_test_1_week_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Filter DataFrame to numeric columns in order to fill NaN values\n",
    "numeric_cols_train = stock_data_train_1_week_md_3.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = stock_data_test_1_week_md_3.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Fill NaN values with the median of each numeric column\n",
    "stock_data_train_1_week_md_3[numeric_cols_train] = stock_data_train_1_week_md_3[numeric_cols_train].fillna(stock_data_train_1_week_md_3[numeric_cols_train].median())\n",
    "stock_data_test_1_week_md_3[numeric_cols_test] = stock_data_test_1_week_md_3[numeric_cols_test].fillna(stock_data_test_1_week_md_3[numeric_cols_test].median())\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_md_3 = stock_data_train_1_week_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_md_3 = stock_data_train_1_week_md_3['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_md_3 = stock_data_test_1_week_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_md_3 = stock_data_test_1_week_md_3['Close_Target']\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_week_md_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000, # original setting: 1000\n",
    "    learning_rate=0.05, # original settings: 0.05\n",
    "    max_depth=3, # original setting: 3\n",
    "    alpha=0, # original setting: 0\n",
    "    reg_lambda=1,  # Original setting = 1\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan,  # Ensure missing values are handled correctly\n",
    "    gamma=0, # original setting = 0\n",
    "    subsample = 1, # original setting = 1\n",
    "    colsample_bytree = 0.5, # default colsample_bytree = \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_week_md_3.fit(X_train_1_week_md_3, y_train_1_week_md_3)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_1_week_md_3 = model_1_week_md_3.predict(X_test_1_week_md_3)\n",
    "\n",
    "# Print upon completion of training\n",
    "print(\"Training for Model 3 is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883777b-c111-457c-aa6d-07a947b0440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics on Model 3 test data\n",
    "\n",
    "# Mean Absolute Error = average absolute difference between actual values and predicted values\n",
    "# Mean Squared Error = average squared difference between actual values and predicted values\n",
    "# Root Mean Squared Error = Square root of Mean Squared Error\n",
    "# R_Squared = Measures how well model explains variance in the target variable\n",
    "# Median Absolute Error = Median of Absolute differences between actual avlues and predicted values\n",
    "# Mean Absolute Percentage Error (MAPE) = Expresses Percentage error as a percentage of actual values\n",
    "# Durbin-Watson (DW) = Detects autocorrelation in residuals\n",
    "\n",
    "mse_1_week_md_3 = mean_squared_error(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "mae_1_week_md_3 = mean_absolute_error(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "rmse_1_week_md_3 = np.sqrt(mse_1_week_md_3)  # Root Mean Squared Error\n",
    "r2_1_week_md_3 = r2_score(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "medae_1_week_md_3 = median_absolute_error(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "mape_1_week_md_3 = np.mean(np.abs((y_test_1_week_md_3 - y_pred_1_week_md_3) / y_test_1_week_md_3)) * 100\n",
    "dw_stat_1_week_md_3 = durbin_watson(y_test_1_week_md_3 - y_pred_1_week_md_3)\n",
    "\n",
    "# Print out the metrics for test data\n",
    "print(f'Mean Squared Error on test data: {mse_1_week_md_3}')\n",
    "print(f'Mean Absolute Error on test data: {mae_1_week_md_3}')\n",
    "print(f'Root Mean Squared Error on test data: {rmse_1_week_md_3}')\n",
    "print(f'R-squared on test data: {r2_1_week_md_3}')\n",
    "print(f'Median Absolute Error on test data: {medae_1_week_md_3}')\n",
    "print(f'MAPE on test data: {mape_1_week_md_3:.2f}%')\n",
    "print(f'Durbin-Watson Statistic on test data: {dw_stat_1_week_md_3}')\n",
    "\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_md_3 = dict(zip(X_train_1_week_md_3.columns, model_1_week_md_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_md_3 = sorted(feature_importance_1_week_md_3.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_md_3:\n",
    "    if int(importance*100) >= 1:\n",
    "        print(f\"{feature}: {importance * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7af63b-4af2-4632-9326-3a5c47cf4600",
   "metadata": {},
   "source": [
    "### Model 4: max_depth = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153b866-efcf-4e21-ad86-d2eb822cc22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: max_depth = 7\n",
    "# colsample_bytree = 0.5\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "stock_data_1_week = stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until January 24 2023 for training\n",
    "stock_data_train_1_week_md_7 = stock_data_1_week[stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from January 31 2023 onwards for testing\n",
    "stock_data_test_1_week_md_7 = stock_data_1_week[stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "stock_data_train_1_week_md_7['Close_Target'] = stock_data_train_1_week_md_7.groupby('Symbol')['Close'].shift(-5)\n",
    "stock_data_test_1_week_md_7['Close_Target'] = stock_data_test_1_week_md_7.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "stock_data_train_1_week_md_7 = stock_data_train_1_week_md_7.dropna(subset=['Close_Target'])\n",
    "stock_data_test_1_week_md_7 = stock_data_test_1_week_md_7.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "stock_data_train_1_week_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "stock_data_test_1_week_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Filter DataFrame to numeric columns in order to fill NaN values\n",
    "numeric_cols_train = stock_data_train_1_week_md_7.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = stock_data_test_1_week_md_7.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Fill NaN values with the median of each numeric column\n",
    "stock_data_train_1_week_md_7[numeric_cols_train] = stock_data_train_1_week_md_7[numeric_cols_train].fillna(stock_data_train_1_week_md_7[numeric_cols_train].median())\n",
    "stock_data_test_1_week_md_7[numeric_cols_test] = stock_data_test_1_week_md_7[numeric_cols_test].fillna(stock_data_test_1_week_md_7[numeric_cols_test].median())\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_md_7 = stock_data_train_1_week_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_md_7 = stock_data_train_1_week_md_7['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_md_7 = stock_data_test_1_week_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_md_7 = stock_data_test_1_week_md_7['Close_Target']\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_week_md_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan,  # Ensure missing values are handled correctly\n",
    "    colsample_bytree = 0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_week_md_7.fit(X_train_1_week_md_7, y_train_1_week_md_7)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_1_week_md_7 = model_1_week_md_7.predict(X_test_1_week_md_7)\n",
    "\n",
    "# Print upon completion of training\n",
    "print(\"Training for Model 4 is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb309a8-801e-49ac-b3be-5b61c9843585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics on Model 4 test data\n",
    "\n",
    "# Mean Absolute Error = average absolute difference between actual values and predicted values\n",
    "# Mean Squared Error = average squared difference between actual values and predicted values\n",
    "# Root Mean Squared Error = Square root of Mean Squared Error\n",
    "# R_Squared = Measures how well model explains variance in the target variable\n",
    "# Median Absolute Error = Median of Absolute differences between actual avlues and predicted values\n",
    "# Mean Absolute Percentage Error (MAPE) = Expresses Percentage error as a percentage of actual values\n",
    "# Durbin-Watson (DW) = Detects autocorrelation in residuals\n",
    "\n",
    "mse_1_week_md_7 = mean_squared_error(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "mae_1_week_md_7 = mean_absolute_error(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "rmse_1_week_md_7 = np.sqrt(mse_1_week_md_7)  # Root Mean Squared Error\n",
    "r2_1_week_md_7 = r2_score(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "medae_1_week_md_7 = median_absolute_error(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "mape_1_week_md_7 = np.mean(np.abs((y_test_1_week_md_7 - y_pred_1_week_md_7) / y_test_1_week_md_7)) * 100\n",
    "dw_stat_1_week_md_7 = durbin_watson(y_test_1_week_md_7 - y_pred_1_week_md_7)\n",
    "\n",
    "# Print out the metrics for test data\n",
    "print(f'Mean Squared Error on test data: {mse_1_week_md_7}')\n",
    "print(f'Mean Absolute Error on test data: {mae_1_week_md_7}')\n",
    "print(f'Root Mean Squared Error on test data: {rmse_1_week_md_7}')\n",
    "print(f'R-squared on test data: {r2_1_week_md_7}')\n",
    "print(f'Median Absolute Error on test data: {medae_1_week_md_7}')\n",
    "print(f'MAPE on test data: {mape_1_week_md_7:.2f}%')\n",
    "print(f'Durbin-Watson Statistic on test data: {dw_stat_1_week_md_7}')\n",
    "\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_md_7 = dict(zip(X_train_1_week_md_7.columns, model_1_week_md_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_md_7 = sorted(feature_importance_1_week_md_7.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_md_7:\n",
    "    if int(importance*100) >= 1:\n",
    "        print(f\"{feature}: {importance * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee00e812-cd60-4b8d-b1d5-93c3822f8114",
   "metadata": {},
   "source": [
    "# Model Evaluation: Model 2 vs. Model 3 vs. Model 4 \n",
    "\n",
    "| Performance Metric|Model 2 |Model 3 |Model 4 | Top Performer\n",
    "|-|-|-|-|-|\n",
    "| R-Squared| 0.86958 | 0.869199 | 0.869695 | Model 4\n",
    "| Mape| 4.84%| 4.80% | 4.91%| Model 3\n",
    "| Root Mean Squared Error| 69.844 | 69.9469| 69.814 | Model 4\n",
    "| Mean Squared Error| 4879.234 | 4892.573 | 4874.03 | Model 4\n",
    "| Mean Abolute Error| 21.514 | 21.429| 21.66 | Model 3\n",
    "| Median Absolute Error | 3.983 | 3.925 | 4.13\n",
    "\n",
    "|Feature|Model 2 Feature Diversity|\n",
    "|-|-|\n",
    "|Daily_Low|32.95%|\n",
    "|Daily_High|29.79%|\n",
    "|3_day_Fib_61.8%|10.72%|\n",
    "|Open|7.62%|\n",
    "|3_day_Fib_23.6%|3.41%|\n",
    "|Rolling_Quantile_25_3|3.23%|\n",
    "\n",
    "Model 3 feature diversity\n",
    "|Feature|Model 3 Feature Diversity|\n",
    "|-|-|\n",
    "|Daily_High|24.68%|\n",
    "|Daily_Low|24.15%%|\n",
    "|Rolling_Quantile_25_3|14.06%|\n",
    "|Open|10.25%|\n",
    "|Daily_High_5day_avg|5.76%|\n",
    "|3_day_Fib_61.8%|4.45%|\n",
    "\n",
    "\n",
    "Model 4 feature diversity\n",
    "|Feature|Model 4 Feature Diversity|\n",
    "|-|-|\n",
    "|Daily_Low|33.44%|\n",
    "|Daily_High|30.65%|\n",
    "|3_day_Fib_61.8%|6.51%|\n",
    "|3_day_Fib_50.0%|6.13%|\n",
    "|Open|6.09%|\n",
    "|3_day_Fib_78.6%|3.02%|\n",
    "\n",
    "\n",
    "Upon comparing the three models with varying 'max_depth' hyperparameters, Model 3's (max_depth = 5) performance seems to align best with our goals. Although Model 4 performed best for the squared metrics (MSE and RMSE), Model 3 performed the best in the absolute metrics (Mean Absolute Error, Median Absolute Error, and MAPE). With Model 3 acheiving the best results, we will go forward with 'max_depth' = 3 and continue to the next hyperparameter.\n",
    "\n",
    "The next hyperparameter we are going to adjust is 'learning_rate'. 'learning_rate' controls the size of the step each boosting iteration takes when updating the model. It determines how much weight new trees should have when correcting previous errors. A 'learning_rate' that is too high will learn too quicky and overfit the training data, while a 'learning_rate' that is too low may learn too slowly and require many trees to reach optimal performance, increasing training time. Currently, our 'learning_rate' hyperparameter is set to 0.05. We are going to try setting the value to 0.1 and 0.01 and evaluate which 'learning_rate' value results in the best performance. \n",
    "\n",
    "#### Next Steps: Solidify max_depth as 3 moving forward and adjust hyperparameter learning_rate to the values 0.1 and 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6f02a-5dc2-4b11-b33a-4aed514e1a55",
   "metadata": {},
   "source": [
    "### Model 5: learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8deae-7fa7-4ac8-8ac8-600bbcaaa2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5: learning_rate = 0.01\n",
    "# colsample_bytree = 0.5\n",
    "# max_depth = 5\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "stock_data_1_week = stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until January 24 2023 for training\n",
    "stock_data_train_1_week_lr_01 = stock_data_1_week[stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from January 31 2023 onwards for testing\n",
    "stock_data_test_1_week_lr_01 = stock_data_1_week[stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "stock_data_train_1_week_lr_01['Close_Target'] = stock_data_train_1_week_lr_01.groupby('Symbol')['Close'].shift(-5)\n",
    "stock_data_test_1_week_lr_01['Close_Target'] = stock_data_test_1_week_lr_01.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "stock_data_train_1_week_lr_01 = stock_data_train_1_week_lr_01.dropna(subset=['Close_Target'])\n",
    "stock_data_test_1_week_lr_01 = stock_data_test_1_week_lr_01.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "stock_data_train_1_week_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "stock_data_test_1_week_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Filter DataFrame to numeric columns in order to fill NaN values\n",
    "numeric_cols_train = stock_data_train_1_week_lr_01.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = stock_data_test_1_week_lr_01.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Fill NaN values with the median of each numeric column\n",
    "stock_data_train_1_week_lr_01[numeric_cols_train] = stock_data_train_1_week_lr_01[numeric_cols_train].fillna(stock_data_train_1_week_lr_01[numeric_cols_train].median())\n",
    "stock_data_test_1_week_lr_01[numeric_cols_test] = stock_data_test_1_week_lr_01[numeric_cols_test].fillna(stock_data_test_1_week_lr_01[numeric_cols_test].median())\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_lr_01 = stock_data_train_1_week_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_lr_01 = stock_data_train_1_week_lr_01['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_lr_01 = stock_data_test_1_week_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_lr_01 = stock_data_test_1_week_lr_01['Close_Target']\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_week_lr_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan,  # Ensure missing values are handled correctly\n",
    "    colsample_bytree = 0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_week_lr_01.fit(X_train_1_week_lr_01, y_train_1_week_lr_01)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_1_week_lr_01 = model_1_week_lr_01.predict(X_test_1_week_lr_01)\n",
    "\n",
    "# Print upon completion of training\n",
    "print(\"Training for Model 5 is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f7d07-9367-43ff-ab48-b4f44670b32a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate performance metrics on Model 5 test data\n",
    "\n",
    "# Mean Absolute Error = average absolute difference between actual values and predicted values\n",
    "# Mean Squared Error = average squared difference between actual values and predicted values\n",
    "# Root Mean Squared Error = Square root of Mean Squared Error\n",
    "# R_Squared = Measures how well model explains variance in the target variable\n",
    "# Median Absolute Error = Median of Absolute differences between actual avlues and predicted values\n",
    "# Mean Absolute Percentage Error (MAPE) = Expresses Percentage error as a percentage of actual values\n",
    "# Durbin-Watson (DW) = Detects autocorrelation in residuals\n",
    "\n",
    "mse_1_week_lr_01 = mean_squared_error(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "mae_1_week_lr_01 = mean_absolute_error(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "rmse_1_week_lr_01 = np.sqrt(mse_1_week_lr_01)  # Root Mean Squared Error\n",
    "r2_1_week_lr_01 = r2_score(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "medae_week_lr_01 = median_absolute_error(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "mape_1_week_lr_01 = np.mean(np.abs((y_test_1_week_lr_01 - y_pred_1_week_lr_01) / y_test_1_week_lr_01)) * 100\n",
    "dw_stat_1_week_lr_01 = durbin_watson(y_test_1_week_lr_01 - y_pred_1_week_lr_01)\n",
    "\n",
    "# Print out the metrics for test data\n",
    "print(f'Mean Squared Error on test data: {mse_1_week_lr_01}')\n",
    "print(f'Mean Absolute Error on test data: {mae_1_week_lr_01}')\n",
    "print(f'Root Mean Squared Error on test data: {rmse_1_week_lr_01}')\n",
    "print(f'R-squared on test data: {r2_1_week_lr_01}')\n",
    "print(f'Median Absolute Error on test data: {medae_week_lr_01}')\n",
    "print(f'MAPE on test data: {mape_1_week_lr_01:.2f}%')\n",
    "print(f'Durbin-Watson Statistic on test data: {dw_stat_1_week_lr_01}')\n",
    "\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_lr_01 = dict(zip(X_train_1_week_lr_01.columns, model_1_week_lr_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_lr_01 = sorted(feature_importance_1_week_lr_01.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_lr_01:\n",
    "    if int(importance*100) >= 1:\n",
    "        print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb6e00a-2c0b-4694-8c87-a2d1863980a5",
   "metadata": {},
   "source": [
    "### Model 6: learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53d222-e712-4709-82eb-3999a328899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6: learning_rate = 0.1\n",
    "# colsample_bytree = 0.5\n",
    "# max_depth = 5\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "stock_data_1_week = stock_data_1_week .sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until January 24 2023 for training\n",
    "stock_data_train_1_week_lr_1 = stock_data_1_week [stock_data_1_week ['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from January 31 2023 onwards for testing\n",
    "stock_data_test_1_week_lr_1 = stock_data_1_week [stock_data_1_week ['Date'] > '2023-01-31']\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "stock_data_train_1_week_lr_1['Close_Target'] = stock_data_train_1_week_lr_1.groupby('Symbol')['Close'].shift(-5)\n",
    "stock_data_test_1_week_lr_1['Close_Target'] = stock_data_test_1_week_lr_1.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "stock_data_train_1_week_lr_1 = stock_data_train_1_week_lr_1.dropna(subset=['Close_Target'])\n",
    "stock_data_test_1_week_lr_1 = stock_data_test_1_week_lr_1.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "stock_data_train_1_week_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "stock_data_test_1_week_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Filter DataFrame to numeric columns in order to fill NaN values\n",
    "numeric_cols_train = stock_data_train_1_week_lr_1.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = stock_data_test_1_week_lr_1.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Fill NaN values with the median of each numeric column\n",
    "stock_data_train_1_week_lr_1[numeric_cols_train] = stock_data_train_1_week_lr_1[numeric_cols_train].fillna(stock_data_train_1_week_lr_1[numeric_cols_train].median())\n",
    "stock_data_test_1_week_lr_1[numeric_cols_test] = stock_data_test_1_week_lr_1[numeric_cols_test].fillna(stock_data_test_1_week_lr_1[numeric_cols_test].median())\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_lr_1 = stock_data_train_1_week_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_lr_1 = stock_data_train_1_week_lr_1['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_lr_1 = stock_data_test_1_week_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_lr_1 = stock_data_test_1_week_lr_1['Close_Target']\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_week_lr_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan,  # Ensure missing values are handled correctly\n",
    "    colsample_bytree = 0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_week_lr_1.fit(X_train_1_week_lr_1, y_train_1_week_lr_1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_1_week_lr_1 = model_1_week_lr_1.predict(X_test_1_week_lr_1)\n",
    "\n",
    "# Print upon completion of training\n",
    "print(\"Training for Model 6 is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c41240-ce28-461a-8ca2-e17fa249276b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate performance metrics on Model 6 test data\n",
    "\n",
    "# Mean Absolute Error = average absolute difference between actual values and predicted values\n",
    "# Mean Squared Error = average squared difference between actual values and predicted values\n",
    "# Root Mean Squared Error = Square root of Mean Squared Error\n",
    "# R_Squared = Measures how well model explains variance in the target variable\n",
    "# Median Absolute Error = Median of Absolute differences between actual avlues and predicted values\n",
    "# Mean Absolute Percentage Error (MAPE) = Expresses Percentage error as a percentage of actual values\n",
    "# Durbin-Watson (DW) = Detects autocorrelation in residuals\n",
    "\n",
    "mse_1_week_lr_1 = mean_squared_error(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "mae_1_week_lr_1 = mean_absolute_error(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "rmse_1_week_lr_1 = np.sqrt(mse_1_week_lr_1)  # Root Mean Squared Error\n",
    "r2_1_week_lr_1 = r2_score(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "medae_1_week_lr_1 = median_absolute_error(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "mape_1_week_lr_1 = np.mean(np.abs((y_test_1_week_lr_1 - y_pred_1_week_lr_1) / y_test_1_week_lr_1)) * 100\n",
    "dw_stat_1_week_lr_1 = durbin_watson(y_test_1_week_lr_1 - y_pred_1_week_lr_1)\n",
    "\n",
    "# Print out the metrics for test data\n",
    "print(f'Mean Squared Error on test data: {mse_1_week_lr_1}')\n",
    "print(f'Mean Absolute Error on test data: {mae_1_week_lr_1}')\n",
    "print(f'Root Mean Squared Error on test data: {rmse_1_week_lr_1}')\n",
    "print(f'R-squared on test data: {r2_1_week_lr_1}')\n",
    "print(f'Median Absolute Error on test data: {medae_1_week_lr_1}')\n",
    "print(f'MAPE on test data: {mape_1_week_lr_1:.2f}%')\n",
    "print(f'Durbin-Watson Statistic on test data: {dw_stat_1_week_lr_1}')\n",
    "\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_lr_1 = dict(zip(X_train_1_week_lr_1.columns, model_1_week_lr_1.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_lr_1 = sorted(feature_importance_1_week_lr_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_lr_1:\n",
    "    if int(importance*100) >= 1:\n",
    "        print(f\"{feature}: {importance * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d1e2f-4ced-4932-9af4-f698951b4200",
   "metadata": {},
   "source": [
    "| Performance Metric|Model 3|Model 4|Model 5| Top Performer\n",
    "|-|-|-|-|-|\n",
    "| R-Squared| 0.869199 | 0.86748| 0.86924| Model 6\n",
    "| Mape| 4.80%| 4.79% | 4.89%| Model 5\n",
    "| Root Mean Squared Error| 69.9469 | 70.403| 69.936 | Model 6\n",
    "| Mean Squared Error| 4892.5734 | 4956.63 | 4891.137 | Model 6\n",
    "| Mean Abolute Error| 21.429 | 21.4675 | 21.574 | Model 3\n",
    "| Median Absolute Error | 3.925 | 3.895 | 3.983 | Model 5\n",
    "\n",
    "\n",
    "\n",
    "|Feature|Model 3 Feature Diversity|\n",
    "|-|-|\n",
    "|Daily_High|24.68%|\n",
    "|Daily_Low|24.15%%|\n",
    "|Rolling_Quantile_25_3|14.06%|\n",
    "|Open|10.25%|\n",
    "|Daily_High_5day_avg|5.76%|\n",
    "|3_day_Fib_61.8%|4.45%|\n",
    "\n",
    "\n",
    "|Feature|Model 5 Feature Diversity|\n",
    "|-|-|\n",
    "|3_day_Fib_50.0%|24.68%|\n",
    "|3_day_Fib_61.8%|24.15%%|\n",
    "|Rolling_Quantile_25_3|14.06%|\n",
    "|Open|10.25%|\n",
    "|Daily_Low|5.76%|\n",
    "|5_day_Fib_61.8%|6.27%|\n",
    "|Daily_High|6.17%|\n",
    "|3_day_Fib_23.6%|4.47%|\n",
    "|5_day_Fib_23.6%|3.28%|\n",
    "|EMA_7|3.27%|\n",
    "|Lower_Band_3|3.08%|\n",
    "\n",
    "\n",
    "|Feature|Model 6 Diversity|\n",
    "|-|-|\n",
    "|Daily_Low|32.34%|\n",
    "|Daily_High|27.79|\n",
    "|_day_Fib_23.6%|20.21%|\n",
    "|Open|4.83%|\n",
    "\n",
    "\n",
    "Based on the results of our metrics, Model 3 and Model 5 had very close performances in the aboslute metrics. Model 5 had 0.01% better MAPE than Model 3, as well as a slightly better Median Absolute error. Model 3 had a significantly better Mean Absolute Error. Although Model 5 technically beat Model 3 in 2/3 of the absolute metrics, the values were fairly insigificant (4.80% vs. 4.79% MAPE and 3.895 vs. 3.925 Median Absolute Error. In arguably the most important absolute error metric, Mean Absolute Error, Model 3 performed significantly better at a value of 21.429, vs the 21.4675 which Model 5 had. If we then consider the squared metrics, Model 3 performs significantly better than Model 5 in R-Squared, Mean Squared Error, and Root Mean Squared Error. Because of this, Model 3 is our best performing model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d130af-4448-4c14-8447-613fd75e230b",
   "metadata": {},
   "source": [
    "### Final Model: Model 3\n",
    "Model 3 is going to be our final model. The finalized best values of the altered hyperparameters are below:\n",
    "* colsample_bytree = 0.5\n",
    "* max_depth = 3\n",
    "* learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6825b956-6beb-4fee-b2c4-9043e594d928",
   "metadata": {},
   "source": [
    "### Model 3 Top Features Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b727455-bcda-46ef-98ab-fcafa2f7d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy arrays (ensuring correct types) \n",
    "features = np.array([feature for feature, importance in sorted_features_1_week_md_3[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features_1_week_md_3[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=20)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 6)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=20, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=20, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Final Model: Top 5 Most Important Features\", fontsize=22, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=18)\n",
    "plt.xticks(fontsize=18)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c642528-8633-47b4-b180-2b658672da70",
   "metadata": {},
   "source": [
    "### Model Testing Using Trading Simulation\n",
    "Now that we have chosen our best model, we want to test how well it actually performs in as close to a real world situation as we can get: a  trading simulation. This will be a good test to see if our model is profitable, unprofitable, or breaks even on average across the 25 stocks we trained it on. Before we do that, we need to build a new DataFrame for the simulation to use. To begin building this DataFrame, we'll include our descriptive columns, 'Symbol' and 'Date', our target variable, 'Close', which will be renamed to 'Actual Price', and two new columns called 'Starting Price' and \"Predicted Price\". 'Starting Price' will just be the 'Actual Price' column shifted 5 days and 'Predicted Price' will be the predicted values from our final best performing model, model 3. Each row will contain the 'Actual Price' (the actual daily closing stock price on that date), the 'Starting Price' (the daily closing stock price from 5 trading days prior), and 'Predicted Price' (the daily closing price predicted for that day). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60ae23-873e-4237-9ed0-253149db45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of stock_data_1_week just incase we need to reference it later\n",
    "stock_data_1_week_test = stock_data_1_week.copy()\n",
    "\n",
    "# Make sure date column is in datetime format\n",
    "stock_data_1_week_test['Date'] = pd.to_datetime(stock_data_1_week_test['Date'], errors='coerce')\n",
    "\n",
    "# Drop all columns from stock_data_1_week_test except 'Symbol' 'Date', and 'Close'\n",
    "columns_to_keep = ['Symbol', 'Date', 'Close']\n",
    "stock_data_1_week_test.drop(columns = [col for col in stock_data_1_week_test.columns if col not in columns_to_keep], inplace=True)\n",
    "\n",
    "# Filter stock_data_1_week_test to only keep rows where 'Date' also exists in stock_data_test_1_week_md_3 DataFrame\n",
    "# Then reset index\n",
    "stock_data_1_week_test = stock_data_1_week_test[stock_data_1_week_test['Date'].isin(stock_data_test_1_week_md_3['Date'])]\n",
    "stock_data_1_week_test = stock_data_1_week_test.reset_index(drop=True)\n",
    "\n",
    "# y_pred_1_week_md_3 contains the predicted values for Model 3 test data\n",
    "# Convert it to a DataFrame\n",
    "predicted_values_1_week = pd.DataFrame(y_pred_1_week_md_3)\n",
    "\n",
    "# Combine stock_data_1_week_test and predicted_values_1_week so that we have\n",
    "# the actual values, predicted values, and descriptive columns all in one DataFrame\n",
    "pred_one_week = pd.concat([stock_data_1_week_test, predicted_values_1_week], axis=1)\n",
    "\n",
    "# Rename columns for readability\n",
    "pred_one_week = pred_one_week.rename(columns={0: 'Future Predicted Price',\n",
    "                                                         'Close': 'Actual Price',\n",
    "                                                         'Close_Target': 'Starting Price'})\n",
    "\n",
    "# Group by Symbol and shift prices within each Symbol group\n",
    "# Create 'Starting Price' column by copying 'Actual Price' and doing shift(5), so that\n",
    "# the current price and price 5 days prior are in the same row\n",
    "\n",
    "pred_one_week['Starting Price'] = pred_one_week.groupby('Symbol')['Actual Price'].shift(5)\n",
    "pred_one_week[\"Today's Predicted Price\"] = pred_one_week.groupby('Symbol')['Future Predicted Price'].shift(5)\n",
    "pred_one_week = pred_one_week.dropna(subset=['Future Predicted Price', 'Starting Price']).reset_index(drop=True)\n",
    "pred_one_week = pred_one_week[['Symbol', 'Date', 'Actual Price', 'Starting Price', \"Today's Predicted Price\", 'Future Predicted Price']]\n",
    "pred_one_week.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0333ed-d796-4c5b-8cbc-9cc4d54f44a1",
   "metadata": {},
   "source": [
    "### Performance Measures and Hedging Options\n",
    "To further evaluate our model, we will calculate the bullish and bearish values for mean error, standard deviation of the mean error, and predicted rate of return. These performance evaluations are not only useful to judge the effectiveness of our model and deduce interesting statistical conclusions, but also useful to create a hedge strategy based on the errors of our model. Hedging is a way to reduce risk by making another trade to offset potential losses. For example, if our model predicts a price will go up, we can hedge by taking a small portion of our 'Long' position and put it into a 'Short' position. This way, we limit losses if the trade does not go our way. Since we don't want to accidently give future data to trading simulation, each \"mean\" value is really just a rolling average of the previous 10 values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d11829f-1833-405d-9ec7-d772dada5b7e",
   "metadata": {},
   "source": [
    "### Mean Error\n",
    "Mean error measures how far off the predicted price is from the actual price, as a percentage of the starting price. Evaluating this for both bullish and bearish scenarios can tell us if the model systematically overestimates or underestimates price movements. Making sure to group by 'Symbol', we calculate both bullish mean error and bearish mean error. For both mean errors, we'll then create a 10-day rolling average to smooth out fluctuations over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115d575-7149-4c90-b366-80d0688514be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and create columns for Bullish and Bearish Mean Error\n",
    "\n",
    "bullish_errors = [] # List to store bullish predictions\n",
    "bearish_errors = [] # List to store Bearish Predictions\n",
    "\n",
    "# Group predictions by stock symbol\n",
    "for symbol, group in pred_one_week.groupby('Symbol'):\n",
    "\n",
    "    # Shift Predicted, Actual, and Starting price to prevent future data leakage\n",
    "    group['Shifted Predicted Price'] = group[\"Today's Predicted Price\"].shift(1)\n",
    "    group['Shifted Actual Price'] = group['Actual Price'].shift(1)\n",
    "    group['Shifted Starting Price'] = group['Starting Price'].shift(1)   \n",
    "\n",
    "    # Iterate through each row and confirm actual price direction and predicted price direction\n",
    "    for i, row in group.iterrows():\n",
    "        actual_bull = row['Shifted Actual Price'] > row['Shifted Starting Price']\n",
    "        actual_bear = row['Shifted Actual Price'] < row['Shifted Starting Price']\n",
    "        predicted_bull = row['Shifted Predicted Price'] > row['Shifted Starting Price']\n",
    "        predicted_bear = row['Shifted Predicted Price'] < row['Shifted Starting Price']\n",
    "\n",
    "        # If actual price and predicted price are both bullish, calculate bullish error\n",
    "        if actual_bull and predicted_bull:\n",
    "            error_bull = (row['Shifted Actual Price'] - row['Shifted Predicted Price']) / row['Shifted Starting Price']\n",
    "            bullish_errors.append(error_bull)\n",
    "        else:\n",
    "            bullish_errors.append(0) # If not, append 0. Need to do this to maintain same index/row length\n",
    "\n",
    "        # If actual price and predicted price are both bearish, calculate bearish error\n",
    "        if actual_bear and predicted_bear:\n",
    "            error_bear = (row['Shifted Actual Price'] - row['Shifted Predicted Price']) / row['Shifted Starting Price']\n",
    "            bearish_errors.append(error_bear)\n",
    "        else:\n",
    "            bearish_errors.append(0)\n",
    "\n",
    "# Add calculated Bullish/Bearish errors to DataFrame\n",
    "pred_one_week['Bullish Error'] = bullish_errors\n",
    "pred_one_week['Bearish Error'] = bearish_errors\n",
    "\n",
    "# Calculate expanding averages within each symbol's group\n",
    "pred_one_week['Rolling Bullish Mean Error'] = pred_one_week.groupby(\n",
    "    'Symbol')['Bullish Error'].transform(lambda x: x.rolling(window=10, min_periods=1).mean())\n",
    "\n",
    "pred_one_week['Rolling Bearish Mean Error'] = pred_one_week.groupby(\n",
    "    'Symbol')['Bearish Error'].transform(lambda x: x.rolling(window=10, min_periods=1).mean())\n",
    "\n",
    "pred_one_week.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc60b40-4df4-4c0c-bf3f-d43854972594",
   "metadata": {},
   "source": [
    "### Standard Deviation of the Mean Error\n",
    "Using the bullish and bearish mean error we just calculated, we can find the standard deviation of the bullish and bearish mean errors. This tells us how much the values in the dataset deviate from the mean. If standard deviations are far from the mean, it means the data is volatile and therefore more caution should be taken with the trade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084bf5f-d58c-43ff-bc33-7d5edb48062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Standard Deviation of the Error and rolling MA of that value\n",
    "# standard deviation: tells you how much the values in a dataset deviate from the mean\n",
    "# it tells you how spread out the data is; whether its stable or volatile\n",
    "\n",
    "# Calculate rolling standard deviation for bullish mean error with a window size of 10\n",
    "pred_one_week['Rolling Bullish STD'] = pred_one_week.groupby('Symbol')['Bullish Error'].transform(lambda x: x.rolling(window=10, min_periods=1).std())\n",
    "\n",
    "# Calculate rolling standard deviation for bearish mean error with a window size of 10\n",
    "pred_one_week['Rolling Bearish STD'] = pred_one_week.groupby('Symbol')['Bearish Error'].transform(lambda x: x.rolling(window=10, min_periods=1).std())\n",
    "\n",
    "# Backfill missing values in the rolling bullish standard deviation column within each symbol\n",
    "pred_one_week['Rolling Bullish STD'] = pred_one_week.groupby('Symbol')['Rolling Bullish STD'].apply(lambda group: group.fillna(method='bfill')).reset_index(level=0, drop=True)\n",
    "\n",
    "# Backfill missing values in the rolling bearish standard deviation column within each symbol\n",
    "pred_one_week['Rolling Bearish STD'] = pred_one_week.groupby('Symbol')['Rolling Bearish STD'].apply(lambda group: group.fillna(method='bfill')).reset_index(level=0, drop=True)\n",
    "\n",
    "pred_one_week.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795f246-e696-4643-9ba0-cfac506860b5",
   "metadata": {},
   "source": [
    "### Rate of Return\n",
    "Whether the model predicts a bullish (price will increase) or bearish (price will decrease) trade, a rate of return can be calculated. If the model predicted price will go up and it did, the rate of return will be positive. If the actual price did not go up, the rate of return will be negative. if the model predicted price will go down and it did, the rate of return will be positive. If the actual price did not decrease, the rate of return will be negative. This is calculated for both bullish and bearish scenarios and a 10day rolling average window is added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eae702-71b1-4c4e-b4ec-12a4d0d7d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to calculate Rate of Return of based on predicted values\n",
    "\n",
    "def ror(df):\n",
    "    # Identify Bullish and Bearish moves\n",
    "    bullish_move = df['Actual Price'] > df['Starting Price']\n",
    "    bearish_move = df['Actual Price'] < df['Starting Price']\n",
    "    \n",
    "    # For bullish moves, calculate ROR\n",
    "    df.loc[bullish_move, 'Predicted ROR'] = (df.loc[bullish_move, \"Today's Predicted Price\"] - df.loc[bullish_move, 'Starting Price']) / df.loc[bullish_move, 'Starting Price']\n",
    "    \n",
    "    # For bearish moves, calculate ROR\n",
    "    df.loc[bearish_move, 'Predicted ROR'] = (df.loc[bearish_move, 'Starting Price'] - df.loc[bearish_move, \"Today's Predicted Price\"]) / df.loc[bearish_move, 'Starting Price']\n",
    "    \n",
    "    # Calculate Rolling Predicted ROR within each symbol over a 10-day window\n",
    "    df['Rolling Predicted ROR'] = df.groupby('Symbol')['Predicted ROR'].transform(lambda x: x.rolling(window=10, min_periods=1).mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply ror() function to pred_one_week DataFrame\n",
    "pred_one_week = ror(pred_one_week)\n",
    "\n",
    "# Shift Rolling Predicted ROR by 1 to prevent future data leakage\n",
    "pred_one_week['Rolling Predicted ROR'] = pred_one_week['Rolling Predicted ROR'].shift(1)\n",
    "\n",
    "# Backward fill NaN values in Rolling Predicted ROR\n",
    "pred_one_week['Rolling Predicted ROR'] = pred_one_week.groupby('Symbol')['Rolling Predicted ROR'].apply(\n",
    "    lambda group: group.fillna(method='bfill', limit=1)\n",
    ").reset_index(level=0, drop=True)  \n",
    "\n",
    "pred_one_week.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c1cf1-bfc4-447c-9981-44b8ddca63ab",
   "metadata": {},
   "source": [
    "### Stock Borrow Fee and Trade Fee\n",
    "In scenarios where our stock is a bearish prediction or we are hedging to the downside to protect losses, we will be shorting stocks, which means we borrow stocks from a broker, sell the shares, then buy them back at a later date (5 trading days later), and return the stocks to the broker. In an ideal situation, we borrow the stocks from the broker, sell them, the price of the stocks decreases, we then buy the stocks back for a lower price, return the stocks to the broker, and profit from the difference. If the stocks increase during this time, we lose profit. In shorting, the period for which you are holding the stocks comes with a fee called a stock borrow fee, which is typically around 3% APY. To add accuracy to our simulation, we calculate how many days we are holding each short trade and incorporate the stock borrow fee in our trade. \n",
    "\n",
    "On top of this, each broker charges a fee to make a trade. The fee is typically around 0.25% per trade. Therefore, calculated into each trade will be a trade fee of 0.25%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabb87e-f3d3-4193-a53b-cf66af6f4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate length of each trade (in days) 5 trading days apart\n",
    "\n",
    "# Using average number of trading days in a calendar year\n",
    "# calculate average trading days per week and average non-trading days per week\n",
    "# Add them together to get the average length of holding a trade\n",
    "# This is to calculate how many days we would be charged for a stock borrow fee\n",
    "# in a shorting situation\n",
    "avg_trading_days_per_calendar_year = 252\n",
    "non_trading_days_per_year = 365 - 252\n",
    "avg_non_trading_days_per_week = (non_trading_days_per_year / (365)) * 7\n",
    "avg_trading_days_per_calendar_day = (avg_trading_days_per_calendar_year / 365)\n",
    "avg_trading_days_1_week = avg_trading_days_per_calendar_day * 7\n",
    "avg_days_holding_trade_1_week = avg_trading_days_1_week + avg_non_trading_days_per_week\n",
    "\n",
    "print(f'Average length holding 1 week trade: {avg_days_holding_trade_1_week}')\n",
    "\n",
    "# Create function to calculate how many calendar days are between each trade\n",
    "def length_holding_trade(df, avg_days_holding_trade_1_week):\n",
    "    # Ensure 'Date' is in datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Shift 'Date' by 5 trading days within each 'Symbol'\n",
    "    df['Trade Entry Date'] = df.groupby('Symbol')['Date'].shift(5)\n",
    "\n",
    "    # Calculate actual holding time in days\n",
    "    df['Lengh of Trade (Days)'] = (df['Date'] - df['Trade Entry Date']).dt.days\n",
    "\n",
    "    # Fill in missing values with the calculated average holding duration\n",
    "    df['Lengh of Trade (Days)'].fillna(avg_days_holding_trade_1_week, inplace=True)\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df.drop(columns=['Trade Entry Date'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply length_holding_trade function to pred_one_week dataset\n",
    "pred_one_week = length_holding_trade(pred_one_week, avg_days_holding_trade_1_week)\n",
    "\n",
    "pred_one_week.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a82a1a-3b35-41ed-a919-060e50cba861",
   "metadata": {},
   "source": [
    "### Bullish and Bearish Hedge\n",
    "Our simple hedge strategy is going to utilize bullish/bearish mean error, bullish/bearish standard deviation, and predicted rate of return. If the model's prediction is bullish, we add up the values of bullish mean error, bullish standard deviation, and predicted rate of return. If bullish mean error is > 0, it is left out of the hedge and if predicted rate of return is > 0, it is also left out of the hedge. If the model's predicted is bearish, we add up the values just like the bullish prediction. However, if bearish mean error is < 0, it is left out of the hedge and if predicted rate of return is > 0, it is also left out of the hedge. This gives us two new columns 'Bullish Hedge' and 'Bearish Hedge' which we can use in our trading simulation to easily add a hedge to our trades. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8665d-3053-459a-a9c8-86448d1ec6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bullish and Bearish Hedge columns with calculated hedge values\n",
    "\n",
    "# Initialize empty lists to store hedge values\n",
    "bullish_hedge = []\n",
    "bearish_hedge = []\n",
    "\n",
    "# Iterate over each stock symbol in the dataset\n",
    "for symbol, group in pred_one_week.groupby('Symbol'):\n",
    "\n",
    "    # Shift columns to only use previous rows value's and prevent future data leakage\n",
    "    group['Shifted Predicted Price'] = group[\"Today's Predicted Price\"].shift(1)\n",
    "    group['Shifted Actual Price'] = group['Actual Price'].shift(1)\n",
    "    group['Shifted Starting Price'] = group['Starting Price'].shift(1)\n",
    "\n",
    "    # Iterate over each row in the group\n",
    "    for i, row in group.iterrows():\n",
    "        # Determine if the Predicted Price indicates a Bullish or Bearish move\n",
    "        bullish_pred = row['Shifted Predicted Price'] > row['Shifted Starting Price']\n",
    "        bearish_pred = row['Shifted Predicted Price'] < row['Shifted Starting Price']\n",
    "\n",
    "        # Extract statistical values to build hedge\n",
    "        bullish_mean_error = row['Rolling Bullish Mean Error']\n",
    "        bearish_mean_error = row['Rolling Bearish Mean Error']\n",
    "        bullish_stdev = row['Rolling Bullish STD']\n",
    "        bearish_stdev = row['Rolling Bearish STD']\n",
    "        pred_ror = row['Rolling Predicted ROR']\n",
    "\n",
    "        # Calculate hedge in bullish scenario\n",
    "        if bullish_pred:\n",
    "            hedge = 0\n",
    "            hedge += bullish_stdev # Add Bullish Standard Deviation\n",
    "            if bullish_mean_error < 0:\n",
    "                hedge += abs(bullish_mean_error) # Add Bullish Mean Error if it\n",
    "                                                 # has a value less than 0\n",
    "            if pred_ror < 0:\n",
    "                hedge += abs(pred_ror)           # Add Predicted ROR value if it\n",
    "                                                 # is less than 0            \n",
    "            bullish_hedge.append(hedge) \n",
    "        else:\n",
    "            bullish_hedge.append(0) # If trade is not bullish, hedge value is 0\n",
    "\n",
    "        # Calculate hedge in bearish scenario\n",
    "        if bearish_pred:\n",
    "            hedge = 0\n",
    "            hedge += bearish_stdev # Add Bearish Standard Deviation\n",
    "            if bearish_mean_error > 0:\n",
    "                hedge += bearish_mean_error # Add Bearish Mean Error if it\n",
    "                                            # has a value greater than 0\n",
    "            if pred_ror < 0:\n",
    "                hedge += abs(pred_ror)      # Add Predicted ROR value if it\n",
    "                                            # is less than 0            \n",
    "            bearish_hedge.append(hedge)     \n",
    "        else:\n",
    "            bearish_hedge.append(0)         # If trade is not Bearish, hedge value is 0\n",
    "\n",
    "# Add calculated hedge values as new columns in our DataFrame\n",
    "pred_one_week['Bullish Hedge'] = bullish_hedge\n",
    "pred_one_week['Bearish Hedge'] = bearish_hedge\n",
    "pred_one_week.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e924be2-f54a-429f-8edb-1d3375823655",
   "metadata": {},
   "source": [
    "### Trading Simulation: Logic\n",
    "We initalize a trading account to $100,000 to start and each trade uses 5% of our portfolio. Each trade is charged a trade fee and if shorting, that  position is charged a stock borrow fee. In order to limit the number of trades, logic has been added to skip days before trading. If num_iteration_skips is > 0, it will loop through the specified number of rows until skip_iteration resets to 0, then will calcualte that trade. This explanation is under the assumption that num_iteration_skips is 0. This trading simulation works by grouping by stock ticker and iterating through each row. Upon each iteration, if the model's predicted price is greater than the starting price (price one week ago), then the trade is bullish. If the model's predicted price is less than the starting price, then the trade is bearish. \n",
    "\n",
    "If the trade is bullish, 5% of current portfolio will be calculated and we find the max number of shares we can buy for the price of the starting value. We then execute that trade by multiplying number of shares we have by starting price, subtracting the total cost of that trade from our trading account value, then, we calculate the end of our position by multiplying our the number of shares we bought by the actual price. We then add the value of that back to our trading account. \n",
    "\n",
    "If the trade is bearish, the same logic will be used to find the number of shares to trade, except this time since we are shorting, we borrow the shares to sell. The value of the short is added to our trading account, then those shares are multiplied by actual price, and that value is subtracted from our trading account. The profit/loss is kept track of, as well as the number of total bullish and bearish trades, the positive bullish/bearish and negative bullish/bearish trades, and the percent gain/loss. \n",
    "\n",
    "Two simluations will be run, one without our hedging strategy and one with. How hedging works is once a position is established, whether bullish or bearish, if the hedge column for that direction (bull_hedge or bear_hedge) is > 0, that value is multiplied by a risk multiplier (set to 1 by default), the hedge portion is then multiplied by the total trade amount to calculate the hedge portion of the trade. That hedge portion is then subtracted from the main position to get the reduced main trade value and hedge trade value For example, if it is a bullish trade and bull_hedge = 0.07, bull_hedge * risk multipler (3) = 0.21 hedge portion. Bullish trade is now 1.00 - 0.21 = 0.79. In this scenario, 79% of the full trading position enters a bullish position (buys shares), and the remaining 21% enters a bearish position (short sell shares). The goal of this hedge is to try to realistically account for errors in the model, as well as manage risk by limiting loss from trade. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4278b7-3deb-442e-9b61-beff670b414f",
   "metadata": {},
   "source": [
    "### Trading Simulation with no hedge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b558d-6634-487c-adcc-87a39d208b3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trade Simulation with no hedge\n",
    "# risk_mult = 0\n",
    "\n",
    "# Initialize list to store overall results of trading simulation\n",
    "overall_results = []\n",
    "\n",
    "# Group the dataframe by 'Symbol' and run the simulation for each stock separately\n",
    "for symbol, group in pred_one_week.groupby('Symbol'):\n",
    "    print(f\"\\nStarting trading simulation for {symbol}\\n\")\n",
    "\n",
    "    # Reset trading account for each stock\n",
    "    trading_account = 100000\n",
    "    profit_loss = 0\n",
    "    bullish_trade_count = 0\n",
    "    bearish_trade_count = 0\n",
    "    positive_bullish_trade = 0\n",
    "    negative_bullish_trade = 0\n",
    "    positive_bearish_trade = 0\n",
    "    negative_bearish_trade = 0\n",
    "    percent_gain = (profit_loss / 100000) * 100\n",
    "    skip_iterations = 0\n",
    "\n",
    "    # Loop through each row (each row represents one trade)\n",
    "    for i, row in group.iterrows():\n",
    "\n",
    "        # Extract relevant features for trade\n",
    "        starting_price = row['Starting Price']\n",
    "        actual_price = row['Actual Price']\n",
    "        predicted_price = row[\"Today's Predicted Price\"]\n",
    "        trade_date = row['Date']\n",
    "        length_holding_trade = row['Lengh of Trade (Days)']\n",
    "        bull_hedge = row['Bullish Hedge']\n",
    "        bear_hedge = row['Bearish Hedge']\n",
    "\n",
    "        # Determine trade predictions (bullish or bearish)\n",
    "        bullish_pred = predicted_price > starting_price\n",
    "        actual_bull = actual_price > starting_price\n",
    "        bearish_pred = predicted_price < starting_price\n",
    "        actual_bear = actual_price < starting_price\n",
    "        \n",
    "        # % Portfolio risked per trade\n",
    "        portfolio_trade_percent = 0.05\n",
    "      \n",
    "        # Iteration Skipper: Limit Number of trades over testing timeframe\n",
    "        num_iteration_skips = 0 \n",
    "        if skip_iterations > 0:\n",
    "            skip_iterations -= 1\n",
    "            continue\n",
    "\n",
    "        # Position size per trade        \n",
    "        trade_size = trading_account * portfolio_trade_percent\n",
    "\n",
    "        # Trade fee: trade_size * trade_fee\n",
    "        trade_fee = 0.0025\n",
    "\n",
    "        # Stock borrow fee for short trades\n",
    "        stock_borrow_fee_percent = 0.03\n",
    "        total_days_one_year = 365\n",
    "        stock_borrow_daily_fee = stock_borrow_fee_percent / total_days_one_year\n",
    "\n",
    "        # Leverage and margin fee\n",
    "        leverage = 1\n",
    "        margin_rate = 0.12\n",
    "        margin_daily_rate = margin_rate / total_days_one_year\n",
    "\n",
    "        # Risk multiplier for hedging\n",
    "        risk_mult = 0\n",
    "\n",
    "\n",
    "        \n",
    "        # Bullish Prediction trading logic     \n",
    "        if bullish_pred:\n",
    "            \n",
    "            bullish_trade_count += 1\n",
    "            total_trade_position_value = trade_size\n",
    "            hedge_portion = 0\n",
    "            bull_hedge_starting_trade_cost = 0\n",
    "            bull_hedge_starting_trade_fee = 0\n",
    "            if bull_hedge > 0:\n",
    "                hedge_portion = bull_hedge * risk_mult\n",
    "\n",
    "                # Cap hedge at 50%\n",
    "                hedge_portion = min(hedge_portion, 0.25)\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            bull_hedge_starting_value = total_trade_position_value * hedge_portion\n",
    "            bull_hedge_starting_share_amount = bull_hedge_starting_value // starting_price\n",
    "\n",
    "            # Calculate trade fee and stock borrow fee for bullish hedge\n",
    "            if bull_hedge_starting_share_amount > 0: \n",
    "                bull_hedge_starting_trade_cost = bull_hedge_starting_share_amount * starting_price\n",
    "                bull_hedge_starting_trade_fee = bull_hedge_starting_trade_cost * trade_fee\n",
    "            else:\n",
    "                bull_hedge_starting_value = 0\n",
    "                \n",
    "            # Calculate bullish trade value, number of shares to purchase, total cost of trade, and trade fee\n",
    "            bull_starting_trade_value = total_trade_position_value - bull_hedge_starting_value\n",
    "            bull_starting_share_amount = bull_starting_trade_value // starting_price\n",
    "            bull_starting_trade_cost = bull_starting_share_amount * starting_price\n",
    "            bull_starting_trade_fee = bull_starting_trade_cost * trade_fee\n",
    "\n",
    "            # Adjust trading_account with bullish trade and hedge trade (if there is a hedge)\n",
    "            trading_account -= (bull_starting_trade_cost + bull_starting_trade_fee)\n",
    "            trading_account += (bull_hedge_starting_trade_cost - bull_hedge_starting_trade_fee)\n",
    "\n",
    "            # Calculate and update final trade values for hedge trade\n",
    "            bull_hedge_actual_trade_value = (bull_hedge_starting_share_amount * actual_price)\n",
    "            bull_hedge_actual_trade_fee = bull_hedge_actual_trade_value * trade_fee\n",
    "            bull_hedge_stock_borrow_fee = bull_hedge_starting_trade_cost * (stock_borrow_daily_fee * length_holding_trade)\n",
    "\n",
    "            # Update trading_account with end of hedge trade\n",
    "            trading_account -= (bull_hedge_actual_trade_value + bull_hedge_actual_trade_fee + bull_hedge_stock_borrow_fee)\n",
    "\n",
    "            # Calculate and update final trade values for hedge trade\n",
    "            bull_actual_trade_sell = bull_starting_share_amount * actual_price\n",
    "            bull_actual_trade_fee = bull_actual_trade_sell * trade_fee\n",
    "\n",
    "            # Update trading_account with end of bull trade\n",
    "            trading_account += (bull_actual_trade_sell - bull_actual_trade_fee)\n",
    "\n",
    "            # Calculate bullish_profit for bull trade and bearish_profit for hedge trade\n",
    "            bearish_profit = (bull_hedge_starting_trade_cost - bull_hedge_starting_trade_fee) - (\n",
    "                bull_hedge_actual_trade_value + bull_hedge_actual_trade_fee + bull_hedge_stock_borrow_fee)\n",
    "\n",
    "            bullish_profit = ((bull_actual_trade_sell - bull_actual_trade_fee) - (\n",
    "                bull_starting_trade_cost + bull_starting_trade_fee))\n",
    "\n",
    "            # Add both bearish profit and bullish profit to cumulating profit_loss bucket\n",
    "            profit_loss += bearish_profit\n",
    "            profit_loss += bullish_profit\n",
    "\n",
    "            # If total profit (bullish profit + bearish profit) > 0, add 1 count to positive bullish trade\n",
    "            # If total profit is less than 0, add 1 count to negative bullish trade\n",
    "            if (bullish_profit + bearish_profit) > 0:\n",
    "                positive_bullish_trade += 1\n",
    "            else:\n",
    "                negative_bullish_trade += 1\n",
    "\n",
    "            \n",
    "\n",
    "        # Bearish Prediction trading logic       \n",
    "        elif bearish_pred:\n",
    "\n",
    "            bearish_trade_count += 1\n",
    "            total_trade_position_value = trade_size\n",
    "            hedge_portion = 0\n",
    "            bear_hedge_starting_trade_cost = 0\n",
    "            bear_hedge_starting_trade_fee = 0\n",
    "            if bear_hedge > 0:\n",
    "                hedge_portion =  bear_hedge * risk_mult\n",
    "\n",
    "                # Cap hedge at 50%\n",
    "                hedge_portion = min(hedge_portion, 0.25)\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            bear_hedge_starting_value = total_trade_position_value * hedge_portion\n",
    "            bear_hedge_starting_share_amount = bear_hedge_starting_value // starting_price\n",
    "\n",
    "            # Calculate cost of bearish hedge trade\n",
    "            bear_hedge_starting_trade_cost = 0\n",
    "            if bear_hedge_starting_share_amount > 0: \n",
    "                bear_hedge_starting_trade_cost = bear_hedge_starting_share_amount * starting_price\n",
    "                bear_hedge_starting_trade_fee = bear_hedge_starting_trade_cost * trade_fee\n",
    "            else:\n",
    "                bear_hedge_starting_value = 0\n",
    "\n",
    "            # Update trade values, get number of shares, get total trade cost, and calculate trade fee of bearish trade\n",
    "            bear_starting_trade_value = total_trade_position_value - bear_hedge_starting_value\n",
    "            bear_starting_share_amount = bear_starting_trade_value // starting_price\n",
    "            bear_starting_trade_cost = bear_starting_share_amount * starting_price\n",
    "            bear_starting_trade_fee = bear_starting_trade_cost * trade_fee\n",
    "\n",
    "            # Adjust trading_account with bear trade value and hedge trade value\n",
    "            trading_account -= (bear_hedge_starting_trade_cost + bear_hedge_starting_trade_fee)\n",
    "            trading_account += (bear_starting_trade_cost - bear_starting_trade_fee)\n",
    "\n",
    "            # Calculate final hedge trade values\n",
    "            bear_hedge_actual_trade_value = (bear_hedge_starting_share_amount * actual_price)\n",
    "            bear_hedge_actual_trade_fee = bear_hedge_actual_trade_value * trade_fee\n",
    "\n",
    "            # Adjust trading_account with end of hedge trade value \n",
    "            trading_account += (bear_hedge_actual_trade_value - bear_hedge_actual_trade_fee)\n",
    "            \n",
    "            # Calculate final bearish trade values\n",
    "            bear_actual_trade_value = bear_starting_share_amount * actual_price\n",
    "            bear_actual_trade_fee = bear_actual_trade_value * trade_fee\n",
    "            bear_actual_stock_borrow_fee = bear_hedge_starting_trade_cost * (stock_borrow_daily_fee * length_holding_trade)\n",
    "\n",
    "            # Adjust trading_account with end of bearish trade value\n",
    "            trading_account -= (bear_actual_trade_value + bear_actual_trade_fee + bear_actual_stock_borrow_fee)\n",
    "\n",
    "            # Calculate bearish profit (profits from bearish trade)\n",
    "            # and bullish profits (profits from hedge)\n",
    "            bearish_profit = (bear_starting_trade_cost - bear_starting_trade_fee) - (\n",
    "                bear_actual_trade_value + bear_actual_trade_fee + bear_actual_stock_borrow_fee)\n",
    "                  \n",
    "            bullish_profit = ((bear_hedge_actual_trade_value - bear_hedge_actual_trade_fee) - (\n",
    "                bear_hedge_starting_trade_cost + bear_hedge_starting_trade_fee))\n",
    "\n",
    "            # Add bearish and bullish profit to cumulating profit_loss bucket\n",
    "            profit_loss += bearish_profit\n",
    "            profit_loss += bullish_profit\n",
    "\n",
    "            # If bearish profit + bullish profit > 0, add 1 count of positive bearish trade\n",
    "            # If bearish profit + bullish profit is < 0, add 1 count of negative bearish trade\n",
    "            if (bearish_profit + bullish_profit) > 0:\n",
    "                positive_bearish_trade += 1\n",
    "            else:\n",
    "                negative_bearish_trade += 1\n",
    "                    \n",
    "\n",
    "        # Iteration Skipping (limits number of trades if num_iteration_skips > 0)\n",
    "        skip_iterations = num_iteration_skips\n",
    "\n",
    "        # Check if account has dropped below $75,000 and stop the simulation\n",
    "        if trading_account <= 75000:\n",
    "            print(f\"Trading account for {symbol} has dropped below $75,000. Stopping simulation.\")\n",
    "            break\n",
    "\n",
    "    # Calculate percent gain\n",
    "    percent_gain = (profit_loss / 100000) * 100\n",
    "    # Store results for each stock symbol\n",
    "    result = {\n",
    "        'Symbol': symbol,\n",
    "        'Ending Account Value': trading_account,\n",
    "        'Total Profit/Loss': profit_loss,\n",
    "        'Bullish Trades': bullish_trade_count,\n",
    "        'Bearish Trades': bearish_trade_count,\n",
    "        'Positive Bullish Trades': positive_bullish_trade,\n",
    "        'Negative Bullish Trades': negative_bullish_trade,\n",
    "        'Positive Bearish Trades': positive_bearish_trade,\n",
    "        'Negative Bearish Trades': negative_bearish_trade,\n",
    "        'Percent Gain/Loss': percent_gain\n",
    "    }\n",
    "    overall_results.append(result)\n",
    "\n",
    "    # Print results for the current stock\n",
    "    print(f\"\\nResults for {symbol}:\")\n",
    "    print(f'Trade Account Ending Value: ${trading_account:.2f}')\n",
    "    print(f'Total Profit/Loss: ${profit_loss:.2f}')\n",
    "    print(f'Number of Bullish Trades: {bullish_trade_count}')\n",
    "    print(f'Number of Bearish Trades: {bearish_trade_count}')\n",
    "    print(f'Number of Positive Bullish Trades: {positive_bullish_trade}')\n",
    "    print(f'Number of Negative Bullish Trades: {negative_bullish_trade}')\n",
    "    print(f'Number of Positive Bearish Trades: {positive_bearish_trade}')\n",
    "    print(f'Number of Negative Bearish Trades: {negative_bearish_trade}')\n",
    "    print(f'Percentage Gain: {(profit_loss / 100000) * 100}')\n",
    "    \n",
    "# Convert overall results to a DataFrame for easier analysis\n",
    "trade_simulation_results = pd.DataFrame(overall_results)\n",
    "print(\"\\nFinal summary of all stocks:\")\n",
    "trade_simulation_results = trade_simulation_results.round(2)\n",
    "trade_simulation_results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c664dc8d-e321-4511-bd8d-984355ef859f",
   "metadata": {},
   "source": [
    "### Trade Simulation with No Hedge: Process Data Results.\n",
    "The results of the simulation are turned into a dataframe which include the stock ticker, the ending account value, the total profit/loss, the number of bullish and bearish trades, the nmber of negative bullish and bearish trades, the number of positive bullish and bearish trades, and the Percent Gain/Loss. Two additional columns are added: Total Profit/Loss and Ending Account Value are copied and put into currency format for readability. \n",
    "\n",
    "Using the first and last date of our testing data, we calculate Average Percent Return Per Year and Average Profit Per Trade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd75f01-9ac6-4c72-8a9f-5fa4a2a8d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of 'Total Profit/Loss' column but formatted to display as currency for readability\n",
    "trade_simulation_results['Total Profit/Loss ($)'] = (\n",
    "    trade_simulation_results['Total Profit/Loss'].apply(\n",
    "        lambda x: f\"${x:,.2f}\")\n",
    ")\n",
    "\n",
    "# Create copy of 'Ending Account Value' column but formatted to display as currency for readability\n",
    "trade_simulation_results['Ending Account Value ($)'] = (\n",
    "    trade_simulation_results['Ending Account Value'].apply(\n",
    "        lambda x: f\"${x:,.2f}\")\n",
    ")\n",
    "\n",
    "# Select specific columns and reorder to display final results of trading simulation with no hedge\n",
    "trade_simulation_results = (\n",
    "    trade_simulation_results[['Symbol', 'Total Profit/Loss ($)', \n",
    "                              'Ending Account Value ($)', 'Ending Account Value', \n",
    "                              'Total Profit/Loss', 'Bullish Trades', \n",
    "                              'Bearish Trades', 'Positive Bullish Trades', \n",
    "                              'Negative Bullish Trades', 'Positive Bearish Trades', \n",
    "                              'Negative Bearish Trades','Percent Gain/Loss',]]\n",
    ")\n",
    "\n",
    "trade_simulation_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f942699-7e95-4982-ac64-c34807cb0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of trade_simluation_results DataFrame\n",
    "one_week_trade_simulation = trade_simulation_results.copy()\n",
    "\n",
    "# Define start date and end date of the trade simulation\n",
    "start_date = datetime(2023, 2, 8)\n",
    "end_date = datetime(2025, 2, 25)\n",
    "\n",
    "# Calculate total number of trading days between start date and end date\n",
    "duration_of_trading_days = (end_date - start_date).days\n",
    "\n",
    "# Convert trading duration from days to years\n",
    "duration_of_trading_years = duration_of_trading_days / 365.25\n",
    "\n",
    "# Starting value of trade account is $100,000\n",
    "starting_trade_account_value = 100000\n",
    "\n",
    "# Calculate average percent return per year based on ending account value and trade duration\n",
    "one_week_trade_simulation['Avg % Return Per Year'] = ((((\n",
    "    one_week_trade_simulation['Ending Account Value'] - starting_trade_account_value) / (\n",
    "        starting_trade_account_value)) / duration_of_trading_years) * 100\n",
    ")\n",
    "\n",
    "# Calculate average profit per trade by dividing total profit by the number of total trades\n",
    "one_week_trade_simulation['Avg Profit per Trade'] = (\n",
    "    one_week_trade_simulation['Total Profit/Loss'] / (\n",
    "        one_week_trade_simulation['Bullish Trades'] + (\n",
    "            one_week_trade_simulation['Bearish Trades']))\n",
    ")\n",
    "\n",
    "# Sort simulation results by 'Avg % Return Per Year' in descending order to view \n",
    "# most profitable stock symbols\n",
    "one_week_trade_simulation = one_week_trade_simulation.sort_values(\n",
    "    by = 'Avg % Return Per Year', ascending=False)\n",
    "\n",
    "one_week_trade_simulation.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238b3d9-056a-4f63-bdd7-9e37e8a1e48f",
   "metadata": {},
   "source": [
    "### Trade Simulation with No Hedge: Evaluation\n",
    "We calculate Average Profit Per Year, Average Yearly Return, Average Profit Per Trade, and then the two most important metrics which are Win Ratio and Sharpe Ratio. The Win Ratio is the number of winning trades out of total number of trades. Sharpe Ratio is a measure to evalute the risk-adjusted return of an investment or portfolio. It tells you if the return on trades was worth the risk taken. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589cc3c-42b3-4c3c-a646-356fe74229ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_week_trade_simulation\n",
    "\n",
    "print(one_week_trade_simulation['Negative Bullish Trades'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae399b15-5b36-4492-961f-34d0defa30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Week Prediction Trade Simulation Results\n",
    "\n",
    "# Calculate average profit per year based on total profit/loss and \n",
    "# duration of simulation in years\n",
    "avg_profit_per_year = (one_week_trade_simulation['Total Profit/Loss'].mean()) / duration_of_trading_years\n",
    "print(f'Average Profit Per Year: ${avg_profit_per_year:,.2f}')\n",
    "\n",
    "# Calculate win ratio by counting the number of positive profit/loss entries and\n",
    "# dividing by total number of trades\n",
    "win_ratio = (\n",
    "    one_week_trade_simulation[one_week_trade_simulation['Total Profit/Loss'] > 0].shape[0] / (\n",
    "        one_week_trade_simulation.shape[0])\n",
    ")\n",
    "print(f'win ratio: {win_ratio*100}%')\n",
    "\n",
    "# Calculate the Sharpe Ratio, which measures return relative to it's volatility\n",
    "# Sharpe Ratio indicates if the profit from trade was worth the risk taken\n",
    "# If Sharpe Ratio > 0, trade was worth the risk. If Sharpe Ratio < 0, trade was not worth the risk\n",
    "sharp_ratio = (one_week_trade_simulation['Total Profit/Loss'].mean(\n",
    ") / one_week_trade_simulation['Total Profit/Loss'].std())\n",
    "print(f'sharpe ratio: {sharp_ratio:.2f}')\n",
    "\n",
    "# Calculate average yearly return using the 'Avg % Return Per Year' column\n",
    "avg_percent_yearly_return = one_week_trade_simulation['Avg % Return Per Year'].mean()\n",
    "print(f'Avg Yearly Return: {avg_percent_yearly_return:.2f}%')\n",
    "\n",
    "# Calculate average profit per trade by taking the mean of 'Avg Profit per Trade' column\n",
    "avg_profit_per_trade = one_week_trade_simulation['Avg Profit per Trade'].mean()\n",
    "print(f'Average Profit Per Trade: ${avg_profit_per_trade:,.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69c067-1c0e-4b25-bccf-318bf9b91bbf",
   "metadata": {},
   "source": [
    "### Trade Simulation with No Hedge Results:\n",
    "The win ratio is 36%, which is favorable because then we only need a risk-to-reward value of a little over 1:2.5. The Shape ratio is -.32, which means that the risk with the trades we are taking is not worth it. The average yearly return for using our model with no hedge is -1.47%, with an average loss of $6.08 per trade. Overall, with no hedging, our model was unprofitable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782336f0-3f27-44a2-a318-9840b22eadb1",
   "metadata": {},
   "source": [
    "### Trade Simulation With Hedge\n",
    "Our hedge has a risk multiplier of 2.5. This means that if there is a hedge value, it will be multiplied by 2.5. The hedge values came out very small, so they need to be amplified a certain amount for it to be effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba997ef1-3eb4-4bf7-b19a-af24a8ac1773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trade Simulation with Hedge\n",
    "# risk_mult = 2.5\n",
    "\n",
    "# Initialize list to store overall results of trading simulation\n",
    "overall_results_hedge = []\n",
    "\n",
    "# Group the dataframe by 'Symbol' and run the simulation for each stock separately\n",
    "for symbol, group in pred_one_week.groupby('Symbol'):\n",
    "    print(f\"\\nStarting trading simulation for {symbol}\\n\")\n",
    "\n",
    "    # Reset trading account for each stock\n",
    "    trading_account = 100000\n",
    "    profit_loss = 0\n",
    "    bullish_trade_count = 0\n",
    "    bearish_trade_count = 0\n",
    "    positive_bullish_trade = 0\n",
    "    negative_bullish_trade = 0\n",
    "    positive_bearish_trade = 0\n",
    "    negative_bearish_trade = 0\n",
    "    percent_gain = (profit_loss / 100000) * 100\n",
    "    skip_iterations = 0\n",
    "\n",
    "    # Loop through each row (each row represents one trade)\n",
    "    for i, row in group.iterrows():\n",
    "\n",
    "        # Extract relevant features for trade\n",
    "        starting_price = row['Starting Price']\n",
    "        actual_price = row['Actual Price']\n",
    "        predicted_price = row[\"Today's Predicted Price\"]\n",
    "        trade_date = row['Date']\n",
    "        length_holding_trade = row['Lengh of Trade (Days)']\n",
    "        bull_hedge = row['Bullish Hedge']\n",
    "        bear_hedge = row['Bearish Hedge']\n",
    "\n",
    "        # Determine trade predictions (bullish or bearish)\n",
    "        bullish_pred = predicted_price > starting_price\n",
    "        actual_bull = actual_price > starting_price\n",
    "        bearish_pred = predicted_price < starting_price\n",
    "        actual_bear = actual_price < starting_price\n",
    "        \n",
    "        # % Portfolio risked per trade\n",
    "        portfolio_trade_percent = 0.05\n",
    "      \n",
    "        # Iteration Skipper: Limit Number of trades over testing timeframe\n",
    "        num_iteration_skips = 0 \n",
    "        if skip_iterations > 0:\n",
    "            skip_iterations -= 1\n",
    "            continue\n",
    "\n",
    "        # Position size per trade        \n",
    "        trade_size = trading_account * portfolio_trade_percent\n",
    "\n",
    "        # Trade fee: trade_size * trade_fee\n",
    "        trade_fee = 0.0025\n",
    "\n",
    "        # Stock borrow fee for short trades\n",
    "        stock_borrow_fee_percent = 0.03\n",
    "        total_days_one_year = 365\n",
    "        stock_borrow_daily_fee = stock_borrow_fee_percent / total_days_one_year\n",
    "\n",
    "        # Leverage and margin fee\n",
    "        leverage = 1\n",
    "        margin_rate = 0.12\n",
    "        margin_daily_rate = margin_rate / total_days_one_year\n",
    "\n",
    "        # Risk multiplier for hedging\n",
    "        risk_mult = 2.5\n",
    "\n",
    "\n",
    "        \n",
    "        # Bullish Prediction trading logic     \n",
    "        if bullish_pred:\n",
    "            \n",
    "            bullish_trade_count += 1\n",
    "            total_trade_position_value = trade_size\n",
    "            hedge_portion = 0\n",
    "            bull_hedge_starting_trade_cost = 0\n",
    "            bull_hedge_starting_trade_fee = 0\n",
    "            if bull_hedge > 0:\n",
    "                hedge_portion = bull_hedge * risk_mult\n",
    "\n",
    "                # Cap hedge at 50%\n",
    "                hedge_portion = min(hedge_portion, 0.50)\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            bull_hedge_starting_value = total_trade_position_value * hedge_portion\n",
    "            bull_hedge_starting_share_amount = bull_hedge_starting_value // starting_price\n",
    "\n",
    "            # Calculate trade fee and stock borrow fee for bullish hedge\n",
    "            if bull_hedge_starting_share_amount > 0: \n",
    "                bull_hedge_starting_trade_cost = bull_hedge_starting_share_amount * starting_price\n",
    "                bull_hedge_starting_trade_fee = bull_hedge_starting_trade_cost * trade_fee\n",
    "            else:\n",
    "                bull_hedge_starting_value = 0\n",
    "\n",
    "            # Calculate bullish trade value, number of shares to purchase, total cost of trade, and trade fee\n",
    "            bull_starting_trade_value = total_trade_position_value - bull_hedge_starting_value\n",
    "            bull_starting_share_amount = bull_starting_trade_value // starting_price\n",
    "            bull_starting_trade_cost = bull_starting_share_amount * starting_price\n",
    "            bull_starting_trade_fee = bull_starting_trade_cost * trade_fee\n",
    "\n",
    "            # Adjust trading_account with bullish trade and hedge trade (if there is a hedge)\n",
    "            trading_account -= (bull_starting_trade_cost + bull_starting_trade_fee)\n",
    "            trading_account += (bull_hedge_starting_trade_cost - bull_hedge_starting_trade_fee)\n",
    "\n",
    "            # Calculate and update final trade values for hedge trade\n",
    "            bull_hedge_actual_trade_value = (bull_hedge_starting_share_amount * actual_price)\n",
    "            bull_hedge_actual_trade_fee = bull_hedge_actual_trade_value * trade_fee\n",
    "            bull_hedge_stock_borrow_fee = bull_hedge_starting_trade_cost * (stock_borrow_daily_fee * length_holding_trade)\n",
    "\n",
    "            # Update trading_account with end of hedge trade\n",
    "            trading_account -= (bull_hedge_actual_trade_value + bull_hedge_actual_trade_fee + bull_hedge_stock_borrow_fee)\n",
    "\n",
    "            # Calculate and update final trade values for hedge trade\n",
    "            bull_actual_trade_sell = bull_starting_share_amount * actual_price\n",
    "            bull_actual_trade_fee = bull_actual_trade_sell * trade_fee\n",
    "\n",
    "            # Update trading_account with end of bull trade\n",
    "            trading_account += (bull_actual_trade_sell - bull_actual_trade_fee)\n",
    "\n",
    "            # Calculate bullish_profit for bull trade and bearish_profit for hedge trade\n",
    "            bearish_profit = (bull_hedge_starting_trade_cost - bull_hedge_starting_trade_fee) - (\n",
    "                bull_hedge_actual_trade_value + bull_hedge_actual_trade_fee + bull_hedge_stock_borrow_fee)\n",
    "\n",
    "            bullish_profit = ((bull_actual_trade_sell - bull_actual_trade_fee) - (\n",
    "                bull_starting_trade_cost + bull_starting_trade_fee))\n",
    "\n",
    "            # Add both bearish profit and bullish profit to cumulating profit_loss bucket\n",
    "            profit_loss += bearish_profit\n",
    "            profit_loss += bullish_profit\n",
    "\n",
    "            # If total profit (bullish profit + bearish profit) > 0, add 1 count to positive bullish trade\n",
    "            # If total profit is less than 0, add 1 count to negative bullish trade\n",
    "            if (bullish_profit + bearish_profit) > 0:\n",
    "                positive_bullish_trade += 1\n",
    "            else:\n",
    "                negative_bullish_trade += 1\n",
    "\n",
    "            \n",
    "\n",
    "        # Bearish Prediction trading logic       \n",
    "        elif bearish_pred:\n",
    "\n",
    "            bearish_trade_count += 1\n",
    "            total_trade_position_value = trade_size\n",
    "            hedge_portion = 0\n",
    "            bear_hedge_starting_trade_cost = 0\n",
    "            bear_hedge_starting_trade_fee = 0\n",
    "            if bear_hedge > 0:\n",
    "                hedge_portion =  bear_hedge * risk_mult\n",
    "\n",
    "                # Cap hedge at 50%\n",
    "                hedge_portion = min(hedge_portion, 0.50)\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            bear_hedge_starting_value = total_trade_position_value * hedge_portion\n",
    "            bear_hedge_starting_share_amount = bear_hedge_starting_value // starting_price\n",
    "\n",
    "            # Calculate cost of bearish hedge trade\n",
    "            if bear_hedge_starting_share_amount > 0: \n",
    "                bear_hedge_starting_trade_cost = bear_hedge_starting_share_amount * starting_price\n",
    "                bear_hedge_starting_trade_fee = bear_hedge_starting_trade_cost * trade_fee\n",
    "            else:\n",
    "                bear_hedge_starting_value = 0\n",
    "\n",
    "            # Update trade values, get number of shares, get total trade cost, and calculate trade fee of bearish trade\n",
    "            bear_starting_trade_value = total_trade_position_value - bear_hedge_starting_value\n",
    "            bear_starting_share_amount = bear_starting_trade_value // starting_price\n",
    "            bear_starting_trade_cost = bear_starting_share_amount * starting_price\n",
    "            bear_starting_trade_fee = bear_starting_trade_cost * trade_fee\n",
    "\n",
    "            # Adjust trading_account with bear trade value and hedge trade value\n",
    "            trading_account -= (bear_hedge_starting_trade_cost + bear_hedge_starting_trade_fee)\n",
    "            trading_account += (bear_starting_trade_cost - bear_starting_trade_fee)\n",
    "\n",
    "            # Calculate final hedge trade values\n",
    "            bear_hedge_actual_trade_value = (bear_hedge_starting_share_amount * actual_price)\n",
    "            bear_hedge_actual_trade_fee = bear_hedge_actual_trade_value * trade_fee\n",
    "\n",
    "            # Adjust trading_account with end of hedge trade value \n",
    "            trading_account += (bear_hedge_actual_trade_value - bear_hedge_actual_trade_fee)\n",
    "            \n",
    "            # Calculate final bearish trade values\n",
    "            bear_actual_trade_value = bear_starting_share_amount * actual_price\n",
    "            bear_actual_trade_fee = bear_actual_trade_value * trade_fee\n",
    "            bear_actual_stock_borrow_fee = bear_hedge_starting_trade_cost * (stock_borrow_daily_fee * length_holding_trade)\n",
    "\n",
    "            # Adjust trading_account with end of bearish trade value\n",
    "            trading_account -= (bear_actual_trade_value + bear_actual_trade_fee + bear_actual_stock_borrow_fee)\n",
    "\n",
    "            # Calculate bearish profit (profits from bearish trade)\n",
    "            # and bullish profits (profits from hedge)\n",
    "            bearish_profit = (bear_starting_trade_cost - bear_starting_trade_fee) - (\n",
    "                bear_actual_trade_value + bear_actual_trade_fee + bear_actual_stock_borrow_fee)\n",
    "                  \n",
    "            bullish_profit = ((bear_hedge_actual_trade_value - bear_hedge_actual_trade_fee) - (\n",
    "                bear_hedge_starting_trade_cost + bear_hedge_starting_trade_fee))\n",
    "\n",
    "            # Add bearish and bullish profit to cumulating profit_loss bucket\n",
    "            profit_loss += bearish_profit\n",
    "            profit_loss += bullish_profit\n",
    "\n",
    "            # If bearish profit + bullish profit > 0, add 1 count of positive bearish trade\n",
    "            # If bearish profit + bullish profit is < 0, add 1 count of negative bearish trade\n",
    "            if (bearish_profit + bullish_profit) > 0:\n",
    "                positive_bearish_trade += 1\n",
    "            else:\n",
    "                negative_bearish_trade += 1\n",
    "                    \n",
    "\n",
    "        # Iteration Skipping (limits number of trades if num_iteration_skips > 0)\n",
    "        skip_iterations = num_iteration_skips\n",
    "\n",
    "        # Check if account has dropped below $75,000 and stop the simulation\n",
    "        if trading_account <= 75000:\n",
    "            print(f\"Trading account for {symbol} has dropped below $75,000. Stopping simulation.\")\n",
    "            break\n",
    "\n",
    "    # Calculate percent gain\n",
    "    percent_gain = (profit_loss / 100000) * 100\n",
    "    # Store results for each stock symbol\n",
    "    result = {\n",
    "        'Symbol': symbol,\n",
    "        'Ending Account Value': trading_account,\n",
    "        'Total Profit/Loss': profit_loss,\n",
    "        'Bullish Trades': bullish_trade_count,\n",
    "        'Bearish Trades': bearish_trade_count,\n",
    "        'Positive Bullish Trades': positive_bullish_trade,\n",
    "        'Negative Bullish Trades': negative_bullish_trade,\n",
    "        'Positive Bearish Trades': positive_bearish_trade,\n",
    "        'Negative Bearish Trades': negative_bearish_trade,\n",
    "        'Percent Gain/Loss': percent_gain\n",
    "    }\n",
    "    overall_results_hedge.append(result)\n",
    "\n",
    "    # Print results for the current stock\n",
    "    print(f\"\\nResults for {symbol}:\")\n",
    "    print(f'Trade Account Ending Value: ${trading_account:.2f}')\n",
    "    print(f'Total Profit/Loss: ${profit_loss:.2f}')\n",
    "    print(f'Number of Bullish Trades: {bullish_trade_count}')\n",
    "    print(f'Number of Bearish Trades: {bearish_trade_count}')\n",
    "    print(f'Number of Positive Bullish Trades: {positive_bullish_trade}')\n",
    "    print(f'Number of Negative Bullish Trades: {negative_bullish_trade}')\n",
    "    print(f'Number of Positive Bearish Trades: {positive_bearish_trade}')\n",
    "    print(f'Number of Negative Bearish Trades: {negative_bearish_trade}')\n",
    "    print(f'Percentage Gain: {(profit_loss / 100000) * 100}')\n",
    "    \n",
    "# Convert overall results to a DataFrame for easier analysis\n",
    "trade_sim_hedge_results = pd.DataFrame(overall_results_hedge)\n",
    "print(\"\\nFinal summary of all stocks:\")\n",
    "trade_sim_hedge_results = trade_sim_hedge_results.round(2)\n",
    "trade_sim_hedge_results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b43479-b77b-47dd-a271-710b5df2940b",
   "metadata": {},
   "source": [
    "### Trading Simulation With Hedge: Process Data Results\n",
    "Repeat the same process as with first simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a351d89-5013-4125-b27b-508736379726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of 'Total Profit/Loss' column but formatted to display as currency for readability\n",
    "trade_sim_hedge_results['Total Profit/Loss ($)'] = (\n",
    "    trade_sim_hedge_results['Total Profit/Loss'].apply(\n",
    "        lambda x: f\"${x:,.2f}\")\n",
    ")\n",
    "\n",
    "# Create copy of 'Ending Account Value' column but formatted to display as currency for readability\n",
    "trade_sim_hedge_results['Ending Account Value ($)'] = (\n",
    "    trade_sim_hedge_results['Ending Account Value'].apply(\n",
    "        lambda x: f\"${x:,.2f}\")\n",
    ")\n",
    "\n",
    "# Select specific columns and reorder to display final results of trading simulation with no hedge\n",
    "trade_sim_hedge_results = (\n",
    "    trade_sim_hedge_results[['Symbol', 'Total Profit/Loss ($)', \n",
    "                              'Ending Account Value ($)', 'Ending Account Value', \n",
    "                              'Total Profit/Loss', 'Bullish Trades', \n",
    "                              'Bearish Trades', 'Positive Bullish Trades', \n",
    "                              'Negative Bullish Trades', 'Positive Bearish Trades', \n",
    "                              'Negative Bearish Trades','Percent Gain/Loss',]]\n",
    ")\n",
    "\n",
    "trade_sim_hedge_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459a853-5f42-4dee-ba88-818b442082a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of trade_sim_hedge_results DataFrame\n",
    "one_week_hedge_trade_sim = trade_sim_hedge_results.copy()\n",
    "\n",
    "# Define start date and end date of the trade simulation\n",
    "start_date = datetime(2023, 2, 8)\n",
    "end_date = datetime(2025, 2, 25)\n",
    "\n",
    "# Calculate total number of trading days between start date and end date\n",
    "duration_of_trading_days = (end_date - start_date).days\n",
    "\n",
    "# Convert trading duration from days to years\n",
    "duration_of_trading_years = duration_of_trading_days / 365.25\n",
    "\n",
    "# Starting value of trade account is $100,000\n",
    "starting_trade_account_value = 100000\n",
    "\n",
    "# Calculate average percent return per year based on ending account value and trade duration\n",
    "one_week_hedge_trade_sim['Avg % Return Per Year'] = ((((\n",
    "    one_week_hedge_trade_sim['Ending Account Value'] - starting_trade_account_value) / (\n",
    "        starting_trade_account_value)) / duration_of_trading_years) * 100\n",
    ")\n",
    "\n",
    "# Calculate average profit per trade by dividing total profit by the number of total trades\n",
    "one_week_hedge_trade_sim['Avg Profit per Trade'] = (\n",
    "    one_week_hedge_trade_sim['Total Profit/Loss'] / (\n",
    "        one_week_hedge_trade_sim['Bullish Trades'] + (\n",
    "            one_week_hedge_trade_sim['Bearish Trades']))\n",
    ")\n",
    "\n",
    "# Sort simulation results by 'Avg % Return Per Year' in descending order to view \n",
    "# most profitable stock symbols\n",
    "one_week_hedge_trade_sim = one_week_hedge_trade_sim.sort_values(\n",
    "    by = 'Avg % Return Per Year', ascending=False)\n",
    "\n",
    "one_week_hedge_trade_sim.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be757e1-9579-4370-98e2-dd4949feb85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Week Prediction Trade Simulation Results with hedge\n",
    "\n",
    "# Calculate average profit per year based on total profit/loss and \n",
    "# duration of simulation in years\n",
    "avg_profit_per_year = (one_week_hedge_trade_sim['Total Profit/Loss'].mean()) / duration_of_trading_years\n",
    "print(f'Average Profit Per Year: ${avg_profit_per_year:,.2f}')\n",
    "\n",
    "# Calculate win ratio by counting the number of positive profit/loss entries and\n",
    "# dividing by total number of trades\n",
    "win_ratio = (\n",
    "    one_week_hedge_trade_sim[one_week_hedge_trade_sim['Total Profit/Loss'] > 0].shape[0] / (\n",
    "        one_week_hedge_trade_sim.shape[0])\n",
    ")\n",
    "print(f'win ratio: {win_ratio*100}%')\n",
    "\n",
    "# Calculate the Sharpe Ratio, which measures return relative to it's volatility\n",
    "# Sharpe Ratio indicates if the profit from trade was worth the risk taken\n",
    "# If Sharpe Ratio > 0, trade was worth the risk. If Sharpe Ratio < 0, trade was not worth the risk\n",
    "sharp_ratio = (one_week_hedge_trade_sim['Total Profit/Loss'].mean(\n",
    ") / one_week_hedge_trade_sim['Total Profit/Loss'].std())\n",
    "print(f'sharpe ratio: {sharp_ratio:.2f}')\n",
    "\n",
    "# Calculate average yearly return using the 'Avg % Return Per Year' column\n",
    "avg_percent_yearly_return = one_week_hedge_trade_sim['Avg % Return Per Year'].mean()\n",
    "print(f'Avg Yearly Return: {avg_percent_yearly_return:.2f}%')\n",
    "\n",
    "# Calculate average profit per trade by taking the mean of 'Avg Profit per Trade' column\n",
    "avg_profit_per_trade = one_week_hedge_trade_sim['Avg Profit per Trade'].mean()\n",
    "print(f'Average Profit Per Trade: ${avg_profit_per_trade:,.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a862ce9-095f-4b68-9285-f7adbe2fb370",
   "metadata": {},
   "source": [
    "### Trade Simulation with Hedge Results\n",
    "With our hedge strategy, all metrics decreased. This shows us that we not only need to modify the model to make it profitble, our hedging strategies need to also be modified, in order to limit downside risk and prevent too much loss from occuring. A hedge should make it so our model does not lose as much money. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f51326b-959a-4de8-a69d-b8e084048caf",
   "metadata": {},
   "source": [
    "## Forecasting Future Price Using Technical Analysis, Price Action, and Supervised Machine Learning\n",
    "### Summary, Limitations & Solutions, and Future Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08aa07-f045-4cb4-967d-830bb509adf5",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Using XGBoost, we were able to create a model to predict stock prices 1 week in the future using TA indicators and it had some predictive power in some scenarios, but not enough to consistenly turn a profit. Even though we used a lot of features, which were also multiplied when you added rolling averages and lags, filtering by feature performance and using regularlization techniques helped to reduce the noise and preventing the model from overfitting to the training data. Even though our test metrics were good, up to par with industry standards in terms of algorithmic stock prediction modeling, the results fell short when it came to the trading simulation. Although statistical performance metrics, such as R-Squared, Mean Squared Error, and MAPE are commonplace in model evaluation, they are less interpretable to people outside of the statistical modeling space. However, the results from the trade simulation, such as Average percent returns per year and total profit/loss, is much more understandable and translates those statistical performance metrics into real-life results. Below I dive into limitations of the creation of this model and potential solutions, as well as future actions to enhance the predictive power of this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398fba47-9aa5-4204-9d2d-d4d811b0ac57",
   "metadata": {},
   "source": [
    "### Limitations, Solutions, and Future Actions\n",
    "#### Limitations\n",
    "\n",
    "* Computational Power\n",
    "  * Time series data is large and requires substantial computational power.\n",
    "  * I was not able to utilize Grid_Search_CV to optimize my XGBoost model, instead I had to run the model one-by-one to adjust hyperparameters.\n",
    "    \n",
    "* Delayed Data Importation\n",
    "  * yfinance is not officially supported by Yahoo Finance, so the rate limits are unknown and come at random times\n",
    "  * We were able to pull daily price data like open price, close price, daily volume, daily high, and daily low, but if we wanted to pull in more detailed things about each company, such as current outstanding shares or recent earnings per share numbers, we would not be able to\n",
    "  * Data is not updated in real-time, there is a delay. It did not matter as much for our 1 week price prediction, but if we were on a shorter timescale, it would impact performance of model greatly\n",
    "\n",
    "* TA indicators\n",
    "  * Data aggregators, such as Alpha Vantage and Interactive Brokers, provide TA indicators calculated for you\n",
    "  * This saves computational power and time, as well as introducing you to TA indicators you may not have heard of or used before\n",
    "\n",
    "#### Solutions\n",
    "* Computational Power\n",
    "  * Use Google Clouds' Compute Engine and access their GPUs\n",
    "  * Rent out space in a data center/cloud center to run your more complex models\n",
    "  * Buy a GPU for your own computer\n",
    "* Delayed Data Importation\n",
    "  * Instead of relying on yfinance, use a data aggregator, like Alpha Vantage or Interactive Brokers, and pay for the tier to gain access to real-time data\n",
    "* TA Indicators\n",
    "  * Many of the stock market data aggregators, like the ones we just mentioned above, also provide pre-calculated TA indicators to import\n",
    "\n",
    "#### Future Actions\n",
    "* Try other financial/economic features\n",
    "  * Although TA indicators are useful, there are a large number of other features to try using, such as financial ratios for stock valuation, macroeconomic policies like interest rates and inflation, sentiment analysis using X, earnings reports, industry news, and more.\n",
    "  * Most likely, a model will end up being a mix of features above. It will not be every possible feature added to a model, but it will be a couple features from a variety of sources.\n",
    "    \n",
    "* More model types\n",
    "  * XGBoost is very popular machine learning algorithm for time series analysis, but when researching, there are a few other algorithms used frequently in time series forcasting\n",
    "  * Auto Regressive Integrated Moving Average (ARIMA) is a traditional statistical model that uses past values to predict future values.\n",
    "  * Long Short-Term Memory (LSTM) is a type of recurrent neural network that can learn order dependence between items in a sequence\n",
    "    * This model was the most common time series forecasting model I came across\n",
    "  * Prophet is an algorithm created by Facebook that is designed for automatic forecasting of univariate time series analysis\n",
    "\n",
    "* Focus on one stock\n",
    "  * Instead of buildling a generalized model using many stocks, try building a predictive model for just one stock\n",
    " \n",
    "* Use classification\n",
    "  * As I was researching for this project, I came across many comments saying that instead of trying to predict an exact price, I should turn it into a classification problem and and predict directionality\n",
    "  * Creating a model to predict whether a stock will go up or down, instead of to an exact price, may be more successful\n",
    "  * It does not have to be binary, it could contain 5 or 6 classifications to identify magnitude of directional move as well\n",
    "\n",
    "* Model trading strategies\n",
    "  * There are many trading strategies commonly talked about in the day/swing trading world, such as Smart Money Concepts (SMC), which is a trading philosphy centered around monitoring the movements of large institutional investors to predict future market movements. It is often using support and resistance lines, TA indicators, and price action.\n",
    "  * ICT (Inner Circle Trader) is another trading strategy, related to SMC, focuses on things like imbalanced liquidity and Fair Value Gaps.\n",
    "  * Beside broad general trading philosophies, simpler specific trading strategies can be modeled and backtested.\n",
    "\n",
    "* Model strategies of popular investors\n",
    "  * Research Warren Buffets investing strategy in detail and create a model to predict directionality based off the features he uses\n",
    "\n",
    "* Model trade patterns of public officials\n",
    "  * Certain public officials, like members of congress, are required to pubicly disclose stock trades within 30 days of the transaction\n",
    "  * Modeling the trading patterns of the most profitable stock traders in congress could yield insightful information on growth potential of certain stocks or industries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
