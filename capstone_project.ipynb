{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce9d5af-0fda-41db-a2fc-22a5d7e0f586",
   "metadata": {},
   "source": [
    "# Technical Analysis Indicator Price Prediction\n",
    "The goal of this project is to analyze the predictive power of the top 10 most popular TA indicators and see how well they do to predict price over a 30 day period. I am going to find the value of the indicators on day 1 (30 trading days ago) and then find the daily closing price for 30 days later and measure how well the indicator predicted the price.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b635c3-9842-408e-9f61-0c982d9c49a2",
   "metadata": {},
   "source": [
    "first we'll find the top 500 stocks by market cap from nasdaq and pull them into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc0c83b9-37ed-4b1b-89ab-35bc8e4a9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning Supression\n",
    "import warnings\n",
    "from pandas.errors import PerformanceWarning \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "warnings.filterwarnings('ignore', category=PerformanceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e2975-7d47-4313-93a9-26bcc74988d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas library for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = '/Users/evancallaghan/Downloads/nasdaq_screener_1726538993372.csv' \n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Inspect the DataFrame to understand its structure\n",
    "print(df.head())\n",
    "\n",
    "# Filter DataFrame to only show the columns 'Symbol', 'Name', and 'Market Cap'\n",
    "df = df[['Symbol', 'Name', 'Market Cap']]\n",
    "\n",
    "# Convert 'Market Cap' to numeric if it's not already\n",
    "# Remove commas, dollar signs, and replace these symbols with empty spaces\n",
    "df['Market Cap'] = df['Market Cap'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "\n",
    "# Sort the DataFrame by Market Cap in descending order\n",
    "df_sorted = df.sort_values(by='Market Cap', ascending=False).head(1000)                                                                        \n",
    "df_sorted.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434eced7-138d-4dbe-a4d3-c9c92f32fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the DataFrame and drop the old index\n",
    "df_sorted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update the index to start from 1 instead of 0\n",
    "df_sorted.index = df_sorted.index + 1\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674b213-6fd9-428b-aa96-56497343cb2f",
   "metadata": {},
   "source": [
    "remove all stocks except common stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fe5b5-1215-423f-9acf-fbcbae90e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no leading or trailing whitespaces in the 'Name' column\n",
    "df_sorted['Name'] = df_sorted['Name'].str.strip()\n",
    "\n",
    "# List of terms to filter out\n",
    "terms_to_drop = [\"Capital Stock\", \"Depository Shares\", \"Global Notes\", \"ADS\", \n",
    "                 \"Registry Shares\", \"Depositary Shares\"\n",
    "]\n",
    "\n",
    "# Create a regex pattern to match any of the terms\n",
    "# //b ensures that the match occues only at the start or end of a word\n",
    "# pipe '|' ensures that if any of the terms in 'terms_to_drop' are seen, \n",
    "# there is a match\n",
    "pattern = '|'.join([f\"\\\\b{term}\\\\b\" for term in terms_to_drop])\n",
    "\n",
    "# Apply filtering based on the updated pattern\n",
    "df_filtered = df_sorted[~df_sorted['Name'].str.contains(pattern, case=False, \n",
    "                                                        na=False)\n",
    "]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41bc92-091c-4d0d-9ad7-c143c72bd072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the DataFrame and drop the old index\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update the index to start from 1 instead of 0\n",
    "df_filtered.index = df_filtered.index + 1\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ae08f-7207-4e72-94bc-beb65e8f66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[595:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd41b6a-5b30-413e-9090-5e6a9e99e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f48fa-2e14-4f6e-ae73-f451fa6fd1be",
   "metadata": {},
   "source": [
    "below are the 10 technical indicators we are going to use for this project.\n",
    "1. Relative Strength Index (RSI)\n",
    "2. Moving Average Convergence Divergence (MACD)\n",
    "3. Stochastic Oscillator\n",
    "4. Simple Moving Average (SMA)\n",
    "5. Exponential Moving Average (EMA)\n",
    "6. Volume Weighted Average Price (VWAP)\n",
    "7. Bollinger Bands\n",
    "8. Average True Range (ATR)\n",
    "9. Fibonacci Retracement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972bb04-97c8-4a0a-90c4-9e7f5e7fad8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10 year historical data top 600 stocks\n",
    "# Pulls data from yahoo finance into CSV files\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to download stock data for a single stock\n",
    "def download_stock_data(ticker, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Add a delay to avoid rate limiting\n",
    "            time.sleep(1)  # Increase delay to 2 seconds between requests\n",
    "            \n",
    "            print(f\"Downloading data for {ticker}, attempt {attempt + 1}\")\n",
    "            data = yf.download(ticker, start=\"2015-02-10\", end=\"2025-02-17\", interval=\"1d\")[['Close', 'High', 'Low', 'Volume']]\n",
    "            \n",
    "            if data.empty:\n",
    "                print(f\"Warning: No data found for {ticker}\")\n",
    "                return None  # Return None if data is empty\n",
    "\n",
    "            # Explicitly add 'Date' as a column before resetting the index\n",
    "            data['Date'] = data.index\n",
    "\n",
    "            # Reset the index and make 'Date' a normal column\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            data['Ticker'] = ticker\n",
    "            print(f\"Downloaded data for {ticker}:\\n{data.head()}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading data for {ticker}: {e}\")\n",
    "            time.sleep(1)  # Longer delay before retrying in case of failure\n",
    "    return None  # Return None after retries if still fails\n",
    "\n",
    "# List of tickers from your df_filtered dataframe\n",
    "tickers = df_filtered['Symbol'].head(600).astype(str).tolist()  # Ensure tickers are strings\n",
    "\n",
    "# Batch size for processing tickers in smaller chunks\n",
    "batch_size = 100  # Reduce batch size to avoid rate limits\n",
    "\n",
    "# Directory to save CSV files\n",
    "output_dir = \"/Users/evancallaghan/flatiron_ds/phase_5/capstone_project\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a function to download data for a batch of tickers in parallel\n",
    "def download_batch(batch_tickers, batch_index):\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:  # Use fewer threads to reduce load\n",
    "        results = list(executor.map(lambda ticker: download_stock_data(ticker), batch_tickers))\n",
    "\n",
    "    # Remove None values and ensure we have valid data\n",
    "    valid_results = [(batch_tickers[i], results[i]) for i in range(len(batch_tickers)) if results[i] is not None]\n",
    "    \n",
    "    # Add ticker info to the valid results\n",
    "    for ticker, df in valid_results:\n",
    "        df['Ticker'] = ticker  # Explicitly add a column for ticker\n",
    "    \n",
    "    # Combine all the valid stock data into a single DataFrame\n",
    "    if valid_results:\n",
    "        df_batch = pd.concat([df for _, df in valid_results], ignore_index=False)  # Don't lose index info\n",
    "        print(f\"Saving batch {batch_index} data to CSV.\")\n",
    "        # Save the batch to a CSV file\n",
    "        df_batch.to_csv(f\"{output_dir}/top600_10yr_stock_price_data_{batch_index}.csv\", index=False)\n",
    "    else:\n",
    "        print(f\"No data downloaded for batch {batch_index}.\")\n",
    "\n",
    "# Split tickers into batches\n",
    "for i in range(0, len(tickers), batch_size):\n",
    "    batch_tickers = tickers[i:i + batch_size]\n",
    "    batch_index = (i // batch_size) + 1  # Batch index starts from 1\n",
    "    download_batch(batch_tickers, batch_index)\n",
    "\n",
    "print(\"All batches processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab381910-c48d-4d2d-9426-96842e88517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if we will use thiis\n",
    "\n",
    "# 5 year historical data top 600 stocks\n",
    "# Pulls data from yahoo finance into CSV files\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to download stock data for a single stock\n",
    "def download_stock_data(ticker, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Add a delay to avoid rate limiting\n",
    "            time.sleep(1)  # Increase delay to 2 seconds between requests\n",
    "            \n",
    "            print(f\"Downloading data for {ticker}, attempt {attempt + 1}\")\n",
    "            data = yf.download(ticker, start=\"2020-02-10\", end=\"2025-02-17\", interval=\"1d\")[['Close', 'High', 'Low', 'Volume']]\n",
    "            \n",
    "            if data.empty:\n",
    "                print(f\"Warning: No data found for {ticker}\")\n",
    "                return None  # Return None if data is empty\n",
    "\n",
    "            # Explicitly add 'Date' as a column before resetting the index\n",
    "            data['Date'] = data.index\n",
    "\n",
    "            # Reset the index and make 'Date' a normal column\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            data['Ticker'] = ticker\n",
    "            print(f\"Downloaded data for {ticker}:\\n{data.head()}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading data for {ticker}: {e}\")\n",
    "            time.sleep(1)  # Longer delay before retrying in case of failure\n",
    "    return None  # Return None after retries if still fails\n",
    "\n",
    "# List of tickers from your df_filtered dataframe\n",
    "tickers = df_filtered['Symbol'].head(600).astype(str).tolist()  # Ensure tickers are strings\n",
    "\n",
    "# Batch size for processing tickers in smaller chunks\n",
    "batch_size = 100  # Reduce batch size to avoid rate limits\n",
    "\n",
    "# Directory to save CSV files\n",
    "output_dir = \"/Users/evancallaghan/flatiron_ds/phase_5/capstone_project\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a function to download data for a batch of tickers in parallel\n",
    "def download_batch(batch_tickers, batch_index):\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:  # Use fewer threads to reduce load\n",
    "        results = list(executor.map(lambda ticker: download_stock_data(ticker), batch_tickers))\n",
    "\n",
    "    # Remove None values and ensure we have valid data\n",
    "    valid_results = [(batch_tickers[i], results[i]) for i in range(len(batch_tickers)) if results[i] is not None]\n",
    "    \n",
    "    # Add ticker info to the valid results\n",
    "    for ticker, df in valid_results:\n",
    "        df['Ticker'] = ticker  # Explicitly add a column for ticker\n",
    "    \n",
    "    # Combine all the valid stock data into a single DataFrame\n",
    "    if valid_results:\n",
    "        df_batch = pd.concat([df for _, df in valid_results], ignore_index=False)  # Don't lose index info\n",
    "        print(f\"Saving batch {batch_index} data to CSV.\")\n",
    "        # Save the batch to a CSV file\n",
    "        df_batch.to_csv(f\"{output_dir}/top600_5yr_stock_price_data_{batch_index}.csv\", index=False)\n",
    "    else:\n",
    "        print(f\"No data downloaded for batch {batch_index}.\")\n",
    "\n",
    "# Split tickers into batches\n",
    "for i in range(0, len(tickers), batch_size):\n",
    "    batch_tickers = tickers[i:i + batch_size]\n",
    "    batch_index = (i // batch_size) + 1  # Batch index starts from 1\n",
    "    download_batch(batch_tickers, batch_index)\n",
    "\n",
    "print(\"All batches processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b1a6ef-b1f9-4587-ac6a-865665dc5d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>248034000</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>294247200</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>297898000</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>217088800</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>252609600</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Close       High        Low     Volume        Date Ticker\n",
       "0  30.504999  30.537500  30.040001  248034000  2015-02-10   AAPL\n",
       "1  31.219999  31.230000  30.625000  294247200  2015-02-11   AAPL\n",
       "2  31.615000  31.870001  31.392500  297898000  2015-02-12   AAPL\n",
       "3  31.770000  31.820000  31.412500  217088800  2015-02-13   AAPL\n",
       "4  31.957500  32.220001  31.730000  252609600  2015-02-17   AAPL"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to hold DataFrames\n",
    "df_list = []\n",
    "\n",
    "# List of specific file indices\n",
    "file_indices = range(1, 7)\n",
    "\n",
    "# Loop through the specific CSV file indices\n",
    "for i in file_indices:\n",
    "    # Construct the file path for each batch\n",
    "    csv_file_path = f'/Users/evancallaghan/flatiron_ds/phase_5/capstone_project/top600_10yr_stock_price_data_{i}.csv'\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list along the rows (axis=0)\n",
    "df_all = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f92f501-4ae0-42fe-ac19-0b367ea5e548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1404503, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e050b0-c4b5-4d9a-ae51-2f857455b8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Daily_High</th>\n",
       "      <th>Daily_Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>248034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>294247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>297898000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>217088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>252609600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date      Close  Daily_High  Daily_Low     Volume\n",
       "0   AAPL  2015-02-10  30.504999   30.537500  30.040001  248034000\n",
       "1   AAPL  2015-02-11  31.219999   31.230000  30.625000  294247200\n",
       "2   AAPL  2015-02-12  31.615000   31.870001  31.392500  297898000\n",
       "3   AAPL  2015-02-13  31.770000   31.820000  31.412500  217088800\n",
       "4   AAPL  2015-02-17  31.957500   32.220001  31.730000  252609600"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = df_all[['Ticker', 'Date', 'Close', 'High', 'Low', 'Volume']]\n",
    "df_all = df_all.rename(columns={'Ticker': 'Symbol',\n",
    "                               'High': 'Daily_High',\n",
    "                               'Low': 'Daily_Low'})\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7a6efa-5a3c-4573-93a6-b5eee084e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "print(df_all['Date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bfc5b5a-3c22-49fc-9f90-ae8f9da72463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "df_all['Date'] = pd.to_datetime(df_all['Date'], errors='coerce')\n",
    "print(df_all['Date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad96f7a3-17ed-43c4-95b0-caa2a6be9cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1404503, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540232d0-08c4-48a9-a32d-3f5151d39cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Daily_High</th>\n",
       "      <th>Daily_Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>248034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>294247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>297898000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>217088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>252609600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol       Date      Close  Daily_High  Daily_Low     Volume\n",
       "0   AAPL 2015-02-10  30.504999   30.537500  30.040001  248034000\n",
       "1   AAPL 2015-02-11  31.219999   31.230000  30.625000  294247200\n",
       "2   AAPL 2015-02-12  31.615000   31.870001  31.392500  297898000\n",
       "3   AAPL 2015-02-13  31.770000   31.820000  31.412500  217088800\n",
       "4   AAPL 2015-02-17  31.957500   32.220001  31.730000  252609600"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_stocks = df_all['Symbol'].unique().tolist()\n",
    "df_all = df_all[df_all['Symbol'].isin(top_stocks[:200])]\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ceb11e4-340d-4508-8491-41605f0109b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_all['Symbol'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa3e059-7a9e-44f3-b5ee-4a988b72621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_1_week = df_all.copy()\n",
    "stock_data_1_month = df_all.copy()\n",
    "stock_data_3_month = df_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aa6085b4-fabf-4368-bc6b-5a349db53bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows for my TA indicator calculations\n",
    "\n",
    "one_week_window = [3, 5, 7]\n",
    "one_month_window = [7, 10, 14, 20, 30]\n",
    "three_month_window = [14, 20, 30, 50, 60, 90]\n",
    "\n",
    "# Window mapping for loops\n",
    "window_mapping = {\n",
    "    \"stock_data_1_week\": (stock_data_1_week, one_week_window),\n",
    "    \"stock_data_1_month\": (stock_data_1_month, one_month_window),\n",
    "    \"stock_data_3_month\": (stock_data_3_month, three_month_window)\n",
    "}\n",
    "\n",
    "# List of dataframes for loops\n",
    "stock_dataframes = [stock_data_1_week, stock_data_1_month, stock_data_3_month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1568d69e-d075-4d9a-ae32-feeaf402868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Volume MA\n",
    "\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    for window in windows:\n",
    "        df[f'Volume_{window}day_avg'] = df.groupby('Symbol')['Volume'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a57e5dc1-bd10-4229-a475-7ef2b0f44639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily High Price MA\n",
    "\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    for window in windows:\n",
    "        df[f'Daily_High_{window}day_avg'] = df.groupby('Symbol')['Daily_High'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "253b1430-ce37-4ba8-85ac-d21962ec5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Low Price MA\n",
    "\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    for window in windows:\n",
    "        df[f'Daily_Low_{window}day_avg'] = df.groupby('Symbol')['Daily_Low'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2909fe0e-16fa-46c0-b6ca-39b48978bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Moving Average\n",
    "\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    for window in windows:\n",
    "        df[f'SMA_{window}'] = stock_data_1_week.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72e77e20-54e1-4ed0-9ff2-a9780e9b2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Moving Average\n",
    "\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    for window in windows:\n",
    "        df[f'EMA_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.ewm(span=window, adjust=False).mean()\n",
    "        )       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e23f402-1e54-4428-95bf-d12d9c4b2d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Daily_High</th>\n",
       "      <th>Daily_Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volume_14day_avg</th>\n",
       "      <th>Volume_20day_avg</th>\n",
       "      <th>Volume_30day_avg</th>\n",
       "      <th>Volume_50day_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>SMA_30</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>SMA_60</th>\n",
       "      <th>SMA_90</th>\n",
       "      <th>EMA_14</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>EMA_30</th>\n",
       "      <th>EMA_50</th>\n",
       "      <th>EMA_60</th>\n",
       "      <th>EMA_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>248034000</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>294247200</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.600333</td>\n",
       "      <td>30.573094</td>\n",
       "      <td>30.551128</td>\n",
       "      <td>30.533038</td>\n",
       "      <td>30.528442</td>\n",
       "      <td>30.520713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>297898000</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>30.735621</td>\n",
       "      <td>30.672323</td>\n",
       "      <td>30.619765</td>\n",
       "      <td>30.575468</td>\n",
       "      <td>30.564067</td>\n",
       "      <td>30.544764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>217088800</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>30.873539</td>\n",
       "      <td>30.776864</td>\n",
       "      <td>30.693974</td>\n",
       "      <td>30.622313</td>\n",
       "      <td>30.603605</td>\n",
       "      <td>30.571692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>252609600</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.018067</td>\n",
       "      <td>30.889306</td>\n",
       "      <td>30.775492</td>\n",
       "      <td>30.674673</td>\n",
       "      <td>30.647995</td>\n",
       "      <td>30.602149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol       Date      Close  Daily_High  Daily_Low     Volume  \\\n",
       "0   AAPL 2015-02-10  30.504999   30.537500  30.040001  248034000   \n",
       "1   AAPL 2015-02-11  31.219999   31.230000  30.625000  294247200   \n",
       "2   AAPL 2015-02-12  31.615000   31.870001  31.392500  297898000   \n",
       "3   AAPL 2015-02-13  31.770000   31.820000  31.412500  217088800   \n",
       "4   AAPL 2015-02-17  31.957500   32.220001  31.730000  252609600   \n",
       "\n",
       "   Volume_14day_avg  Volume_20day_avg  Volume_30day_avg  Volume_50day_avg  \\\n",
       "0      2.480340e+08      2.480340e+08      2.480340e+08      2.480340e+08   \n",
       "1      2.711406e+08      2.711406e+08      2.711406e+08      2.711406e+08   \n",
       "2      2.800597e+08      2.800597e+08      2.800597e+08      2.800597e+08   \n",
       "3      2.643170e+08      2.643170e+08      2.643170e+08      2.643170e+08   \n",
       "4      2.619755e+08      2.619755e+08      2.619755e+08      2.619755e+08   \n",
       "\n",
       "   ...     SMA_30     SMA_50     SMA_60     SMA_90     EMA_14     EMA_20  \\\n",
       "0  ...  30.504999  30.504999  30.504999  30.504999  30.504999  30.504999   \n",
       "1  ...  30.862499  30.862499  30.862499  30.862499  30.600333  30.573094   \n",
       "2  ...  31.113333  31.113333  31.113333  31.113333  30.735621  30.672323   \n",
       "3  ...  31.277500  31.277500  31.277500  31.277500  30.873539  30.776864   \n",
       "4  ...  31.413500  31.413500  31.413500  31.413500  31.018067  30.889306   \n",
       "\n",
       "      EMA_30     EMA_50     EMA_60     EMA_90  \n",
       "0  30.504999  30.504999  30.504999  30.504999  \n",
       "1  30.551128  30.533038  30.528442  30.520713  \n",
       "2  30.619765  30.575468  30.564067  30.544764  \n",
       "3  30.693974  30.622313  30.603605  30.571692  \n",
       "4  30.775492  30.674673  30.647995  30.602149  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_3_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0301330-a6b3-4916-bdf1-4f7c376f34a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RSI\n",
    "\n",
    "# Define a function to calculate RSI\n",
    "def calculate_rsi(df, window):\n",
    "    # Calculate price changes\n",
    "    delta = df['Close'].diff()\n",
    "\n",
    "    # Separate gains and losses\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "    # Calculate the rolling average of gains and losses\n",
    "    avg_gain = gain.rolling(window=window).mean()\n",
    "    avg_loss = loss.rolling(window=window).mean()\n",
    "\n",
    "    # Calculate Relative Strength (RS)\n",
    "    rs = avg_gain / avg_loss\n",
    "\n",
    "    # Calculate RSI\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['RSI'] = rsi\n",
    "\n",
    "    return rsi\n",
    "\n",
    "# Apply the function all dataframes to calculate RSI with industry standard 14day window\n",
    "for df in stock_dataframes:\n",
    "    calculate_rsi(df, 14)\n",
    "\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    for window in windows:\n",
    "        df[f'RSI_{window}'] = df.groupby('Symbol', group_keys=False).apply(\n",
    "        lambda x: calculate_rsi(x, window=window)\n",
    "        )\n",
    "        if window == 14:\n",
    "            df.drop(columns = ['RSI_14'], inplace=True)\n",
    "        else:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44e9627f-e119-4714-93e1-a5c0c073f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACD\n",
    "\n",
    "def calculate_macd(df):\n",
    "    \n",
    "    df['EMA_12_MACD'] = df.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.ewm(span=12, adjust=False).mean()\n",
    "    )\n",
    "    df['EMA_26_MACD'] = df.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.ewm(span=26, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    df['MACD'] = df['EMA_12_MACD'] - df['EMA_26_MACD']\n",
    "    df['Signal_Line'] = df.groupby('Symbol')['MACD'].transform(\n",
    "        lambda x: x.ewm(span=9, adjust=False).mean()\n",
    "    )\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['Signal_Line']\n",
    "\n",
    "    df.drop(columns = ['EMA_12_MACD', 'EMA_26_MACD'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_rolling_macd(df, window):\n",
    "\n",
    "    df[f'MACD_rolling_{window}'] = df.groupby('Symbol')['MACD'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Apply rolling average to the Signal Line\n",
    "    df[f'Signal_rolling_{window}'] = df.groupby('Symbol')['Signal_Line'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    df[f'MACD_Histogram_rolling_{window}'] = df.groupby('Symbol')['MACD_Histogram'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    calculate_macd(df)\n",
    "    for window in windows:\n",
    " \n",
    "        calculate_rolling_macd(df, window)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8bfe0cbc-984c-4585-b08d-ec460754adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic oscillator\n",
    "def calculate_stoch_oscillator(df, windows):\n",
    "        \n",
    "    df['Stoch_Lowest_Low'] = df.groupby('Symbol')['Daily_Low'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "    )\n",
    "    \n",
    "    df['Stoch_Highest_High'] = df.groupby('Symbol')['Daily_High'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "    )\n",
    "    \n",
    "    df[f'%K_{window}'] = ((df['Close'] - df['Stoch_Lowest_Low']) / (df['Stoch_Highest_High'] - df['Stoch_Lowest_Low'])) * 100\n",
    "\n",
    "    df[f'%D_{window}'] = df.groupby('Symbol')[f'%K_{window}'].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "    df.drop(columns=['Stoch_Lowest_Low', 'Stoch_Highest_High'], inplace=True)\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "def standard_stoch_oscillator(df):\n",
    "    df['Stoch_Lowest_Low'] = df.groupby('Symbol')['Daily_Low'].transform(\n",
    "        lambda x: x.rolling(window=14, min_periods=1).min()\n",
    "    )\n",
    "    \n",
    "    df['Stoch_Highest_High'] = df.groupby('Symbol')['Daily_High'].transform(\n",
    "        lambda x: x.rolling(window=14, min_periods=1).max()\n",
    "    )\n",
    "    \n",
    "    df['%K'] = ((df['Close'] - df['Stoch_Lowest_Low']) / (df['Stoch_Highest_High'] - df['Stoch_Lowest_Low'])) * 100\n",
    "\n",
    "    df['%D'] = df.groupby('Symbol')['%K'].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "    df.drop(columns=['Stoch_Lowest_Low', 'Stoch_Highest_High'], inplace=True)\n",
    "    return df\n",
    "    \n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    standard_stoch_oscillator(df)   \n",
    "    for window in windows:\n",
    "\n",
    "        calculate_stoch_oscillator(df, window)\n",
    "        \n",
    "    if 14 in windows:\n",
    "        cols_to_drop = [col for col in ['%K_14', '%D_14'] if col in df.columns]\n",
    "        if cols_to_drop:  # Drop only if the list is not empty\n",
    "            df.drop(columns=cols_to_drop, inplace=True)\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ef2efea-5d80-47cf-bca5-f389b95b7f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VWAP\n",
    "\n",
    "# Calculate Volume Weighted Average Price (VWAP) per symbol\n",
    "def calculate_vwap(df):\n",
    "    # Ensure 'Close' and 'Volume' are numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "    df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')\n",
    "\n",
    "    # Calculate cumulative price-volume product for VWAP\n",
    "    df['Cumulative_Price_Volume'] = df.groupby('Symbol')['Close'].transform(\n",
    "    lambda x: (x * df.loc[x.index, 'Volume']).cumsum()\n",
    "    )\n",
    "    # Calculate cumulative volume for VWAP\n",
    "    df['Cumulative_Volume'] = df.groupby('Symbol')['Volume'].transform(\n",
    "    lambda x: x.cumsum()\n",
    "    )\n",
    "    # Calculate VWAP as the ratio of cumulative sums for each group (symbol)\n",
    "    df['VWAP'] = df['Cumulative_Price_Volume'] / df['Cumulative_Volume']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Add VWAP and VWAP window averages to dataframes\n",
    "i = 0\n",
    "while i < len(stock_dataframes):\n",
    "    df = stock_dataframes[i]\n",
    "    \n",
    "    calculate_vwap(df)\n",
    "    for name, (df_map, windows) in window_mapping.items():\n",
    "        if df is df_map:\n",
    "            for window in windows:\n",
    "    \n",
    "                df[f'VWAP_{window}'] = df.groupby('Symbol')['VWAP'].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "                )\n",
    "    \n",
    "    df.drop(columns=['Cumulative_Price_Volume', 'Cumulative_Volume'], inplace=True)\n",
    "    i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "865196bf-9b2c-4235-bc9e-c4b8285048cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/488875708.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Lower_Band_{window}'] = df[f'bb_Middle_Band_{window}'] - (df[f'Std_Dev_{window}'] * 2)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/488875708.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'bb_Middle_Band_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/488875708.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Std_Dev_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/488875708.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Upper_Band_{window}'] = df[f'bb_Middle_Band_{window}'] + (df[f'Std_Dev_{window}'] * 2)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/488875708.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Lower_Band_{window}'] = df[f'bb_Middle_Band_{window}'] - (df[f'Std_Dev_{window}'] * 2)\n"
     ]
    }
   ],
   "source": [
    "# Calculate Bollinger Bands per symbol\n",
    "\n",
    "def calculate_bollinger_bands(df, windows):\n",
    "    # Ensure 'Close' is numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "\n",
    "    for window in windows:\n",
    "        df[f'bb_Middle_Band_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "        df[f'Std_Dev_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "\n",
    "        df[f'Upper_Band_{window}'] = df[f'bb_Middle_Band_{window}'] + (df[f'Std_Dev_{window}'] * 2)\n",
    "        df[f'Lower_Band_{window}'] = df[f'bb_Middle_Band_{window}'] - (df[f'Std_Dev_{window}'] * 2)\n",
    "   \n",
    "    return df\n",
    "\n",
    "# Add Bollinger Bands to all dataframes\n",
    "i = 0\n",
    "while i < len(stock_dataframes):\n",
    "    \n",
    "    for name, (df, windows) in window_mapping.items():\n",
    "        calculate_bollinger_bands(df, windows)\n",
    "            \n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "770bf089-8654-49a4-8623-61c4b5709900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/2462574716.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ATR_Prev_Close'] = df.groupby('Symbol')['Close'].shift(1)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/2462574716.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ATR_High_Low'] = df['Daily_High'] - df['Daily_Low']  # High - Low\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/2462574716.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ATR_High_Close'] = (df['Daily_High'] - df['ATR_Prev_Close']).abs()  # High - Prev Close\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/2462574716.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ATR_Low_Close'] = (df['Daily_Low'] - df['ATR_Prev_Close']).abs()  # Low - Prev Close\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/2462574716.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ATR'] = df[['ATR_High_Low', 'ATR_High_Close', 'ATR_Low_Close']].max(axis=1)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/2462574716.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ATR_{window}'] = df.groupby('Symbol')['ATR'].transform(\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/2462574716.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ATR_{window}'] = df.groupby('Symbol')['ATR'].transform(\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/2462574716.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ATR_{window}'] = df.groupby('Symbol')['ATR'].transform(\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/2462574716.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ATR_{window}'] = df.groupby('Symbol')['ATR'].transform(\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/2462574716.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ATR_{window}'] = df.groupby('Symbol')['ATR'].transform(\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_19326/2462574716.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ATR_{window}'] = df.groupby('Symbol')['ATR'].transform(\n"
     ]
    }
   ],
   "source": [
    "# Average True Range (ATR)\n",
    "\n",
    "# Function to calculate True Range (TR)\n",
    "def calculate_true_range(df):\n",
    "    # Convert relevant columns to numeric (if not already numeric)\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "    df['Daily_High'] = pd.to_numeric(df['Daily_High'], errors='coerce')\n",
    "    df['Daily_Low'] = pd.to_numeric(df['Daily_Low'], errors='coerce')\n",
    "\n",
    "    # Ensure previous close is calculated per stock symbol to prevent cross-stock contamination\n",
    "    df['ATR_Prev_Close'] = df.groupby('Symbol')['Close'].shift(1)\n",
    "\n",
    "    df['ATR_High_Low'] = df['Daily_High'] - df['Daily_Low']  # High - Low\n",
    "    df['ATR_High_Close'] = (df['Daily_High'] - df['ATR_Prev_Close']).abs()  # High - Prev Close\n",
    "    df['ATR_Low_Close'] = (df['Daily_Low'] - df['ATR_Prev_Close']).abs()  # Low - Prev Close\n",
    "\n",
    "    # True Range is the max of the three\n",
    "    df['ATR'] = df[['ATR_High_Low', 'ATR_High_Close', 'ATR_Low_Close']].max(axis=1)\n",
    "\n",
    "    df.drop(columns=['ATR_Prev_Close', 'ATR_High_Low', 'ATR_High_Close', 'ATR_Low_Close'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add ATR calculation to all dataframes and add rolling windows\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    df = calculate_true_range(df)\n",
    "    for window in windows:\n",
    "\n",
    "        df[f'ATR_{window}'] = df.groupby('Symbol')['ATR'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6fc4f037-63bc-4095-a72c-967dec345c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Daily_High</th>\n",
       "      <th>Daily_Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volume_14day_avg</th>\n",
       "      <th>Volume_20day_avg</th>\n",
       "      <th>Volume_30day_avg</th>\n",
       "      <th>Volume_50day_avg</th>\n",
       "      <th>Volume_60day_avg</th>\n",
       "      <th>Volume_90day_avg</th>\n",
       "      <th>Daily_High_14day_avg</th>\n",
       "      <th>Daily_High_20day_avg</th>\n",
       "      <th>Daily_High_30day_avg</th>\n",
       "      <th>Daily_High_50day_avg</th>\n",
       "      <th>Daily_High_60day_avg</th>\n",
       "      <th>Daily_High_90day_avg</th>\n",
       "      <th>Daily_Low_14day_avg</th>\n",
       "      <th>Daily_Low_20day_avg</th>\n",
       "      <th>Daily_Low_30day_avg</th>\n",
       "      <th>Daily_Low_50day_avg</th>\n",
       "      <th>Daily_Low_60day_avg</th>\n",
       "      <th>Daily_Low_90day_avg</th>\n",
       "      <th>SMA_14</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_30</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>SMA_60</th>\n",
       "      <th>SMA_90</th>\n",
       "      <th>EMA_14</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>EMA_30</th>\n",
       "      <th>EMA_50</th>\n",
       "      <th>EMA_60</th>\n",
       "      <th>EMA_90</th>\n",
       "      <th>RSI</th>\n",
       "      <th>RSI_20</th>\n",
       "      <th>RSI_30</th>\n",
       "      <th>RSI_50</th>\n",
       "      <th>RSI_60</th>\n",
       "      <th>RSI_90</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "      <th>MACD_rolling_14</th>\n",
       "      <th>Signal_rolling_14</th>\n",
       "      <th>MACD_Histogram_rolling_14</th>\n",
       "      <th>MACD_rolling_20</th>\n",
       "      <th>Signal_rolling_20</th>\n",
       "      <th>MACD_Histogram_rolling_20</th>\n",
       "      <th>MACD_rolling_30</th>\n",
       "      <th>Signal_rolling_30</th>\n",
       "      <th>MACD_Histogram_rolling_30</th>\n",
       "      <th>MACD_rolling_50</th>\n",
       "      <th>Signal_rolling_50</th>\n",
       "      <th>MACD_Histogram_rolling_50</th>\n",
       "      <th>MACD_rolling_60</th>\n",
       "      <th>Signal_rolling_60</th>\n",
       "      <th>MACD_Histogram_rolling_60</th>\n",
       "      <th>MACD_rolling_90</th>\n",
       "      <th>Signal_rolling_90</th>\n",
       "      <th>MACD_Histogram_rolling_90</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>%K_20</th>\n",
       "      <th>%D_20</th>\n",
       "      <th>%K_30</th>\n",
       "      <th>%D_30</th>\n",
       "      <th>%K_50</th>\n",
       "      <th>%D_50</th>\n",
       "      <th>%K_60</th>\n",
       "      <th>%D_60</th>\n",
       "      <th>%K_90</th>\n",
       "      <th>%D_90</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>VWAP_14</th>\n",
       "      <th>VWAP_20</th>\n",
       "      <th>VWAP_30</th>\n",
       "      <th>VWAP_50</th>\n",
       "      <th>VWAP_60</th>\n",
       "      <th>VWAP_90</th>\n",
       "      <th>bb_Middle_Band_14</th>\n",
       "      <th>Std_Dev_14</th>\n",
       "      <th>Upper_Band_14</th>\n",
       "      <th>Lower_Band_14</th>\n",
       "      <th>bb_Middle_Band_20</th>\n",
       "      <th>Std_Dev_20</th>\n",
       "      <th>Upper_Band_20</th>\n",
       "      <th>Lower_Band_20</th>\n",
       "      <th>bb_Middle_Band_30</th>\n",
       "      <th>Std_Dev_30</th>\n",
       "      <th>Upper_Band_30</th>\n",
       "      <th>Lower_Band_30</th>\n",
       "      <th>bb_Middle_Band_50</th>\n",
       "      <th>Std_Dev_50</th>\n",
       "      <th>Upper_Band_50</th>\n",
       "      <th>Lower_Band_50</th>\n",
       "      <th>bb_Middle_Band_60</th>\n",
       "      <th>Std_Dev_60</th>\n",
       "      <th>Upper_Band_60</th>\n",
       "      <th>Lower_Band_60</th>\n",
       "      <th>bb_Middle_Band_90</th>\n",
       "      <th>Std_Dev_90</th>\n",
       "      <th>Upper_Band_90</th>\n",
       "      <th>Lower_Band_90</th>\n",
       "      <th>ATR</th>\n",
       "      <th>ATR_14</th>\n",
       "      <th>ATR_20</th>\n",
       "      <th>ATR_30</th>\n",
       "      <th>ATR_50</th>\n",
       "      <th>ATR_60</th>\n",
       "      <th>ATR_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>248034000</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>294247200</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.600333</td>\n",
       "      <td>30.573094</td>\n",
       "      <td>30.551128</td>\n",
       "      <td>30.533038</td>\n",
       "      <td>30.528442</td>\n",
       "      <td>30.520713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.045630</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>30.892965</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>297898000</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>30.735621</td>\n",
       "      <td>30.672323</td>\n",
       "      <td>30.619765</td>\n",
       "      <td>30.575468</td>\n",
       "      <td>30.564067</td>\n",
       "      <td>30.544764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.132584</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>31.148973</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>0.650002</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>217088800</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>30.873539</td>\n",
       "      <td>30.776864</td>\n",
       "      <td>30.693974</td>\n",
       "      <td>30.622313</td>\n",
       "      <td>30.603605</td>\n",
       "      <td>30.571692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202627</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.133588</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>31.276489</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>0.407499</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>252609600</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.018067</td>\n",
       "      <td>30.889306</td>\n",
       "      <td>30.775492</td>\n",
       "      <td>30.674673</td>\n",
       "      <td>30.647995</td>\n",
       "      <td>30.602149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.270153</td>\n",
       "      <td>0.109262</td>\n",
       "      <td>0.160890</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>31.407822</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>0.490002</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol       Date      Close  Daily_High  Daily_Low     Volume  \\\n",
       "0   AAPL 2015-02-10  30.504999   30.537500  30.040001  248034000   \n",
       "1   AAPL 2015-02-11  31.219999   31.230000  30.625000  294247200   \n",
       "2   AAPL 2015-02-12  31.615000   31.870001  31.392500  297898000   \n",
       "3   AAPL 2015-02-13  31.770000   31.820000  31.412500  217088800   \n",
       "4   AAPL 2015-02-17  31.957500   32.220001  31.730000  252609600   \n",
       "\n",
       "   Volume_14day_avg  Volume_20day_avg  Volume_30day_avg  Volume_50day_avg  \\\n",
       "0      2.480340e+08      2.480340e+08      2.480340e+08      2.480340e+08   \n",
       "1      2.711406e+08      2.711406e+08      2.711406e+08      2.711406e+08   \n",
       "2      2.800597e+08      2.800597e+08      2.800597e+08      2.800597e+08   \n",
       "3      2.643170e+08      2.643170e+08      2.643170e+08      2.643170e+08   \n",
       "4      2.619755e+08      2.619755e+08      2.619755e+08      2.619755e+08   \n",
       "\n",
       "   Volume_60day_avg  Volume_90day_avg  Daily_High_14day_avg  \\\n",
       "0      2.480340e+08      2.480340e+08             30.537500   \n",
       "1      2.711406e+08      2.711406e+08             30.883750   \n",
       "2      2.800597e+08      2.800597e+08             31.212500   \n",
       "3      2.643170e+08      2.643170e+08             31.364375   \n",
       "4      2.619755e+08      2.619755e+08             31.535500   \n",
       "\n",
       "   Daily_High_20day_avg  Daily_High_30day_avg  Daily_High_50day_avg  \\\n",
       "0             30.537500             30.537500             30.537500   \n",
       "1             30.883750             30.883750             30.883750   \n",
       "2             31.212500             31.212500             31.212500   \n",
       "3             31.364375             31.364375             31.364375   \n",
       "4             31.535500             31.535500             31.535500   \n",
       "\n",
       "   Daily_High_60day_avg  Daily_High_90day_avg  Daily_Low_14day_avg  \\\n",
       "0             30.537500             30.537500            30.040001   \n",
       "1             30.883750             30.883750            30.332500   \n",
       "2             31.212500             31.212500            30.685834   \n",
       "3             31.364375             31.364375            30.867500   \n",
       "4             31.535500             31.535500            31.040000   \n",
       "\n",
       "   Daily_Low_20day_avg  Daily_Low_30day_avg  Daily_Low_50day_avg  \\\n",
       "0            30.040001            30.040001            30.040001   \n",
       "1            30.332500            30.332500            30.332500   \n",
       "2            30.685834            30.685834            30.685834   \n",
       "3            30.867500            30.867500            30.867500   \n",
       "4            31.040000            31.040000            31.040000   \n",
       "\n",
       "   Daily_Low_60day_avg  Daily_Low_90day_avg     SMA_14     SMA_20     SMA_30  \\\n",
       "0            30.040001            30.040001  30.504999  30.504999  30.504999   \n",
       "1            30.332500            30.332500  30.862499  30.862499  30.862499   \n",
       "2            30.685834            30.685834  31.113333  31.113333  31.113333   \n",
       "3            30.867500            30.867500  31.277500  31.277500  31.277500   \n",
       "4            31.040000            31.040000  31.413500  31.413500  31.413500   \n",
       "\n",
       "      SMA_50     SMA_60     SMA_90     EMA_14     EMA_20     EMA_30  \\\n",
       "0  30.504999  30.504999  30.504999  30.504999  30.504999  30.504999   \n",
       "1  30.862499  30.862499  30.862499  30.600333  30.573094  30.551128   \n",
       "2  31.113333  31.113333  31.113333  30.735621  30.672323  30.619765   \n",
       "3  31.277500  31.277500  31.277500  30.873539  30.776864  30.693974   \n",
       "4  31.413500  31.413500  31.413500  31.018067  30.889306  30.775492   \n",
       "\n",
       "      EMA_50     EMA_60     EMA_90  RSI  RSI_20  RSI_30  RSI_50  RSI_60  \\\n",
       "0  30.504999  30.504999  30.504999  NaN     NaN     NaN     NaN     NaN   \n",
       "1  30.533038  30.528442  30.520713  NaN     NaN     NaN     NaN     NaN   \n",
       "2  30.575468  30.564067  30.544764  NaN     NaN     NaN     NaN     NaN   \n",
       "3  30.622313  30.603605  30.571692  NaN     NaN     NaN     NaN     NaN   \n",
       "4  30.674673  30.647995  30.602149  NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   RSI_90      MACD  Signal_Line  MACD_Histogram  MACD_rolling_14  \\\n",
       "0     NaN  0.000000     0.000000        0.000000         0.000000   \n",
       "1     NaN  0.057037     0.011407        0.045630         0.028519   \n",
       "2     NaN  0.132584     0.035643        0.096941         0.063207   \n",
       "3     NaN  0.202627     0.069040        0.133588         0.098062   \n",
       "4     NaN  0.270153     0.109262        0.160890         0.132480   \n",
       "\n",
       "   Signal_rolling_14  MACD_Histogram_rolling_14  MACD_rolling_20  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_20  MACD_Histogram_rolling_20  MACD_rolling_30  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_30  MACD_Histogram_rolling_30  MACD_rolling_50  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_50  MACD_Histogram_rolling_50  MACD_rolling_60  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_60  MACD_Histogram_rolling_60  MACD_rolling_90  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_90  MACD_Histogram_rolling_90         %K         %D  \\\n",
       "0           0.000000                   0.000000  93.467084  93.467084   \n",
       "1           0.005704                   0.022815  99.159644  96.313364   \n",
       "2           0.015683                   0.047524  86.065515  92.897414   \n",
       "3           0.029022                   0.069040  94.535498  93.253552   \n",
       "4           0.045070                   0.087410  87.958682  89.519898   \n",
       "\n",
       "       %K_20      %D_20      %K_30      %D_30      %K_50      %D_50  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  93.467084  93.467084   \n",
       "1  99.159644  96.313364  99.159644  96.313364  99.159644  96.313364   \n",
       "2  86.065515  92.897414  86.065515  92.897414  86.065515  92.897414   \n",
       "3  94.535498  93.253552  94.535498  93.253552  94.535498  93.253552   \n",
       "4  87.958682  89.519898  87.958682  89.519898  87.958682  89.519898   \n",
       "\n",
       "       %K_60      %D_60      %K_90      %D_90       VWAP    VWAP_14  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  30.504999  30.504999   \n",
       "1  99.159644  96.313364  99.159644  96.313364  30.892965  30.698982   \n",
       "2  86.065515  92.897414  86.065515  92.897414  31.148973  30.848979   \n",
       "3  94.535498  93.253552  94.535498  93.253552  31.276489  30.955857   \n",
       "4  87.958682  89.519898  87.958682  89.519898  31.407822  31.046250   \n",
       "\n",
       "     VWAP_20    VWAP_30    VWAP_50    VWAP_60    VWAP_90  bb_Middle_Band_14  \\\n",
       "0  30.504999  30.504999  30.504999  30.504999  30.504999          30.504999   \n",
       "1  30.698982  30.698982  30.698982  30.698982  30.698982          30.862499   \n",
       "2  30.848979  30.848979  30.848979  30.848979  30.848979          31.113333   \n",
       "3  30.955857  30.955857  30.955857  30.955857  30.955857          31.277500   \n",
       "4  31.046250  31.046250  31.046250  31.046250  31.046250          31.413500   \n",
       "\n",
       "   Std_Dev_14  Upper_Band_14  Lower_Band_14  bb_Middle_Band_20  Std_Dev_20  \\\n",
       "0         NaN            NaN            NaN          30.504999         NaN   \n",
       "1    0.505581      31.873662      29.851336          30.862499    0.505581   \n",
       "2    0.562635      32.238604      29.988062          31.113333    0.562635   \n",
       "3    0.564661      32.406822      30.148177          31.277500    0.564661   \n",
       "4    0.575858      32.565215      30.261785          31.413500    0.575858   \n",
       "\n",
       "   Upper_Band_20  Lower_Band_20  bb_Middle_Band_30  Std_Dev_30  Upper_Band_30  \\\n",
       "0            NaN            NaN          30.504999         NaN            NaN   \n",
       "1      31.873662      29.851336          30.862499    0.505581      31.873662   \n",
       "2      32.238604      29.988062          31.113333    0.562635      32.238604   \n",
       "3      32.406822      30.148177          31.277500    0.564661      32.406822   \n",
       "4      32.565215      30.261785          31.413500    0.575858      32.565215   \n",
       "\n",
       "   Lower_Band_30  bb_Middle_Band_50  Std_Dev_50  Upper_Band_50  Lower_Band_50  \\\n",
       "0            NaN          30.504999         NaN            NaN            NaN   \n",
       "1      29.851336          30.862499    0.505581      31.873662      29.851336   \n",
       "2      29.988062          31.113333    0.562635      32.238604      29.988062   \n",
       "3      30.148177          31.277500    0.564661      32.406822      30.148177   \n",
       "4      30.261785          31.413500    0.575858      32.565215      30.261785   \n",
       "\n",
       "   bb_Middle_Band_60  Std_Dev_60  Upper_Band_60  Lower_Band_60  \\\n",
       "0          30.504999         NaN            NaN            NaN   \n",
       "1          30.862499    0.505581      31.873662      29.851336   \n",
       "2          31.113333    0.562635      32.238604      29.988062   \n",
       "3          31.277500    0.564661      32.406822      30.148177   \n",
       "4          31.413500    0.575858      32.565215      30.261785   \n",
       "\n",
       "   bb_Middle_Band_90  Std_Dev_90  Upper_Band_90  Lower_Band_90       ATR  \\\n",
       "0          30.504999         NaN            NaN            NaN  0.497499   \n",
       "1          30.862499    0.505581      31.873662      29.851336  0.725000   \n",
       "2          31.113333    0.562635      32.238604      29.988062  0.650002   \n",
       "3          31.277500    0.564661      32.406822      30.148177  0.407499   \n",
       "4          31.413500    0.575858      32.565215      30.261785  0.490002   \n",
       "\n",
       "     ATR_14    ATR_20    ATR_30    ATR_50    ATR_60    ATR_90  \n",
       "0  0.497499  0.497499  0.497499  0.497499  0.497499  0.497499  \n",
       "1  0.611250  0.611250  0.611250  0.611250  0.611250  0.611250  \n",
       "2  0.624167  0.624167  0.624167  0.624167  0.624167  0.624167  \n",
       "3  0.570000  0.570000  0.570000  0.570000  0.570000  0.570000  \n",
       "4  0.554000  0.554000  0.554000  0.554000  0.554000  0.554000  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = None  # Show all columns\n",
    "\n",
    "stock_data_3_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e15a4085-12e8-4e7d-97f1-93e45e193eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# def calculate_fibonacci_retracement(df, windows):\n",
    "#     # Convert relevant columns to numeric\n",
    "#     df['Daily_High'] = pd.to_numeric(df['Daily_High'], errors='coerce')\n",
    "#     df['Daily_Low'] = pd.to_numeric(df['Daily_Low'], errors='coerce')\n",
    "\n",
    "#     # Define Fibonacci levels\n",
    "#     fib_levels = [0.236, 0.382, 0.500, 0.618, 0.786, 1.000, 1.618, 2.618, 4.236]\n",
    "\n",
    "#     # Group by 'Symbol' and calculate Fibonacci levels for a given window\n",
    "#     def fib_retracement(stock_df, window):\n",
    "#         stock_df[f'Fib_{window}_High_Max'] = stock_df['Daily_High'].transform(\n",
    "#             lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "#         )\n",
    "#         stock_df[f'Fib_{window}_Low_Min'] = stock_df['Daily_Low'].transform(\n",
    "#             lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "#         )\n",
    "\n",
    "#         # Calculate Fibonacci retracement levels for each level\n",
    "#         for level in fib_levels:\n",
    "#             stock_df[f'{window}_day_Fib_{int(level*100)}'] = stock_df[f'Fib_{window}_High_Max'] - (\n",
    "#                 level * (stock_df[f'Fib_{window}_High_Max'] - stock_df[f'Fib_{window}_Low_Min']))\n",
    "#         stock_df.drop(columns = [f'Fib_{window}_High_Max', f'Fib_{window}_Low_Min'], inplace=True)\n",
    "\n",
    "#         return stock_df\n",
    "\n",
    "#     # Apply the function to each stock symbol and window\n",
    "#     for window in windows:\n",
    "#         df = df.groupby('Symbol', group_keys=False).apply(fib_retracement, window)\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# Group by 'Symbol' and calculate Fibonacci levels for a given window\n",
    "def fib_retracement(df, windows, levels):\n",
    "    for window in windows:\n",
    "        df[f'Fib_{window}_High_Max'] = df['Daily_High'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "        )\n",
    "        df[f'Fib_{window}_Low_Min'] = df['Daily_Low'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "        )\n",
    "    \n",
    "        # Calculate Fibonacci retracement levels for each level\n",
    "        for level in fib_levels:\n",
    "            df[f'{window}_day_Fib_{level*100:.1f}%'] = df[f'Fib_{window}_High_Max'] - (\n",
    "                level * (df[f'Fib_{window}_High_Max'] - df[f'Fib_{window}_Low_Min']))\n",
    "        df.drop(columns = [f'Fib_{window}_High_Max', f'Fib_{window}_Low_Min'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Define Fibonacci levels\n",
    "fib_levels = [0.236, 0.382, 0.500, 0.618, 0.786, 1.000, 1.618, 2.618, 4.236]\n",
    "fib_windows = [5, 14, 30]\n",
    "\n",
    "for df in stock_dataframes:\n",
    "    fib_retracement(df, fib_windows, fib_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d11f959e-be42-4799-9273-e01afc55d667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Daily_High</th>\n",
       "      <th>Daily_Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volume_14day_avg</th>\n",
       "      <th>Volume_20day_avg</th>\n",
       "      <th>Volume_30day_avg</th>\n",
       "      <th>Volume_50day_avg</th>\n",
       "      <th>Volume_60day_avg</th>\n",
       "      <th>Volume_90day_avg</th>\n",
       "      <th>Daily_High_14day_avg</th>\n",
       "      <th>Daily_High_20day_avg</th>\n",
       "      <th>Daily_High_30day_avg</th>\n",
       "      <th>Daily_High_50day_avg</th>\n",
       "      <th>Daily_High_60day_avg</th>\n",
       "      <th>Daily_High_90day_avg</th>\n",
       "      <th>Daily_Low_14day_avg</th>\n",
       "      <th>Daily_Low_20day_avg</th>\n",
       "      <th>Daily_Low_30day_avg</th>\n",
       "      <th>Daily_Low_50day_avg</th>\n",
       "      <th>Daily_Low_60day_avg</th>\n",
       "      <th>Daily_Low_90day_avg</th>\n",
       "      <th>SMA_14</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_30</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>SMA_60</th>\n",
       "      <th>SMA_90</th>\n",
       "      <th>EMA_14</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>EMA_30</th>\n",
       "      <th>EMA_50</th>\n",
       "      <th>EMA_60</th>\n",
       "      <th>EMA_90</th>\n",
       "      <th>RSI</th>\n",
       "      <th>RSI_20</th>\n",
       "      <th>RSI_30</th>\n",
       "      <th>RSI_50</th>\n",
       "      <th>RSI_60</th>\n",
       "      <th>RSI_90</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "      <th>MACD_rolling_14</th>\n",
       "      <th>Signal_rolling_14</th>\n",
       "      <th>MACD_Histogram_rolling_14</th>\n",
       "      <th>MACD_rolling_20</th>\n",
       "      <th>Signal_rolling_20</th>\n",
       "      <th>MACD_Histogram_rolling_20</th>\n",
       "      <th>MACD_rolling_30</th>\n",
       "      <th>Signal_rolling_30</th>\n",
       "      <th>MACD_Histogram_rolling_30</th>\n",
       "      <th>MACD_rolling_50</th>\n",
       "      <th>Signal_rolling_50</th>\n",
       "      <th>MACD_Histogram_rolling_50</th>\n",
       "      <th>MACD_rolling_60</th>\n",
       "      <th>Signal_rolling_60</th>\n",
       "      <th>MACD_Histogram_rolling_60</th>\n",
       "      <th>MACD_rolling_90</th>\n",
       "      <th>Signal_rolling_90</th>\n",
       "      <th>MACD_Histogram_rolling_90</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>%K_20</th>\n",
       "      <th>%D_20</th>\n",
       "      <th>%K_30</th>\n",
       "      <th>%D_30</th>\n",
       "      <th>%K_50</th>\n",
       "      <th>%D_50</th>\n",
       "      <th>%K_60</th>\n",
       "      <th>%D_60</th>\n",
       "      <th>%K_90</th>\n",
       "      <th>%D_90</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>VWAP_14</th>\n",
       "      <th>VWAP_20</th>\n",
       "      <th>VWAP_30</th>\n",
       "      <th>VWAP_50</th>\n",
       "      <th>VWAP_60</th>\n",
       "      <th>VWAP_90</th>\n",
       "      <th>bb_Middle_Band_14</th>\n",
       "      <th>Std_Dev_14</th>\n",
       "      <th>Upper_Band_14</th>\n",
       "      <th>Lower_Band_14</th>\n",
       "      <th>bb_Middle_Band_20</th>\n",
       "      <th>Std_Dev_20</th>\n",
       "      <th>Upper_Band_20</th>\n",
       "      <th>Lower_Band_20</th>\n",
       "      <th>bb_Middle_Band_30</th>\n",
       "      <th>Std_Dev_30</th>\n",
       "      <th>Upper_Band_30</th>\n",
       "      <th>Lower_Band_30</th>\n",
       "      <th>bb_Middle_Band_50</th>\n",
       "      <th>Std_Dev_50</th>\n",
       "      <th>Upper_Band_50</th>\n",
       "      <th>Lower_Band_50</th>\n",
       "      <th>bb_Middle_Band_60</th>\n",
       "      <th>Std_Dev_60</th>\n",
       "      <th>Upper_Band_60</th>\n",
       "      <th>Lower_Band_60</th>\n",
       "      <th>bb_Middle_Band_90</th>\n",
       "      <th>Std_Dev_90</th>\n",
       "      <th>Upper_Band_90</th>\n",
       "      <th>Lower_Band_90</th>\n",
       "      <th>ATR</th>\n",
       "      <th>ATR_14</th>\n",
       "      <th>ATR_20</th>\n",
       "      <th>ATR_30</th>\n",
       "      <th>ATR_50</th>\n",
       "      <th>ATR_60</th>\n",
       "      <th>ATR_90</th>\n",
       "      <th>5_day_Fib_23.6%</th>\n",
       "      <th>5_day_Fib_38.2%</th>\n",
       "      <th>5_day_Fib_50.0%</th>\n",
       "      <th>5_day_Fib_61.8%</th>\n",
       "      <th>5_day_Fib_78.6%</th>\n",
       "      <th>5_day_Fib_100.0%</th>\n",
       "      <th>5_day_Fib_161.8%</th>\n",
       "      <th>5_day_Fib_261.8%</th>\n",
       "      <th>5_day_Fib_423.6%</th>\n",
       "      <th>14_day_Fib_23.6%</th>\n",
       "      <th>14_day_Fib_38.2%</th>\n",
       "      <th>14_day_Fib_50.0%</th>\n",
       "      <th>14_day_Fib_61.8%</th>\n",
       "      <th>14_day_Fib_78.6%</th>\n",
       "      <th>14_day_Fib_100.0%</th>\n",
       "      <th>14_day_Fib_161.8%</th>\n",
       "      <th>14_day_Fib_261.8%</th>\n",
       "      <th>14_day_Fib_423.6%</th>\n",
       "      <th>30_day_Fib_23.6%</th>\n",
       "      <th>30_day_Fib_38.2%</th>\n",
       "      <th>30_day_Fib_50.0%</th>\n",
       "      <th>30_day_Fib_61.8%</th>\n",
       "      <th>30_day_Fib_78.6%</th>\n",
       "      <th>30_day_Fib_100.0%</th>\n",
       "      <th>30_day_Fib_161.8%</th>\n",
       "      <th>30_day_Fib_261.8%</th>\n",
       "      <th>30_day_Fib_423.6%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>248034000</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>294247200</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.600333</td>\n",
       "      <td>30.573094</td>\n",
       "      <td>30.551128</td>\n",
       "      <td>30.533038</td>\n",
       "      <td>30.528442</td>\n",
       "      <td>30.520713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.045630</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>30.892965</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>297898000</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>30.735621</td>\n",
       "      <td>30.672323</td>\n",
       "      <td>30.619765</td>\n",
       "      <td>30.575468</td>\n",
       "      <td>30.564067</td>\n",
       "      <td>30.544764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.132584</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>31.148973</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>0.650002</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>217088800</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>30.873539</td>\n",
       "      <td>30.776864</td>\n",
       "      <td>30.693974</td>\n",
       "      <td>30.622313</td>\n",
       "      <td>30.603605</td>\n",
       "      <td>30.571692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202627</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.133588</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>31.276489</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>0.407499</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>252609600</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.018067</td>\n",
       "      <td>30.889306</td>\n",
       "      <td>30.775492</td>\n",
       "      <td>30.674673</td>\n",
       "      <td>30.647995</td>\n",
       "      <td>30.602149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.270153</td>\n",
       "      <td>0.109262</td>\n",
       "      <td>0.160890</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>31.407822</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>0.490002</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol       Date      Close  Daily_High  Daily_Low     Volume  \\\n",
       "0   AAPL 2015-02-10  30.504999   30.537500  30.040001  248034000   \n",
       "1   AAPL 2015-02-11  31.219999   31.230000  30.625000  294247200   \n",
       "2   AAPL 2015-02-12  31.615000   31.870001  31.392500  297898000   \n",
       "3   AAPL 2015-02-13  31.770000   31.820000  31.412500  217088800   \n",
       "4   AAPL 2015-02-17  31.957500   32.220001  31.730000  252609600   \n",
       "\n",
       "   Volume_14day_avg  Volume_20day_avg  Volume_30day_avg  Volume_50day_avg  \\\n",
       "0      2.480340e+08      2.480340e+08      2.480340e+08      2.480340e+08   \n",
       "1      2.711406e+08      2.711406e+08      2.711406e+08      2.711406e+08   \n",
       "2      2.800597e+08      2.800597e+08      2.800597e+08      2.800597e+08   \n",
       "3      2.643170e+08      2.643170e+08      2.643170e+08      2.643170e+08   \n",
       "4      2.619755e+08      2.619755e+08      2.619755e+08      2.619755e+08   \n",
       "\n",
       "   Volume_60day_avg  Volume_90day_avg  Daily_High_14day_avg  \\\n",
       "0      2.480340e+08      2.480340e+08             30.537500   \n",
       "1      2.711406e+08      2.711406e+08             30.883750   \n",
       "2      2.800597e+08      2.800597e+08             31.212500   \n",
       "3      2.643170e+08      2.643170e+08             31.364375   \n",
       "4      2.619755e+08      2.619755e+08             31.535500   \n",
       "\n",
       "   Daily_High_20day_avg  Daily_High_30day_avg  Daily_High_50day_avg  \\\n",
       "0             30.537500             30.537500             30.537500   \n",
       "1             30.883750             30.883750             30.883750   \n",
       "2             31.212500             31.212500             31.212500   \n",
       "3             31.364375             31.364375             31.364375   \n",
       "4             31.535500             31.535500             31.535500   \n",
       "\n",
       "   Daily_High_60day_avg  Daily_High_90day_avg  Daily_Low_14day_avg  \\\n",
       "0             30.537500             30.537500            30.040001   \n",
       "1             30.883750             30.883750            30.332500   \n",
       "2             31.212500             31.212500            30.685834   \n",
       "3             31.364375             31.364375            30.867500   \n",
       "4             31.535500             31.535500            31.040000   \n",
       "\n",
       "   Daily_Low_20day_avg  Daily_Low_30day_avg  Daily_Low_50day_avg  \\\n",
       "0            30.040001            30.040001            30.040001   \n",
       "1            30.332500            30.332500            30.332500   \n",
       "2            30.685834            30.685834            30.685834   \n",
       "3            30.867500            30.867500            30.867500   \n",
       "4            31.040000            31.040000            31.040000   \n",
       "\n",
       "   Daily_Low_60day_avg  Daily_Low_90day_avg     SMA_14     SMA_20     SMA_30  \\\n",
       "0            30.040001            30.040001  30.504999  30.504999  30.504999   \n",
       "1            30.332500            30.332500  30.862499  30.862499  30.862499   \n",
       "2            30.685834            30.685834  31.113333  31.113333  31.113333   \n",
       "3            30.867500            30.867500  31.277500  31.277500  31.277500   \n",
       "4            31.040000            31.040000  31.413500  31.413500  31.413500   \n",
       "\n",
       "      SMA_50     SMA_60     SMA_90     EMA_14     EMA_20     EMA_30  \\\n",
       "0  30.504999  30.504999  30.504999  30.504999  30.504999  30.504999   \n",
       "1  30.862499  30.862499  30.862499  30.600333  30.573094  30.551128   \n",
       "2  31.113333  31.113333  31.113333  30.735621  30.672323  30.619765   \n",
       "3  31.277500  31.277500  31.277500  30.873539  30.776864  30.693974   \n",
       "4  31.413500  31.413500  31.413500  31.018067  30.889306  30.775492   \n",
       "\n",
       "      EMA_50     EMA_60     EMA_90  RSI  RSI_20  RSI_30  RSI_50  RSI_60  \\\n",
       "0  30.504999  30.504999  30.504999  NaN     NaN     NaN     NaN     NaN   \n",
       "1  30.533038  30.528442  30.520713  NaN     NaN     NaN     NaN     NaN   \n",
       "2  30.575468  30.564067  30.544764  NaN     NaN     NaN     NaN     NaN   \n",
       "3  30.622313  30.603605  30.571692  NaN     NaN     NaN     NaN     NaN   \n",
       "4  30.674673  30.647995  30.602149  NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   RSI_90      MACD  Signal_Line  MACD_Histogram  MACD_rolling_14  \\\n",
       "0     NaN  0.000000     0.000000        0.000000         0.000000   \n",
       "1     NaN  0.057037     0.011407        0.045630         0.028519   \n",
       "2     NaN  0.132584     0.035643        0.096941         0.063207   \n",
       "3     NaN  0.202627     0.069040        0.133588         0.098062   \n",
       "4     NaN  0.270153     0.109262        0.160890         0.132480   \n",
       "\n",
       "   Signal_rolling_14  MACD_Histogram_rolling_14  MACD_rolling_20  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_20  MACD_Histogram_rolling_20  MACD_rolling_30  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_30  MACD_Histogram_rolling_30  MACD_rolling_50  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_50  MACD_Histogram_rolling_50  MACD_rolling_60  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_60  MACD_Histogram_rolling_60  MACD_rolling_90  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_90  MACD_Histogram_rolling_90         %K         %D  \\\n",
       "0           0.000000                   0.000000  93.467084  93.467084   \n",
       "1           0.005704                   0.022815  99.159644  96.313364   \n",
       "2           0.015683                   0.047524  86.065515  92.897414   \n",
       "3           0.029022                   0.069040  94.535498  93.253552   \n",
       "4           0.045070                   0.087410  87.958682  89.519898   \n",
       "\n",
       "       %K_20      %D_20      %K_30      %D_30      %K_50      %D_50  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  93.467084  93.467084   \n",
       "1  99.159644  96.313364  99.159644  96.313364  99.159644  96.313364   \n",
       "2  86.065515  92.897414  86.065515  92.897414  86.065515  92.897414   \n",
       "3  94.535498  93.253552  94.535498  93.253552  94.535498  93.253552   \n",
       "4  87.958682  89.519898  87.958682  89.519898  87.958682  89.519898   \n",
       "\n",
       "       %K_60      %D_60      %K_90      %D_90       VWAP    VWAP_14  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  30.504999  30.504999   \n",
       "1  99.159644  96.313364  99.159644  96.313364  30.892965  30.698982   \n",
       "2  86.065515  92.897414  86.065515  92.897414  31.148973  30.848979   \n",
       "3  94.535498  93.253552  94.535498  93.253552  31.276489  30.955857   \n",
       "4  87.958682  89.519898  87.958682  89.519898  31.407822  31.046250   \n",
       "\n",
       "     VWAP_20    VWAP_30    VWAP_50    VWAP_60    VWAP_90  bb_Middle_Band_14  \\\n",
       "0  30.504999  30.504999  30.504999  30.504999  30.504999          30.504999   \n",
       "1  30.698982  30.698982  30.698982  30.698982  30.698982          30.862499   \n",
       "2  30.848979  30.848979  30.848979  30.848979  30.848979          31.113333   \n",
       "3  30.955857  30.955857  30.955857  30.955857  30.955857          31.277500   \n",
       "4  31.046250  31.046250  31.046250  31.046250  31.046250          31.413500   \n",
       "\n",
       "   Std_Dev_14  Upper_Band_14  Lower_Band_14  bb_Middle_Band_20  Std_Dev_20  \\\n",
       "0         NaN            NaN            NaN          30.504999         NaN   \n",
       "1    0.505581      31.873662      29.851336          30.862499    0.505581   \n",
       "2    0.562635      32.238604      29.988062          31.113333    0.562635   \n",
       "3    0.564661      32.406822      30.148177          31.277500    0.564661   \n",
       "4    0.575858      32.565215      30.261785          31.413500    0.575858   \n",
       "\n",
       "   Upper_Band_20  Lower_Band_20  bb_Middle_Band_30  Std_Dev_30  Upper_Band_30  \\\n",
       "0            NaN            NaN          30.504999         NaN            NaN   \n",
       "1      31.873662      29.851336          30.862499    0.505581      31.873662   \n",
       "2      32.238604      29.988062          31.113333    0.562635      32.238604   \n",
       "3      32.406822      30.148177          31.277500    0.564661      32.406822   \n",
       "4      32.565215      30.261785          31.413500    0.575858      32.565215   \n",
       "\n",
       "   Lower_Band_30  bb_Middle_Band_50  Std_Dev_50  Upper_Band_50  Lower_Band_50  \\\n",
       "0            NaN          30.504999         NaN            NaN            NaN   \n",
       "1      29.851336          30.862499    0.505581      31.873662      29.851336   \n",
       "2      29.988062          31.113333    0.562635      32.238604      29.988062   \n",
       "3      30.148177          31.277500    0.564661      32.406822      30.148177   \n",
       "4      30.261785          31.413500    0.575858      32.565215      30.261785   \n",
       "\n",
       "   bb_Middle_Band_60  Std_Dev_60  Upper_Band_60  Lower_Band_60  \\\n",
       "0          30.504999         NaN            NaN            NaN   \n",
       "1          30.862499    0.505581      31.873662      29.851336   \n",
       "2          31.113333    0.562635      32.238604      29.988062   \n",
       "3          31.277500    0.564661      32.406822      30.148177   \n",
       "4          31.413500    0.575858      32.565215      30.261785   \n",
       "\n",
       "   bb_Middle_Band_90  Std_Dev_90  Upper_Band_90  Lower_Band_90       ATR  \\\n",
       "0          30.504999         NaN            NaN            NaN  0.497499   \n",
       "1          30.862499    0.505581      31.873662      29.851336  0.725000   \n",
       "2          31.113333    0.562635      32.238604      29.988062  0.650002   \n",
       "3          31.277500    0.564661      32.406822      30.148177  0.407499   \n",
       "4          31.413500    0.575858      32.565215      30.261785  0.490002   \n",
       "\n",
       "     ATR_14    ATR_20    ATR_30    ATR_50    ATR_60    ATR_90  \\\n",
       "0  0.497499  0.497499  0.497499  0.497499  0.497499  0.497499   \n",
       "1  0.611250  0.611250  0.611250  0.611250  0.611250  0.611250   \n",
       "2  0.624167  0.624167  0.624167  0.624167  0.624167  0.624167   \n",
       "3  0.570000  0.570000  0.570000  0.570000  0.570000  0.570000   \n",
       "4  0.554000  0.554000  0.554000  0.554000  0.554000  0.554000   \n",
       "\n",
       "   5_day_Fib_23.6%  5_day_Fib_38.2%  5_day_Fib_50.0%  5_day_Fib_61.8%  \\\n",
       "0        30.420091        30.347456        30.288751        30.230046   \n",
       "1        30.949160        30.775420        30.635000        30.494580   \n",
       "2        31.438121        31.170941        30.955001        30.739061   \n",
       "3        31.438121        31.170941        30.955001        30.739061   \n",
       "4        31.705521        31.387241        31.130001        30.872761   \n",
       "\n",
       "   5_day_Fib_78.6%  5_day_Fib_100.0%  5_day_Fib_161.8%  5_day_Fib_261.8%  \\\n",
       "0        30.146466         30.040001         29.732546         29.235047   \n",
       "1        30.294661         30.040001         29.304582         28.114583   \n",
       "2        30.431621         30.040001         28.909061         27.079061   \n",
       "3        30.431621         30.040001         28.909061         27.079061   \n",
       "4        30.506521         30.040001         28.692761         26.512760   \n",
       "\n",
       "   5_day_Fib_423.6%  14_day_Fib_23.6%  14_day_Fib_38.2%  14_day_Fib_50.0%  \\\n",
       "0         28.430093         30.420091         30.347456         30.288751   \n",
       "1         26.189165         30.949160         30.775420         30.635000   \n",
       "2         24.118121         31.438121         31.170941         30.955001   \n",
       "3         24.118121         31.438121         31.170941         30.955001   \n",
       "4         22.985520         31.705521         31.387241         31.130001   \n",
       "\n",
       "   14_day_Fib_61.8%  14_day_Fib_78.6%  14_day_Fib_100.0%  14_day_Fib_161.8%  \\\n",
       "0         30.230046         30.146466          30.040001          29.732546   \n",
       "1         30.494580         30.294661          30.040001          29.304582   \n",
       "2         30.739061         30.431621          30.040001          28.909061   \n",
       "3         30.739061         30.431621          30.040001          28.909061   \n",
       "4         30.872761         30.506521          30.040001          28.692761   \n",
       "\n",
       "   14_day_Fib_261.8%  14_day_Fib_423.6%  30_day_Fib_23.6%  30_day_Fib_38.2%  \\\n",
       "0          29.235047          28.430093         30.420091         30.347456   \n",
       "1          28.114583          26.189165         30.949160         30.775420   \n",
       "2          27.079061          24.118121         31.438121         31.170941   \n",
       "3          27.079061          24.118121         31.438121         31.170941   \n",
       "4          26.512760          22.985520         31.705521         31.387241   \n",
       "\n",
       "   30_day_Fib_50.0%  30_day_Fib_61.8%  30_day_Fib_78.6%  30_day_Fib_100.0%  \\\n",
       "0         30.288751         30.230046         30.146466          30.040001   \n",
       "1         30.635000         30.494580         30.294661          30.040001   \n",
       "2         30.955001         30.739061         30.431621          30.040001   \n",
       "3         30.955001         30.739061         30.431621          30.040001   \n",
       "4         31.130001         30.872761         30.506521          30.040001   \n",
       "\n",
       "   30_day_Fib_161.8%  30_day_Fib_261.8%  30_day_Fib_423.6%  \n",
       "0          29.732546          29.235047          28.430093  \n",
       "1          29.304582          28.114583          26.189165  \n",
       "2          28.909061          27.079061          24.118121  \n",
       "3          28.909061          27.079061          24.118121  \n",
       "4          28.692761          26.512760          22.985520  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "stock_data_3_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95450a29-d142-4d5b-b665-dc4add35c92b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OBV\n",
    "\n",
    "def calculate_obv(df):\n",
    "    df['OBV'] = df.groupby('Symbol').apply(\n",
    "        lambda group: (np.sign(group['Close'].diff()) * group['Volume']).cumsum()\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    df['OBV'] = df['OBV'].fillna(0) \n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_rolling_obv(df, windows):\n",
    "    df[f'OBV_{window}day_avg'] = df.groupby('Symbol')['OBV'].transform(\n",
    "        lambda x: x.rolling(window=window).mean()\n",
    "    )\n",
    "\n",
    "    df[f'OBV_{window}day_avg'] = df[f'OBV_{window}day_avg'].fillna(0)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    calculate_obv(df)\n",
    "    for window in windows:\n",
    "        calculate_rolling_obv(df, window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3804d661-ebfd-47f2-97c3-84b53bdb17dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Daily_High</th>\n",
       "      <th>Daily_Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volume_14day_avg</th>\n",
       "      <th>Volume_20day_avg</th>\n",
       "      <th>Volume_30day_avg</th>\n",
       "      <th>Volume_50day_avg</th>\n",
       "      <th>Volume_60day_avg</th>\n",
       "      <th>Volume_90day_avg</th>\n",
       "      <th>Daily_High_14day_avg</th>\n",
       "      <th>Daily_High_20day_avg</th>\n",
       "      <th>Daily_High_30day_avg</th>\n",
       "      <th>Daily_High_50day_avg</th>\n",
       "      <th>Daily_High_60day_avg</th>\n",
       "      <th>Daily_High_90day_avg</th>\n",
       "      <th>Daily_Low_14day_avg</th>\n",
       "      <th>Daily_Low_20day_avg</th>\n",
       "      <th>Daily_Low_30day_avg</th>\n",
       "      <th>Daily_Low_50day_avg</th>\n",
       "      <th>Daily_Low_60day_avg</th>\n",
       "      <th>Daily_Low_90day_avg</th>\n",
       "      <th>SMA_14</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_30</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>SMA_60</th>\n",
       "      <th>SMA_90</th>\n",
       "      <th>EMA_14</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>EMA_30</th>\n",
       "      <th>EMA_50</th>\n",
       "      <th>EMA_60</th>\n",
       "      <th>EMA_90</th>\n",
       "      <th>RSI</th>\n",
       "      <th>RSI_20</th>\n",
       "      <th>RSI_30</th>\n",
       "      <th>RSI_50</th>\n",
       "      <th>RSI_60</th>\n",
       "      <th>RSI_90</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "      <th>MACD_rolling_14</th>\n",
       "      <th>Signal_rolling_14</th>\n",
       "      <th>MACD_Histogram_rolling_14</th>\n",
       "      <th>MACD_rolling_20</th>\n",
       "      <th>Signal_rolling_20</th>\n",
       "      <th>MACD_Histogram_rolling_20</th>\n",
       "      <th>MACD_rolling_30</th>\n",
       "      <th>Signal_rolling_30</th>\n",
       "      <th>MACD_Histogram_rolling_30</th>\n",
       "      <th>MACD_rolling_50</th>\n",
       "      <th>Signal_rolling_50</th>\n",
       "      <th>MACD_Histogram_rolling_50</th>\n",
       "      <th>MACD_rolling_60</th>\n",
       "      <th>Signal_rolling_60</th>\n",
       "      <th>MACD_Histogram_rolling_60</th>\n",
       "      <th>MACD_rolling_90</th>\n",
       "      <th>Signal_rolling_90</th>\n",
       "      <th>MACD_Histogram_rolling_90</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>%K_20</th>\n",
       "      <th>%D_20</th>\n",
       "      <th>%K_30</th>\n",
       "      <th>%D_30</th>\n",
       "      <th>%K_50</th>\n",
       "      <th>%D_50</th>\n",
       "      <th>%K_60</th>\n",
       "      <th>%D_60</th>\n",
       "      <th>%K_90</th>\n",
       "      <th>%D_90</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>VWAP_14</th>\n",
       "      <th>VWAP_20</th>\n",
       "      <th>VWAP_30</th>\n",
       "      <th>VWAP_50</th>\n",
       "      <th>VWAP_60</th>\n",
       "      <th>VWAP_90</th>\n",
       "      <th>bb_Middle_Band_14</th>\n",
       "      <th>Std_Dev_14</th>\n",
       "      <th>Upper_Band_14</th>\n",
       "      <th>Lower_Band_14</th>\n",
       "      <th>bb_Middle_Band_20</th>\n",
       "      <th>Std_Dev_20</th>\n",
       "      <th>Upper_Band_20</th>\n",
       "      <th>Lower_Band_20</th>\n",
       "      <th>bb_Middle_Band_30</th>\n",
       "      <th>Std_Dev_30</th>\n",
       "      <th>Upper_Band_30</th>\n",
       "      <th>Lower_Band_30</th>\n",
       "      <th>bb_Middle_Band_50</th>\n",
       "      <th>Std_Dev_50</th>\n",
       "      <th>Upper_Band_50</th>\n",
       "      <th>Lower_Band_50</th>\n",
       "      <th>bb_Middle_Band_60</th>\n",
       "      <th>Std_Dev_60</th>\n",
       "      <th>Upper_Band_60</th>\n",
       "      <th>Lower_Band_60</th>\n",
       "      <th>bb_Middle_Band_90</th>\n",
       "      <th>Std_Dev_90</th>\n",
       "      <th>Upper_Band_90</th>\n",
       "      <th>Lower_Band_90</th>\n",
       "      <th>ATR</th>\n",
       "      <th>ATR_14</th>\n",
       "      <th>ATR_20</th>\n",
       "      <th>ATR_30</th>\n",
       "      <th>ATR_50</th>\n",
       "      <th>ATR_60</th>\n",
       "      <th>ATR_90</th>\n",
       "      <th>5_day_Fib_23.6%</th>\n",
       "      <th>5_day_Fib_38.2%</th>\n",
       "      <th>5_day_Fib_50.0%</th>\n",
       "      <th>5_day_Fib_61.8%</th>\n",
       "      <th>5_day_Fib_78.6%</th>\n",
       "      <th>5_day_Fib_100.0%</th>\n",
       "      <th>5_day_Fib_161.8%</th>\n",
       "      <th>5_day_Fib_261.8%</th>\n",
       "      <th>5_day_Fib_423.6%</th>\n",
       "      <th>14_day_Fib_23.6%</th>\n",
       "      <th>14_day_Fib_38.2%</th>\n",
       "      <th>14_day_Fib_50.0%</th>\n",
       "      <th>14_day_Fib_61.8%</th>\n",
       "      <th>14_day_Fib_78.6%</th>\n",
       "      <th>14_day_Fib_100.0%</th>\n",
       "      <th>14_day_Fib_161.8%</th>\n",
       "      <th>14_day_Fib_261.8%</th>\n",
       "      <th>14_day_Fib_423.6%</th>\n",
       "      <th>30_day_Fib_23.6%</th>\n",
       "      <th>30_day_Fib_38.2%</th>\n",
       "      <th>30_day_Fib_50.0%</th>\n",
       "      <th>30_day_Fib_61.8%</th>\n",
       "      <th>30_day_Fib_78.6%</th>\n",
       "      <th>30_day_Fib_100.0%</th>\n",
       "      <th>30_day_Fib_161.8%</th>\n",
       "      <th>30_day_Fib_261.8%</th>\n",
       "      <th>30_day_Fib_423.6%</th>\n",
       "      <th>OBV</th>\n",
       "      <th>OBV_14day_avg</th>\n",
       "      <th>OBV_20day_avg</th>\n",
       "      <th>OBV_30day_avg</th>\n",
       "      <th>OBV_50day_avg</th>\n",
       "      <th>OBV_60day_avg</th>\n",
       "      <th>OBV_90day_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>248034000</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>294247200</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.600333</td>\n",
       "      <td>30.573094</td>\n",
       "      <td>30.551128</td>\n",
       "      <td>30.533038</td>\n",
       "      <td>30.528442</td>\n",
       "      <td>30.520713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.045630</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>30.892965</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>2.942472e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>297898000</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>30.735621</td>\n",
       "      <td>30.672323</td>\n",
       "      <td>30.619765</td>\n",
       "      <td>30.575468</td>\n",
       "      <td>30.564067</td>\n",
       "      <td>30.544764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.132584</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>31.148973</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>0.650002</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>5.921452e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>217088800</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>30.873539</td>\n",
       "      <td>30.776864</td>\n",
       "      <td>30.693974</td>\n",
       "      <td>30.622313</td>\n",
       "      <td>30.603605</td>\n",
       "      <td>30.571692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202627</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.133588</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>31.276489</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>0.407499</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>8.092340e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>252609600</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.018067</td>\n",
       "      <td>30.889306</td>\n",
       "      <td>30.775492</td>\n",
       "      <td>30.674673</td>\n",
       "      <td>30.647995</td>\n",
       "      <td>30.602149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.270153</td>\n",
       "      <td>0.109262</td>\n",
       "      <td>0.160890</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>31.407822</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>0.490002</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>1.061844e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol       Date      Close  Daily_High  Daily_Low     Volume  \\\n",
       "0   AAPL 2015-02-10  30.504999   30.537500  30.040001  248034000   \n",
       "1   AAPL 2015-02-11  31.219999   31.230000  30.625000  294247200   \n",
       "2   AAPL 2015-02-12  31.615000   31.870001  31.392500  297898000   \n",
       "3   AAPL 2015-02-13  31.770000   31.820000  31.412500  217088800   \n",
       "4   AAPL 2015-02-17  31.957500   32.220001  31.730000  252609600   \n",
       "\n",
       "   Volume_14day_avg  Volume_20day_avg  Volume_30day_avg  Volume_50day_avg  \\\n",
       "0      2.480340e+08      2.480340e+08      2.480340e+08      2.480340e+08   \n",
       "1      2.711406e+08      2.711406e+08      2.711406e+08      2.711406e+08   \n",
       "2      2.800597e+08      2.800597e+08      2.800597e+08      2.800597e+08   \n",
       "3      2.643170e+08      2.643170e+08      2.643170e+08      2.643170e+08   \n",
       "4      2.619755e+08      2.619755e+08      2.619755e+08      2.619755e+08   \n",
       "\n",
       "   Volume_60day_avg  Volume_90day_avg  Daily_High_14day_avg  \\\n",
       "0      2.480340e+08      2.480340e+08             30.537500   \n",
       "1      2.711406e+08      2.711406e+08             30.883750   \n",
       "2      2.800597e+08      2.800597e+08             31.212500   \n",
       "3      2.643170e+08      2.643170e+08             31.364375   \n",
       "4      2.619755e+08      2.619755e+08             31.535500   \n",
       "\n",
       "   Daily_High_20day_avg  Daily_High_30day_avg  Daily_High_50day_avg  \\\n",
       "0             30.537500             30.537500             30.537500   \n",
       "1             30.883750             30.883750             30.883750   \n",
       "2             31.212500             31.212500             31.212500   \n",
       "3             31.364375             31.364375             31.364375   \n",
       "4             31.535500             31.535500             31.535500   \n",
       "\n",
       "   Daily_High_60day_avg  Daily_High_90day_avg  Daily_Low_14day_avg  \\\n",
       "0             30.537500             30.537500            30.040001   \n",
       "1             30.883750             30.883750            30.332500   \n",
       "2             31.212500             31.212500            30.685834   \n",
       "3             31.364375             31.364375            30.867500   \n",
       "4             31.535500             31.535500            31.040000   \n",
       "\n",
       "   Daily_Low_20day_avg  Daily_Low_30day_avg  Daily_Low_50day_avg  \\\n",
       "0            30.040001            30.040001            30.040001   \n",
       "1            30.332500            30.332500            30.332500   \n",
       "2            30.685834            30.685834            30.685834   \n",
       "3            30.867500            30.867500            30.867500   \n",
       "4            31.040000            31.040000            31.040000   \n",
       "\n",
       "   Daily_Low_60day_avg  Daily_Low_90day_avg     SMA_14     SMA_20     SMA_30  \\\n",
       "0            30.040001            30.040001  30.504999  30.504999  30.504999   \n",
       "1            30.332500            30.332500  30.862499  30.862499  30.862499   \n",
       "2            30.685834            30.685834  31.113333  31.113333  31.113333   \n",
       "3            30.867500            30.867500  31.277500  31.277500  31.277500   \n",
       "4            31.040000            31.040000  31.413500  31.413500  31.413500   \n",
       "\n",
       "      SMA_50     SMA_60     SMA_90     EMA_14     EMA_20     EMA_30  \\\n",
       "0  30.504999  30.504999  30.504999  30.504999  30.504999  30.504999   \n",
       "1  30.862499  30.862499  30.862499  30.600333  30.573094  30.551128   \n",
       "2  31.113333  31.113333  31.113333  30.735621  30.672323  30.619765   \n",
       "3  31.277500  31.277500  31.277500  30.873539  30.776864  30.693974   \n",
       "4  31.413500  31.413500  31.413500  31.018067  30.889306  30.775492   \n",
       "\n",
       "      EMA_50     EMA_60     EMA_90  RSI  RSI_20  RSI_30  RSI_50  RSI_60  \\\n",
       "0  30.504999  30.504999  30.504999  NaN     NaN     NaN     NaN     NaN   \n",
       "1  30.533038  30.528442  30.520713  NaN     NaN     NaN     NaN     NaN   \n",
       "2  30.575468  30.564067  30.544764  NaN     NaN     NaN     NaN     NaN   \n",
       "3  30.622313  30.603605  30.571692  NaN     NaN     NaN     NaN     NaN   \n",
       "4  30.674673  30.647995  30.602149  NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   RSI_90      MACD  Signal_Line  MACD_Histogram  MACD_rolling_14  \\\n",
       "0     NaN  0.000000     0.000000        0.000000         0.000000   \n",
       "1     NaN  0.057037     0.011407        0.045630         0.028519   \n",
       "2     NaN  0.132584     0.035643        0.096941         0.063207   \n",
       "3     NaN  0.202627     0.069040        0.133588         0.098062   \n",
       "4     NaN  0.270153     0.109262        0.160890         0.132480   \n",
       "\n",
       "   Signal_rolling_14  MACD_Histogram_rolling_14  MACD_rolling_20  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_20  MACD_Histogram_rolling_20  MACD_rolling_30  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_30  MACD_Histogram_rolling_30  MACD_rolling_50  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_50  MACD_Histogram_rolling_50  MACD_rolling_60  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_60  MACD_Histogram_rolling_60  MACD_rolling_90  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_90  MACD_Histogram_rolling_90         %K         %D  \\\n",
       "0           0.000000                   0.000000  93.467084  93.467084   \n",
       "1           0.005704                   0.022815  99.159644  96.313364   \n",
       "2           0.015683                   0.047524  86.065515  92.897414   \n",
       "3           0.029022                   0.069040  94.535498  93.253552   \n",
       "4           0.045070                   0.087410  87.958682  89.519898   \n",
       "\n",
       "       %K_20      %D_20      %K_30      %D_30      %K_50      %D_50  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  93.467084  93.467084   \n",
       "1  99.159644  96.313364  99.159644  96.313364  99.159644  96.313364   \n",
       "2  86.065515  92.897414  86.065515  92.897414  86.065515  92.897414   \n",
       "3  94.535498  93.253552  94.535498  93.253552  94.535498  93.253552   \n",
       "4  87.958682  89.519898  87.958682  89.519898  87.958682  89.519898   \n",
       "\n",
       "       %K_60      %D_60      %K_90      %D_90       VWAP    VWAP_14  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  30.504999  30.504999   \n",
       "1  99.159644  96.313364  99.159644  96.313364  30.892965  30.698982   \n",
       "2  86.065515  92.897414  86.065515  92.897414  31.148973  30.848979   \n",
       "3  94.535498  93.253552  94.535498  93.253552  31.276489  30.955857   \n",
       "4  87.958682  89.519898  87.958682  89.519898  31.407822  31.046250   \n",
       "\n",
       "     VWAP_20    VWAP_30    VWAP_50    VWAP_60    VWAP_90  bb_Middle_Band_14  \\\n",
       "0  30.504999  30.504999  30.504999  30.504999  30.504999          30.504999   \n",
       "1  30.698982  30.698982  30.698982  30.698982  30.698982          30.862499   \n",
       "2  30.848979  30.848979  30.848979  30.848979  30.848979          31.113333   \n",
       "3  30.955857  30.955857  30.955857  30.955857  30.955857          31.277500   \n",
       "4  31.046250  31.046250  31.046250  31.046250  31.046250          31.413500   \n",
       "\n",
       "   Std_Dev_14  Upper_Band_14  Lower_Band_14  bb_Middle_Band_20  Std_Dev_20  \\\n",
       "0         NaN            NaN            NaN          30.504999         NaN   \n",
       "1    0.505581      31.873662      29.851336          30.862499    0.505581   \n",
       "2    0.562635      32.238604      29.988062          31.113333    0.562635   \n",
       "3    0.564661      32.406822      30.148177          31.277500    0.564661   \n",
       "4    0.575858      32.565215      30.261785          31.413500    0.575858   \n",
       "\n",
       "   Upper_Band_20  Lower_Band_20  bb_Middle_Band_30  Std_Dev_30  Upper_Band_30  \\\n",
       "0            NaN            NaN          30.504999         NaN            NaN   \n",
       "1      31.873662      29.851336          30.862499    0.505581      31.873662   \n",
       "2      32.238604      29.988062          31.113333    0.562635      32.238604   \n",
       "3      32.406822      30.148177          31.277500    0.564661      32.406822   \n",
       "4      32.565215      30.261785          31.413500    0.575858      32.565215   \n",
       "\n",
       "   Lower_Band_30  bb_Middle_Band_50  Std_Dev_50  Upper_Band_50  Lower_Band_50  \\\n",
       "0            NaN          30.504999         NaN            NaN            NaN   \n",
       "1      29.851336          30.862499    0.505581      31.873662      29.851336   \n",
       "2      29.988062          31.113333    0.562635      32.238604      29.988062   \n",
       "3      30.148177          31.277500    0.564661      32.406822      30.148177   \n",
       "4      30.261785          31.413500    0.575858      32.565215      30.261785   \n",
       "\n",
       "   bb_Middle_Band_60  Std_Dev_60  Upper_Band_60  Lower_Band_60  \\\n",
       "0          30.504999         NaN            NaN            NaN   \n",
       "1          30.862499    0.505581      31.873662      29.851336   \n",
       "2          31.113333    0.562635      32.238604      29.988062   \n",
       "3          31.277500    0.564661      32.406822      30.148177   \n",
       "4          31.413500    0.575858      32.565215      30.261785   \n",
       "\n",
       "   bb_Middle_Band_90  Std_Dev_90  Upper_Band_90  Lower_Band_90       ATR  \\\n",
       "0          30.504999         NaN            NaN            NaN  0.497499   \n",
       "1          30.862499    0.505581      31.873662      29.851336  0.725000   \n",
       "2          31.113333    0.562635      32.238604      29.988062  0.650002   \n",
       "3          31.277500    0.564661      32.406822      30.148177  0.407499   \n",
       "4          31.413500    0.575858      32.565215      30.261785  0.490002   \n",
       "\n",
       "     ATR_14    ATR_20    ATR_30    ATR_50    ATR_60    ATR_90  \\\n",
       "0  0.497499  0.497499  0.497499  0.497499  0.497499  0.497499   \n",
       "1  0.611250  0.611250  0.611250  0.611250  0.611250  0.611250   \n",
       "2  0.624167  0.624167  0.624167  0.624167  0.624167  0.624167   \n",
       "3  0.570000  0.570000  0.570000  0.570000  0.570000  0.570000   \n",
       "4  0.554000  0.554000  0.554000  0.554000  0.554000  0.554000   \n",
       "\n",
       "   5_day_Fib_23.6%  5_day_Fib_38.2%  5_day_Fib_50.0%  5_day_Fib_61.8%  \\\n",
       "0        30.420091        30.347456        30.288751        30.230046   \n",
       "1        30.949160        30.775420        30.635000        30.494580   \n",
       "2        31.438121        31.170941        30.955001        30.739061   \n",
       "3        31.438121        31.170941        30.955001        30.739061   \n",
       "4        31.705521        31.387241        31.130001        30.872761   \n",
       "\n",
       "   5_day_Fib_78.6%  5_day_Fib_100.0%  5_day_Fib_161.8%  5_day_Fib_261.8%  \\\n",
       "0        30.146466         30.040001         29.732546         29.235047   \n",
       "1        30.294661         30.040001         29.304582         28.114583   \n",
       "2        30.431621         30.040001         28.909061         27.079061   \n",
       "3        30.431621         30.040001         28.909061         27.079061   \n",
       "4        30.506521         30.040001         28.692761         26.512760   \n",
       "\n",
       "   5_day_Fib_423.6%  14_day_Fib_23.6%  14_day_Fib_38.2%  14_day_Fib_50.0%  \\\n",
       "0         28.430093         30.420091         30.347456         30.288751   \n",
       "1         26.189165         30.949160         30.775420         30.635000   \n",
       "2         24.118121         31.438121         31.170941         30.955001   \n",
       "3         24.118121         31.438121         31.170941         30.955001   \n",
       "4         22.985520         31.705521         31.387241         31.130001   \n",
       "\n",
       "   14_day_Fib_61.8%  14_day_Fib_78.6%  14_day_Fib_100.0%  14_day_Fib_161.8%  \\\n",
       "0         30.230046         30.146466          30.040001          29.732546   \n",
       "1         30.494580         30.294661          30.040001          29.304582   \n",
       "2         30.739061         30.431621          30.040001          28.909061   \n",
       "3         30.739061         30.431621          30.040001          28.909061   \n",
       "4         30.872761         30.506521          30.040001          28.692761   \n",
       "\n",
       "   14_day_Fib_261.8%  14_day_Fib_423.6%  30_day_Fib_23.6%  30_day_Fib_38.2%  \\\n",
       "0          29.235047          28.430093         30.420091         30.347456   \n",
       "1          28.114583          26.189165         30.949160         30.775420   \n",
       "2          27.079061          24.118121         31.438121         31.170941   \n",
       "3          27.079061          24.118121         31.438121         31.170941   \n",
       "4          26.512760          22.985520         31.705521         31.387241   \n",
       "\n",
       "   30_day_Fib_50.0%  30_day_Fib_61.8%  30_day_Fib_78.6%  30_day_Fib_100.0%  \\\n",
       "0         30.288751         30.230046         30.146466          30.040001   \n",
       "1         30.635000         30.494580         30.294661          30.040001   \n",
       "2         30.955001         30.739061         30.431621          30.040001   \n",
       "3         30.955001         30.739061         30.431621          30.040001   \n",
       "4         31.130001         30.872761         30.506521          30.040001   \n",
       "\n",
       "   30_day_Fib_161.8%  30_day_Fib_261.8%  30_day_Fib_423.6%           OBV  \\\n",
       "0          29.732546          29.235047          28.430093  0.000000e+00   \n",
       "1          29.304582          28.114583          26.189165  2.942472e+08   \n",
       "2          28.909061          27.079061          24.118121  5.921452e+08   \n",
       "3          28.909061          27.079061          24.118121  8.092340e+08   \n",
       "4          28.692761          26.512760          22.985520  1.061844e+09   \n",
       "\n",
       "   OBV_14day_avg  OBV_20day_avg  OBV_30day_avg  OBV_50day_avg  OBV_60day_avg  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   OBV_90day_avg  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_3_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d569b05f-ebbf-4eb8-a520-3da9dea52485",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m wma_windows \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m200\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m stock_dataframes:\n\u001b[0;32m---> 18\u001b[0m     wma_rolling(df, wma_windows)\n",
      "Cell \u001b[0;32mIn[116], line 13\u001b[0m, in \u001b[0;36mwma_rolling\u001b[0;34m(df, windows)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwma_rolling\u001b[39m(df, windows):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m window \u001b[38;5;129;01min\u001b[39;00m windows:\n\u001b[1;32m     11\u001b[0m         df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWMA_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymbol\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(\n\u001b[1;32m     12\u001b[0m             window\u001b[38;5;241m=\u001b[39mwindow, min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 13\u001b[0m         )\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: weighted_moving_average(x, np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(x))), raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdroplevel(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/window/rolling.py:2043\u001b[0m, in \u001b[0;36mRolling.apply\u001b[0;34m(self, func, raw, engine, engine_kwargs, args, kwargs)\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m   2011\u001b[0m     template_header,\n\u001b[1;32m   2012\u001b[0m     create_section_header(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2041\u001b[0m     kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2042\u001b[0m ):\n\u001b[0;32m-> 2043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m   2044\u001b[0m         func,\n\u001b[1;32m   2045\u001b[0m         raw\u001b[38;5;241m=\u001b[39mraw,\n\u001b[1;32m   2046\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m   2047\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[1;32m   2048\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2049\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   2050\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/window/rolling.py:1503\u001b[0m, in \u001b[0;36mRollingAndExpandingMixin.apply\u001b[0;34m(self, func, raw, engine, engine_kwargs, args, kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine must be either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumba\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcython\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\n\u001b[1;32m   1504\u001b[0m     apply_func,\n\u001b[1;32m   1505\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1506\u001b[0m     numba_args\u001b[38;5;241m=\u001b[39mnumba_args,\n\u001b[1;32m   1507\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/window/rolling.py:721\u001b[0m, in \u001b[0;36mBaseWindowGroupby._apply\u001b[0;34m(self, func, name, numeric_only, numba_args, **kwargs)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    715\u001b[0m     func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    720\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m--> 721\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_apply(\n\u001b[1;32m    722\u001b[0m         func,\n\u001b[1;32m    723\u001b[0m         name,\n\u001b[1;32m    724\u001b[0m         numeric_only,\n\u001b[1;32m    725\u001b[0m         numba_args,\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    727\u001b[0m     )\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;66;03m# Reconstruct the resulting MultiIndex\u001b[39;00m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;66;03m# 1st set of levels = group by labels\u001b[39;00m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# 2nd set of levels = original DataFrame/Series index\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     grouped_object_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/window/rolling.py:617\u001b[0m, in \u001b[0;36mBaseWindow._apply\u001b[0;34m(self, func, name, numeric_only, numba_args, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_blockwise(homogeneous_func, name, numeric_only)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_tablewise(homogeneous_func, name, numeric_only)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/window/rolling.py:470\u001b[0m, in \u001b[0;36mBaseWindow._apply_blockwise\u001b[0;34m(self, homogeneous_func, name, numeric_only)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_numeric_only(name, numeric_only)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_series(homogeneous_func, name)\n\u001b[1;32m    472\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj, numeric_only)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# GH 12541: Special case for count where we support date-like types\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/window/rolling.py:454\u001b[0m, in \u001b[0;36mBaseWindow._apply_series\u001b[0;34m(self, homogeneous_func, name)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo numeric types to aggregate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m result \u001b[38;5;241m=\u001b[39m homogeneous_func(values)\n\u001b[1;32m    455\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_axis_for_step(obj\u001b[38;5;241m.\u001b[39mindex, result)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor(result, index\u001b[38;5;241m=\u001b[39mindex, name\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/window/rolling.py:612\u001b[0m, in \u001b[0;36mBaseWindow._apply.<locals>.homogeneous_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(x, start, end, min_periods, \u001b[38;5;241m*\u001b[39mnumba_args)\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 612\u001b[0m     result \u001b[38;5;241m=\u001b[39m calc(values)\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/window/rolling.py:609\u001b[0m, in \u001b[0;36mBaseWindow._apply.<locals>.homogeneous_func.<locals>.calc\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    600\u001b[0m start, end \u001b[38;5;241m=\u001b[39m window_indexer\u001b[38;5;241m.\u001b[39mget_window_bounds(\n\u001b[1;32m    601\u001b[0m     num_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(x),\n\u001b[1;32m    602\u001b[0m     min_periods\u001b[38;5;241m=\u001b[39mmin_periods,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m     step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep,\n\u001b[1;32m    606\u001b[0m )\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_window_bounds(start, end, \u001b[38;5;28mlen\u001b[39m(x))\n\u001b[0;32m--> 609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(x, start, end, min_periods, \u001b[38;5;241m*\u001b[39mnumba_args)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/window/rolling.py:1530\u001b[0m, in \u001b[0;36mRollingAndExpandingMixin._generate_cython_apply_func.<locals>.apply_func\u001b[0;34m(values, begin, end, min_periods, raw)\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw:\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;66;03m# GH 45912\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m     values \u001b[38;5;241m=\u001b[39m Series(values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m window_func(values, begin, end, min_periods)\n",
      "File \u001b[0;32maggregations.pyx:1423\u001b[0m, in \u001b[0;36mpandas._libs.window.aggregations.roll_apply\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[116], line 13\u001b[0m, in \u001b[0;36mwma_rolling.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwma_rolling\u001b[39m(df, windows):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m window \u001b[38;5;129;01min\u001b[39;00m windows:\n\u001b[1;32m     11\u001b[0m         df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWMA_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymbol\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(\n\u001b[1;32m     12\u001b[0m             window\u001b[38;5;241m=\u001b[39mwindow, min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 13\u001b[0m         )\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: weighted_moving_average(x, np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(x))), raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdroplevel(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[0;32mIn[116], line 6\u001b[0m, in \u001b[0;36mweighted_moving_average\u001b[0;34m(series, weights)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweighted_moving_average\u001b[39m(series, weights):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(series, weights) \u001b[38;5;241m/\u001b[39m weights\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#WMA\n",
    "# delete, taking too long to calculate\n",
    "# delete\n",
    "\n",
    "# Define the function for weighted moving average\n",
    "def weighted_moving_average(series, weights):\n",
    "    return np.dot(series, weights) / weights.sum()\n",
    "\n",
    "# Function to apply rolling WMA for each window in the dataframe\n",
    "def wma_rolling(df, windows):\n",
    "    for window in windows:\n",
    "        df[f'WMA_{window}'] = df.groupby('Symbol')['Close'].rolling(\n",
    "            window=window, min_periods=1\n",
    "        ).apply(lambda x: weighted_moving_average(x, np.linspace(1, 0.1, len(x))), raw=False).droplevel(0)\n",
    "    return df\n",
    "\n",
    "wma_windows = [3, 7, 14, 50, 200]\n",
    "for df in stock_dataframes:\n",
    "    wma_rolling(df, wma_windows)\n",
    "\n",
    "# # Apply the rolling WMA for each window using Parallel\n",
    "# for name, (df, windows) in window_mapping.items():\n",
    "#     Parallel(n_jobs=-1)(\n",
    "#         delayed(wma_rolling)(df, window) for window in windows\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b7d5aa41-1b2d-4f7f-9400-17ff9b4bdfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum Features\n",
    "\n",
    "def calculate_momentum(df, windows):\n",
    "    for window in windows:\n",
    "\n",
    "        df[f'Momentum_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x - x.shift(window)\n",
    "        ).fillna(0)\n",
    "    return df\n",
    "\n",
    "for name, (df, windows) in  window_mapping.items():\n",
    "    calculate_momentum(df, windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "227f7015-b6b5-4b88-a4cc-b79a2bdd48ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Daily_High</th>\n",
       "      <th>Daily_Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volume_14day_avg</th>\n",
       "      <th>Volume_20day_avg</th>\n",
       "      <th>Volume_30day_avg</th>\n",
       "      <th>Volume_50day_avg</th>\n",
       "      <th>Volume_60day_avg</th>\n",
       "      <th>Volume_90day_avg</th>\n",
       "      <th>Daily_High_14day_avg</th>\n",
       "      <th>Daily_High_20day_avg</th>\n",
       "      <th>Daily_High_30day_avg</th>\n",
       "      <th>Daily_High_50day_avg</th>\n",
       "      <th>Daily_High_60day_avg</th>\n",
       "      <th>Daily_High_90day_avg</th>\n",
       "      <th>Daily_Low_14day_avg</th>\n",
       "      <th>Daily_Low_20day_avg</th>\n",
       "      <th>Daily_Low_30day_avg</th>\n",
       "      <th>Daily_Low_50day_avg</th>\n",
       "      <th>Daily_Low_60day_avg</th>\n",
       "      <th>Daily_Low_90day_avg</th>\n",
       "      <th>SMA_14</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_30</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>SMA_60</th>\n",
       "      <th>SMA_90</th>\n",
       "      <th>EMA_14</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>EMA_30</th>\n",
       "      <th>EMA_50</th>\n",
       "      <th>EMA_60</th>\n",
       "      <th>EMA_90</th>\n",
       "      <th>RSI</th>\n",
       "      <th>RSI_20</th>\n",
       "      <th>RSI_30</th>\n",
       "      <th>RSI_50</th>\n",
       "      <th>RSI_60</th>\n",
       "      <th>RSI_90</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "      <th>MACD_rolling_14</th>\n",
       "      <th>Signal_rolling_14</th>\n",
       "      <th>MACD_Histogram_rolling_14</th>\n",
       "      <th>MACD_rolling_20</th>\n",
       "      <th>Signal_rolling_20</th>\n",
       "      <th>MACD_Histogram_rolling_20</th>\n",
       "      <th>MACD_rolling_30</th>\n",
       "      <th>Signal_rolling_30</th>\n",
       "      <th>MACD_Histogram_rolling_30</th>\n",
       "      <th>MACD_rolling_50</th>\n",
       "      <th>Signal_rolling_50</th>\n",
       "      <th>MACD_Histogram_rolling_50</th>\n",
       "      <th>MACD_rolling_60</th>\n",
       "      <th>Signal_rolling_60</th>\n",
       "      <th>MACD_Histogram_rolling_60</th>\n",
       "      <th>MACD_rolling_90</th>\n",
       "      <th>Signal_rolling_90</th>\n",
       "      <th>MACD_Histogram_rolling_90</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>%K_20</th>\n",
       "      <th>%D_20</th>\n",
       "      <th>%K_30</th>\n",
       "      <th>%D_30</th>\n",
       "      <th>%K_50</th>\n",
       "      <th>%D_50</th>\n",
       "      <th>%K_60</th>\n",
       "      <th>%D_60</th>\n",
       "      <th>%K_90</th>\n",
       "      <th>%D_90</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>VWAP_14</th>\n",
       "      <th>VWAP_20</th>\n",
       "      <th>VWAP_30</th>\n",
       "      <th>VWAP_50</th>\n",
       "      <th>VWAP_60</th>\n",
       "      <th>VWAP_90</th>\n",
       "      <th>bb_Middle_Band_14</th>\n",
       "      <th>Std_Dev_14</th>\n",
       "      <th>Upper_Band_14</th>\n",
       "      <th>Lower_Band_14</th>\n",
       "      <th>bb_Middle_Band_20</th>\n",
       "      <th>Std_Dev_20</th>\n",
       "      <th>Upper_Band_20</th>\n",
       "      <th>Lower_Band_20</th>\n",
       "      <th>bb_Middle_Band_30</th>\n",
       "      <th>Std_Dev_30</th>\n",
       "      <th>Upper_Band_30</th>\n",
       "      <th>Lower_Band_30</th>\n",
       "      <th>bb_Middle_Band_50</th>\n",
       "      <th>Std_Dev_50</th>\n",
       "      <th>Upper_Band_50</th>\n",
       "      <th>Lower_Band_50</th>\n",
       "      <th>bb_Middle_Band_60</th>\n",
       "      <th>Std_Dev_60</th>\n",
       "      <th>Upper_Band_60</th>\n",
       "      <th>Lower_Band_60</th>\n",
       "      <th>bb_Middle_Band_90</th>\n",
       "      <th>Std_Dev_90</th>\n",
       "      <th>Upper_Band_90</th>\n",
       "      <th>Lower_Band_90</th>\n",
       "      <th>ATR</th>\n",
       "      <th>ATR_14</th>\n",
       "      <th>ATR_20</th>\n",
       "      <th>ATR_30</th>\n",
       "      <th>ATR_50</th>\n",
       "      <th>ATR_60</th>\n",
       "      <th>ATR_90</th>\n",
       "      <th>5_day_Fib_23.6%</th>\n",
       "      <th>5_day_Fib_38.2%</th>\n",
       "      <th>5_day_Fib_50.0%</th>\n",
       "      <th>5_day_Fib_61.8%</th>\n",
       "      <th>5_day_Fib_78.6%</th>\n",
       "      <th>5_day_Fib_100.0%</th>\n",
       "      <th>5_day_Fib_161.8%</th>\n",
       "      <th>5_day_Fib_261.8%</th>\n",
       "      <th>5_day_Fib_423.6%</th>\n",
       "      <th>14_day_Fib_23.6%</th>\n",
       "      <th>14_day_Fib_38.2%</th>\n",
       "      <th>14_day_Fib_50.0%</th>\n",
       "      <th>14_day_Fib_61.8%</th>\n",
       "      <th>14_day_Fib_78.6%</th>\n",
       "      <th>14_day_Fib_100.0%</th>\n",
       "      <th>14_day_Fib_161.8%</th>\n",
       "      <th>14_day_Fib_261.8%</th>\n",
       "      <th>14_day_Fib_423.6%</th>\n",
       "      <th>30_day_Fib_23.6%</th>\n",
       "      <th>30_day_Fib_38.2%</th>\n",
       "      <th>30_day_Fib_50.0%</th>\n",
       "      <th>30_day_Fib_61.8%</th>\n",
       "      <th>30_day_Fib_78.6%</th>\n",
       "      <th>30_day_Fib_100.0%</th>\n",
       "      <th>30_day_Fib_161.8%</th>\n",
       "      <th>30_day_Fib_261.8%</th>\n",
       "      <th>30_day_Fib_423.6%</th>\n",
       "      <th>OBV</th>\n",
       "      <th>OBV_14day_avg</th>\n",
       "      <th>OBV_20day_avg</th>\n",
       "      <th>OBV_30day_avg</th>\n",
       "      <th>OBV_50day_avg</th>\n",
       "      <th>OBV_60day_avg</th>\n",
       "      <th>OBV_90day_avg</th>\n",
       "      <th>Momentum_14</th>\n",
       "      <th>Momentum_20</th>\n",
       "      <th>Momentum_30</th>\n",
       "      <th>Momentum_50</th>\n",
       "      <th>Momentum_60</th>\n",
       "      <th>Momentum_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>248034000</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>294247200</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.600333</td>\n",
       "      <td>30.573094</td>\n",
       "      <td>30.551128</td>\n",
       "      <td>30.533038</td>\n",
       "      <td>30.528442</td>\n",
       "      <td>30.520713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.045630</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>30.892965</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>2.942472e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>297898000</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>30.735621</td>\n",
       "      <td>30.672323</td>\n",
       "      <td>30.619765</td>\n",
       "      <td>30.575468</td>\n",
       "      <td>30.564067</td>\n",
       "      <td>30.544764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.132584</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>31.148973</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>0.650002</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>5.921452e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>217088800</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>30.873539</td>\n",
       "      <td>30.776864</td>\n",
       "      <td>30.693974</td>\n",
       "      <td>30.622313</td>\n",
       "      <td>30.603605</td>\n",
       "      <td>30.571692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202627</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.133588</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>31.276489</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>0.407499</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>8.092340e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>252609600</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.018067</td>\n",
       "      <td>30.889306</td>\n",
       "      <td>30.775492</td>\n",
       "      <td>30.674673</td>\n",
       "      <td>30.647995</td>\n",
       "      <td>30.602149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.270153</td>\n",
       "      <td>0.109262</td>\n",
       "      <td>0.160890</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>31.407822</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>0.490002</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>1.061844e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol       Date      Close  Daily_High  Daily_Low     Volume  \\\n",
       "0   AAPL 2015-02-10  30.504999   30.537500  30.040001  248034000   \n",
       "1   AAPL 2015-02-11  31.219999   31.230000  30.625000  294247200   \n",
       "2   AAPL 2015-02-12  31.615000   31.870001  31.392500  297898000   \n",
       "3   AAPL 2015-02-13  31.770000   31.820000  31.412500  217088800   \n",
       "4   AAPL 2015-02-17  31.957500   32.220001  31.730000  252609600   \n",
       "\n",
       "   Volume_14day_avg  Volume_20day_avg  Volume_30day_avg  Volume_50day_avg  \\\n",
       "0      2.480340e+08      2.480340e+08      2.480340e+08      2.480340e+08   \n",
       "1      2.711406e+08      2.711406e+08      2.711406e+08      2.711406e+08   \n",
       "2      2.800597e+08      2.800597e+08      2.800597e+08      2.800597e+08   \n",
       "3      2.643170e+08      2.643170e+08      2.643170e+08      2.643170e+08   \n",
       "4      2.619755e+08      2.619755e+08      2.619755e+08      2.619755e+08   \n",
       "\n",
       "   Volume_60day_avg  Volume_90day_avg  Daily_High_14day_avg  \\\n",
       "0      2.480340e+08      2.480340e+08             30.537500   \n",
       "1      2.711406e+08      2.711406e+08             30.883750   \n",
       "2      2.800597e+08      2.800597e+08             31.212500   \n",
       "3      2.643170e+08      2.643170e+08             31.364375   \n",
       "4      2.619755e+08      2.619755e+08             31.535500   \n",
       "\n",
       "   Daily_High_20day_avg  Daily_High_30day_avg  Daily_High_50day_avg  \\\n",
       "0             30.537500             30.537500             30.537500   \n",
       "1             30.883750             30.883750             30.883750   \n",
       "2             31.212500             31.212500             31.212500   \n",
       "3             31.364375             31.364375             31.364375   \n",
       "4             31.535500             31.535500             31.535500   \n",
       "\n",
       "   Daily_High_60day_avg  Daily_High_90day_avg  Daily_Low_14day_avg  \\\n",
       "0             30.537500             30.537500            30.040001   \n",
       "1             30.883750             30.883750            30.332500   \n",
       "2             31.212500             31.212500            30.685834   \n",
       "3             31.364375             31.364375            30.867500   \n",
       "4             31.535500             31.535500            31.040000   \n",
       "\n",
       "   Daily_Low_20day_avg  Daily_Low_30day_avg  Daily_Low_50day_avg  \\\n",
       "0            30.040001            30.040001            30.040001   \n",
       "1            30.332500            30.332500            30.332500   \n",
       "2            30.685834            30.685834            30.685834   \n",
       "3            30.867500            30.867500            30.867500   \n",
       "4            31.040000            31.040000            31.040000   \n",
       "\n",
       "   Daily_Low_60day_avg  Daily_Low_90day_avg     SMA_14     SMA_20     SMA_30  \\\n",
       "0            30.040001            30.040001  30.504999  30.504999  30.504999   \n",
       "1            30.332500            30.332500  30.862499  30.862499  30.862499   \n",
       "2            30.685834            30.685834  31.113333  31.113333  31.113333   \n",
       "3            30.867500            30.867500  31.277500  31.277500  31.277500   \n",
       "4            31.040000            31.040000  31.413500  31.413500  31.413500   \n",
       "\n",
       "      SMA_50     SMA_60     SMA_90     EMA_14     EMA_20     EMA_30  \\\n",
       "0  30.504999  30.504999  30.504999  30.504999  30.504999  30.504999   \n",
       "1  30.862499  30.862499  30.862499  30.600333  30.573094  30.551128   \n",
       "2  31.113333  31.113333  31.113333  30.735621  30.672323  30.619765   \n",
       "3  31.277500  31.277500  31.277500  30.873539  30.776864  30.693974   \n",
       "4  31.413500  31.413500  31.413500  31.018067  30.889306  30.775492   \n",
       "\n",
       "      EMA_50     EMA_60     EMA_90  RSI  RSI_20  RSI_30  RSI_50  RSI_60  \\\n",
       "0  30.504999  30.504999  30.504999  NaN     NaN     NaN     NaN     NaN   \n",
       "1  30.533038  30.528442  30.520713  NaN     NaN     NaN     NaN     NaN   \n",
       "2  30.575468  30.564067  30.544764  NaN     NaN     NaN     NaN     NaN   \n",
       "3  30.622313  30.603605  30.571692  NaN     NaN     NaN     NaN     NaN   \n",
       "4  30.674673  30.647995  30.602149  NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   RSI_90      MACD  Signal_Line  MACD_Histogram  MACD_rolling_14  \\\n",
       "0     NaN  0.000000     0.000000        0.000000         0.000000   \n",
       "1     NaN  0.057037     0.011407        0.045630         0.028519   \n",
       "2     NaN  0.132584     0.035643        0.096941         0.063207   \n",
       "3     NaN  0.202627     0.069040        0.133588         0.098062   \n",
       "4     NaN  0.270153     0.109262        0.160890         0.132480   \n",
       "\n",
       "   Signal_rolling_14  MACD_Histogram_rolling_14  MACD_rolling_20  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_20  MACD_Histogram_rolling_20  MACD_rolling_30  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_30  MACD_Histogram_rolling_30  MACD_rolling_50  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_50  MACD_Histogram_rolling_50  MACD_rolling_60  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_60  MACD_Histogram_rolling_60  MACD_rolling_90  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_90  MACD_Histogram_rolling_90         %K         %D  \\\n",
       "0           0.000000                   0.000000  93.467084  93.467084   \n",
       "1           0.005704                   0.022815  99.159644  96.313364   \n",
       "2           0.015683                   0.047524  86.065515  92.897414   \n",
       "3           0.029022                   0.069040  94.535498  93.253552   \n",
       "4           0.045070                   0.087410  87.958682  89.519898   \n",
       "\n",
       "       %K_20      %D_20      %K_30      %D_30      %K_50      %D_50  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  93.467084  93.467084   \n",
       "1  99.159644  96.313364  99.159644  96.313364  99.159644  96.313364   \n",
       "2  86.065515  92.897414  86.065515  92.897414  86.065515  92.897414   \n",
       "3  94.535498  93.253552  94.535498  93.253552  94.535498  93.253552   \n",
       "4  87.958682  89.519898  87.958682  89.519898  87.958682  89.519898   \n",
       "\n",
       "       %K_60      %D_60      %K_90      %D_90       VWAP    VWAP_14  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  30.504999  30.504999   \n",
       "1  99.159644  96.313364  99.159644  96.313364  30.892965  30.698982   \n",
       "2  86.065515  92.897414  86.065515  92.897414  31.148973  30.848979   \n",
       "3  94.535498  93.253552  94.535498  93.253552  31.276489  30.955857   \n",
       "4  87.958682  89.519898  87.958682  89.519898  31.407822  31.046250   \n",
       "\n",
       "     VWAP_20    VWAP_30    VWAP_50    VWAP_60    VWAP_90  bb_Middle_Band_14  \\\n",
       "0  30.504999  30.504999  30.504999  30.504999  30.504999          30.504999   \n",
       "1  30.698982  30.698982  30.698982  30.698982  30.698982          30.862499   \n",
       "2  30.848979  30.848979  30.848979  30.848979  30.848979          31.113333   \n",
       "3  30.955857  30.955857  30.955857  30.955857  30.955857          31.277500   \n",
       "4  31.046250  31.046250  31.046250  31.046250  31.046250          31.413500   \n",
       "\n",
       "   Std_Dev_14  Upper_Band_14  Lower_Band_14  bb_Middle_Band_20  Std_Dev_20  \\\n",
       "0         NaN            NaN            NaN          30.504999         NaN   \n",
       "1    0.505581      31.873662      29.851336          30.862499    0.505581   \n",
       "2    0.562635      32.238604      29.988062          31.113333    0.562635   \n",
       "3    0.564661      32.406822      30.148177          31.277500    0.564661   \n",
       "4    0.575858      32.565215      30.261785          31.413500    0.575858   \n",
       "\n",
       "   Upper_Band_20  Lower_Band_20  bb_Middle_Band_30  Std_Dev_30  Upper_Band_30  \\\n",
       "0            NaN            NaN          30.504999         NaN            NaN   \n",
       "1      31.873662      29.851336          30.862499    0.505581      31.873662   \n",
       "2      32.238604      29.988062          31.113333    0.562635      32.238604   \n",
       "3      32.406822      30.148177          31.277500    0.564661      32.406822   \n",
       "4      32.565215      30.261785          31.413500    0.575858      32.565215   \n",
       "\n",
       "   Lower_Band_30  bb_Middle_Band_50  Std_Dev_50  Upper_Band_50  Lower_Band_50  \\\n",
       "0            NaN          30.504999         NaN            NaN            NaN   \n",
       "1      29.851336          30.862499    0.505581      31.873662      29.851336   \n",
       "2      29.988062          31.113333    0.562635      32.238604      29.988062   \n",
       "3      30.148177          31.277500    0.564661      32.406822      30.148177   \n",
       "4      30.261785          31.413500    0.575858      32.565215      30.261785   \n",
       "\n",
       "   bb_Middle_Band_60  Std_Dev_60  Upper_Band_60  Lower_Band_60  \\\n",
       "0          30.504999         NaN            NaN            NaN   \n",
       "1          30.862499    0.505581      31.873662      29.851336   \n",
       "2          31.113333    0.562635      32.238604      29.988062   \n",
       "3          31.277500    0.564661      32.406822      30.148177   \n",
       "4          31.413500    0.575858      32.565215      30.261785   \n",
       "\n",
       "   bb_Middle_Band_90  Std_Dev_90  Upper_Band_90  Lower_Band_90       ATR  \\\n",
       "0          30.504999         NaN            NaN            NaN  0.497499   \n",
       "1          30.862499    0.505581      31.873662      29.851336  0.725000   \n",
       "2          31.113333    0.562635      32.238604      29.988062  0.650002   \n",
       "3          31.277500    0.564661      32.406822      30.148177  0.407499   \n",
       "4          31.413500    0.575858      32.565215      30.261785  0.490002   \n",
       "\n",
       "     ATR_14    ATR_20    ATR_30    ATR_50    ATR_60    ATR_90  \\\n",
       "0  0.497499  0.497499  0.497499  0.497499  0.497499  0.497499   \n",
       "1  0.611250  0.611250  0.611250  0.611250  0.611250  0.611250   \n",
       "2  0.624167  0.624167  0.624167  0.624167  0.624167  0.624167   \n",
       "3  0.570000  0.570000  0.570000  0.570000  0.570000  0.570000   \n",
       "4  0.554000  0.554000  0.554000  0.554000  0.554000  0.554000   \n",
       "\n",
       "   5_day_Fib_23.6%  5_day_Fib_38.2%  5_day_Fib_50.0%  5_day_Fib_61.8%  \\\n",
       "0        30.420091        30.347456        30.288751        30.230046   \n",
       "1        30.949160        30.775420        30.635000        30.494580   \n",
       "2        31.438121        31.170941        30.955001        30.739061   \n",
       "3        31.438121        31.170941        30.955001        30.739061   \n",
       "4        31.705521        31.387241        31.130001        30.872761   \n",
       "\n",
       "   5_day_Fib_78.6%  5_day_Fib_100.0%  5_day_Fib_161.8%  5_day_Fib_261.8%  \\\n",
       "0        30.146466         30.040001         29.732546         29.235047   \n",
       "1        30.294661         30.040001         29.304582         28.114583   \n",
       "2        30.431621         30.040001         28.909061         27.079061   \n",
       "3        30.431621         30.040001         28.909061         27.079061   \n",
       "4        30.506521         30.040001         28.692761         26.512760   \n",
       "\n",
       "   5_day_Fib_423.6%  14_day_Fib_23.6%  14_day_Fib_38.2%  14_day_Fib_50.0%  \\\n",
       "0         28.430093         30.420091         30.347456         30.288751   \n",
       "1         26.189165         30.949160         30.775420         30.635000   \n",
       "2         24.118121         31.438121         31.170941         30.955001   \n",
       "3         24.118121         31.438121         31.170941         30.955001   \n",
       "4         22.985520         31.705521         31.387241         31.130001   \n",
       "\n",
       "   14_day_Fib_61.8%  14_day_Fib_78.6%  14_day_Fib_100.0%  14_day_Fib_161.8%  \\\n",
       "0         30.230046         30.146466          30.040001          29.732546   \n",
       "1         30.494580         30.294661          30.040001          29.304582   \n",
       "2         30.739061         30.431621          30.040001          28.909061   \n",
       "3         30.739061         30.431621          30.040001          28.909061   \n",
       "4         30.872761         30.506521          30.040001          28.692761   \n",
       "\n",
       "   14_day_Fib_261.8%  14_day_Fib_423.6%  30_day_Fib_23.6%  30_day_Fib_38.2%  \\\n",
       "0          29.235047          28.430093         30.420091         30.347456   \n",
       "1          28.114583          26.189165         30.949160         30.775420   \n",
       "2          27.079061          24.118121         31.438121         31.170941   \n",
       "3          27.079061          24.118121         31.438121         31.170941   \n",
       "4          26.512760          22.985520         31.705521         31.387241   \n",
       "\n",
       "   30_day_Fib_50.0%  30_day_Fib_61.8%  30_day_Fib_78.6%  30_day_Fib_100.0%  \\\n",
       "0         30.288751         30.230046         30.146466          30.040001   \n",
       "1         30.635000         30.494580         30.294661          30.040001   \n",
       "2         30.955001         30.739061         30.431621          30.040001   \n",
       "3         30.955001         30.739061         30.431621          30.040001   \n",
       "4         31.130001         30.872761         30.506521          30.040001   \n",
       "\n",
       "   30_day_Fib_161.8%  30_day_Fib_261.8%  30_day_Fib_423.6%           OBV  \\\n",
       "0          29.732546          29.235047          28.430093  0.000000e+00   \n",
       "1          29.304582          28.114583          26.189165  2.942472e+08   \n",
       "2          28.909061          27.079061          24.118121  5.921452e+08   \n",
       "3          28.909061          27.079061          24.118121  8.092340e+08   \n",
       "4          28.692761          26.512760          22.985520  1.061844e+09   \n",
       "\n",
       "   OBV_14day_avg  OBV_20day_avg  OBV_30day_avg  OBV_50day_avg  OBV_60day_avg  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   OBV_90day_avg  Momentum_14  Momentum_20  Momentum_30  Momentum_50  \\\n",
       "0            0.0          0.0          0.0          0.0          0.0   \n",
       "1            0.0          0.0          0.0          0.0          0.0   \n",
       "2            0.0          0.0          0.0          0.0          0.0   \n",
       "3            0.0          0.0          0.0          0.0          0.0   \n",
       "4            0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   Momentum_60  Momentum_90  \n",
       "0          0.0          0.0  \n",
       "1          0.0          0.0  \n",
       "2          0.0          0.0  \n",
       "3          0.0          0.0  \n",
       "4          0.0          0.0  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_3_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6e088481-5272-455e-bcf4-8892fb8a4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile-Based Features\n",
    "\n",
    "def calculate_quantiles(df, windows):\n",
    "\n",
    "    for window in windows:\n",
    "    \n",
    "        df[f'Rolling_Median_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.rolling(window=window).median()\n",
    "        ).fillna(0)\n",
    "    \n",
    "        # Rolling 25th Quantile\n",
    "        df[f'Rolling_Quantile_25_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.rolling(window=window).quantile(0.25)\n",
    "        ).fillna(0)\n",
    "    \n",
    "        # Rolling 75th Quantile\n",
    "        df[f'Rolling_Quantile_75_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.rolling(window=window).quantile(0.75)\n",
    "        ).fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    calculate_quantiles(df, windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47a140-aed8-49e2-8e3b-dfdd6a53243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_all_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c32d80bf-cec7-46d6-b799-e600664bb121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of columns to create lags for (focusing on short-term indicators)\n",
    "one_week_lags = ['Close', 'SMA_3', 'SMA_7', \n",
    "                  'EMA_3', 'EMA_7', 'Volume',\n",
    "                  'RSI', 'RSI_3', 'RSI_7', 'Signal_Line', 'MACD', \n",
    "                  'VWAP', '%K', '%D', 'OBV', 'Momentum_3', \n",
    "                  'Momentum_7','Std_Dev_3', 'Std_Dev_7', \n",
    "                  'Rolling_Median_3', 'Rolling_Median_7',\n",
    "                  'Rolling_Quantile_25_3', 'Rolling_Quantile_25_7', \n",
    "                  'Rolling_Quantile_75_3', 'Rolling_Quantile_75_7',\n",
    "                 'Daily_High_3day_avg', 'Daily_High_7day_avg',\n",
    "                 'Daily_Low_3day_avg', 'Daily_Low_7day_avg',\n",
    "                 'Volume_3day_avg', 'Volume_7day_avg']\n",
    "one_month_lags = ['Close', 'SMA_14', 'SMA_30', \n",
    "                  'EMA_14', 'EMA_30', 'Volume',\n",
    "                  'RSI', 'RSI_20', 'RSI_30', 'Signal_Line', 'MACD', \n",
    "                  'VWAP', '%K', '%D', 'OBV', 'Momentum_14', \n",
    "                  'Momentum_30','Std_Dev_14', 'Std_Dev_30', \n",
    "                  'Rolling_Median_14', 'Rolling_Median_30',\n",
    "                  'Rolling_Quantile_25_14', 'Rolling_Quantile_25_30', \n",
    "                  'Rolling_Quantile_75_14', 'Rolling_Quantile_75_30',\n",
    "                 'Daily_High_14day_avg', 'Daily_High_30day_avg',\n",
    "                 'Daily_Low_14day_avg', 'Daily_Low_30day_avg',\n",
    "                 'Volume_14day_avg', 'Volume_30day_avg']\n",
    "\n",
    "three_month_lags = ['Close', 'SMA_90', 'SMA_30', \n",
    "                  'EMA_90', 'EMA_30', 'Volume',\n",
    "                  'RSI', 'RSI_90', 'RSI_30', 'Signal_Line', 'MACD', \n",
    "                  'VWAP', '%K', '%D', 'OBV', 'Momentum_90', \n",
    "                  'Momentum_30','Std_Dev_90', 'Std_Dev_30', \n",
    "                  'Rolling_Median_90', 'Rolling_Median_30',\n",
    "                  'Rolling_Quantile_25_90', 'Rolling_Quantile_25_30', \n",
    "                  'Rolling_Quantile_75_90', 'Rolling_Quantile_75_30',\n",
    "                 'Daily_High_90day_avg', 'Daily_High_30day_avg',\n",
    "                 'Daily_Low_90day_avg', 'Daily_Low_30day_avg',\n",
    "                 'Volume_90day_avg', 'Volume_30day_avg']\n",
    "\n",
    "lagging_cols = [one_week_lags, one_month_lags, three_month_lags]\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 14, 20, 30, 50, 60, 90, 100 180, 200] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "# lags = [1, 3, 5, 7]\n",
    "# for col in columns_to_lag:\n",
    "#     for lag in lags:\n",
    "#         df_all_cleaned[f'{col}_lag_{lag}'] = df_all_cleaned[col].shift(lag)\n",
    "\n",
    "# Define the lags corresponding to each timeframe\n",
    "lags = [[3, 7], [14, 30], [30, 90]]\n",
    "\n",
    "def calculate_lag(df, features, lags):\n",
    "    # Apply lags to the dataframe\n",
    "    for col in features:\n",
    "        for lag in lags:\n",
    "            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Assuming stock_dataframes is a list of your dataframes\n",
    "for df, (lags_for_df, features_for_df) in zip(stock_dataframes, zip(lags, lagging_cols)):\n",
    "    calculate_lag(df, features_for_df, lags_for_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4fef74e3-1761-4404-a55d-70af4dfccdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Daily_High</th>\n",
       "      <th>Daily_Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volume_14day_avg</th>\n",
       "      <th>Volume_20day_avg</th>\n",
       "      <th>Volume_30day_avg</th>\n",
       "      <th>Volume_50day_avg</th>\n",
       "      <th>Volume_60day_avg</th>\n",
       "      <th>Volume_90day_avg</th>\n",
       "      <th>Daily_High_14day_avg</th>\n",
       "      <th>Daily_High_20day_avg</th>\n",
       "      <th>Daily_High_30day_avg</th>\n",
       "      <th>Daily_High_50day_avg</th>\n",
       "      <th>Daily_High_60day_avg</th>\n",
       "      <th>Daily_High_90day_avg</th>\n",
       "      <th>Daily_Low_14day_avg</th>\n",
       "      <th>Daily_Low_20day_avg</th>\n",
       "      <th>Daily_Low_30day_avg</th>\n",
       "      <th>Daily_Low_50day_avg</th>\n",
       "      <th>Daily_Low_60day_avg</th>\n",
       "      <th>Daily_Low_90day_avg</th>\n",
       "      <th>SMA_14</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_30</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>SMA_60</th>\n",
       "      <th>SMA_90</th>\n",
       "      <th>EMA_14</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>EMA_30</th>\n",
       "      <th>EMA_50</th>\n",
       "      <th>EMA_60</th>\n",
       "      <th>EMA_90</th>\n",
       "      <th>RSI</th>\n",
       "      <th>RSI_20</th>\n",
       "      <th>RSI_30</th>\n",
       "      <th>RSI_50</th>\n",
       "      <th>RSI_60</th>\n",
       "      <th>RSI_90</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "      <th>MACD_rolling_14</th>\n",
       "      <th>Signal_rolling_14</th>\n",
       "      <th>MACD_Histogram_rolling_14</th>\n",
       "      <th>MACD_rolling_20</th>\n",
       "      <th>Signal_rolling_20</th>\n",
       "      <th>MACD_Histogram_rolling_20</th>\n",
       "      <th>MACD_rolling_30</th>\n",
       "      <th>Signal_rolling_30</th>\n",
       "      <th>MACD_Histogram_rolling_30</th>\n",
       "      <th>MACD_rolling_50</th>\n",
       "      <th>Signal_rolling_50</th>\n",
       "      <th>MACD_Histogram_rolling_50</th>\n",
       "      <th>MACD_rolling_60</th>\n",
       "      <th>Signal_rolling_60</th>\n",
       "      <th>MACD_Histogram_rolling_60</th>\n",
       "      <th>MACD_rolling_90</th>\n",
       "      <th>Signal_rolling_90</th>\n",
       "      <th>MACD_Histogram_rolling_90</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>%K_20</th>\n",
       "      <th>%D_20</th>\n",
       "      <th>%K_30</th>\n",
       "      <th>%D_30</th>\n",
       "      <th>%K_50</th>\n",
       "      <th>%D_50</th>\n",
       "      <th>%K_60</th>\n",
       "      <th>%D_60</th>\n",
       "      <th>%K_90</th>\n",
       "      <th>%D_90</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>VWAP_14</th>\n",
       "      <th>VWAP_20</th>\n",
       "      <th>VWAP_30</th>\n",
       "      <th>VWAP_50</th>\n",
       "      <th>VWAP_60</th>\n",
       "      <th>VWAP_90</th>\n",
       "      <th>bb_Middle_Band_14</th>\n",
       "      <th>Std_Dev_14</th>\n",
       "      <th>Upper_Band_14</th>\n",
       "      <th>Lower_Band_14</th>\n",
       "      <th>bb_Middle_Band_20</th>\n",
       "      <th>Std_Dev_20</th>\n",
       "      <th>Upper_Band_20</th>\n",
       "      <th>Lower_Band_20</th>\n",
       "      <th>bb_Middle_Band_30</th>\n",
       "      <th>Std_Dev_30</th>\n",
       "      <th>Upper_Band_30</th>\n",
       "      <th>Lower_Band_30</th>\n",
       "      <th>bb_Middle_Band_50</th>\n",
       "      <th>Std_Dev_50</th>\n",
       "      <th>Upper_Band_50</th>\n",
       "      <th>Lower_Band_50</th>\n",
       "      <th>bb_Middle_Band_60</th>\n",
       "      <th>Std_Dev_60</th>\n",
       "      <th>Upper_Band_60</th>\n",
       "      <th>Lower_Band_60</th>\n",
       "      <th>bb_Middle_Band_90</th>\n",
       "      <th>Std_Dev_90</th>\n",
       "      <th>Upper_Band_90</th>\n",
       "      <th>Lower_Band_90</th>\n",
       "      <th>ATR</th>\n",
       "      <th>ATR_14</th>\n",
       "      <th>ATR_20</th>\n",
       "      <th>ATR_30</th>\n",
       "      <th>ATR_50</th>\n",
       "      <th>ATR_60</th>\n",
       "      <th>ATR_90</th>\n",
       "      <th>5_day_Fib_23.6%</th>\n",
       "      <th>5_day_Fib_38.2%</th>\n",
       "      <th>5_day_Fib_50.0%</th>\n",
       "      <th>5_day_Fib_61.8%</th>\n",
       "      <th>5_day_Fib_78.6%</th>\n",
       "      <th>5_day_Fib_100.0%</th>\n",
       "      <th>5_day_Fib_161.8%</th>\n",
       "      <th>5_day_Fib_261.8%</th>\n",
       "      <th>5_day_Fib_423.6%</th>\n",
       "      <th>14_day_Fib_23.6%</th>\n",
       "      <th>14_day_Fib_38.2%</th>\n",
       "      <th>14_day_Fib_50.0%</th>\n",
       "      <th>14_day_Fib_61.8%</th>\n",
       "      <th>14_day_Fib_78.6%</th>\n",
       "      <th>14_day_Fib_100.0%</th>\n",
       "      <th>14_day_Fib_161.8%</th>\n",
       "      <th>14_day_Fib_261.8%</th>\n",
       "      <th>14_day_Fib_423.6%</th>\n",
       "      <th>30_day_Fib_23.6%</th>\n",
       "      <th>30_day_Fib_38.2%</th>\n",
       "      <th>30_day_Fib_50.0%</th>\n",
       "      <th>30_day_Fib_61.8%</th>\n",
       "      <th>30_day_Fib_78.6%</th>\n",
       "      <th>30_day_Fib_100.0%</th>\n",
       "      <th>30_day_Fib_161.8%</th>\n",
       "      <th>30_day_Fib_261.8%</th>\n",
       "      <th>30_day_Fib_423.6%</th>\n",
       "      <th>OBV</th>\n",
       "      <th>OBV_14day_avg</th>\n",
       "      <th>OBV_20day_avg</th>\n",
       "      <th>OBV_30day_avg</th>\n",
       "      <th>OBV_50day_avg</th>\n",
       "      <th>OBV_60day_avg</th>\n",
       "      <th>OBV_90day_avg</th>\n",
       "      <th>Momentum_14</th>\n",
       "      <th>Momentum_20</th>\n",
       "      <th>Momentum_30</th>\n",
       "      <th>Momentum_50</th>\n",
       "      <th>Momentum_60</th>\n",
       "      <th>Momentum_90</th>\n",
       "      <th>Rolling_Median_14</th>\n",
       "      <th>Rolling_Quantile_25_14</th>\n",
       "      <th>Rolling_Quantile_75_14</th>\n",
       "      <th>Rolling_Median_20</th>\n",
       "      <th>Rolling_Quantile_25_20</th>\n",
       "      <th>Rolling_Quantile_75_20</th>\n",
       "      <th>Rolling_Median_30</th>\n",
       "      <th>Rolling_Quantile_25_30</th>\n",
       "      <th>Rolling_Quantile_75_30</th>\n",
       "      <th>Rolling_Median_50</th>\n",
       "      <th>Rolling_Quantile_25_50</th>\n",
       "      <th>Rolling_Quantile_75_50</th>\n",
       "      <th>Rolling_Median_60</th>\n",
       "      <th>Rolling_Quantile_25_60</th>\n",
       "      <th>Rolling_Quantile_75_60</th>\n",
       "      <th>Rolling_Median_90</th>\n",
       "      <th>Rolling_Quantile_25_90</th>\n",
       "      <th>Rolling_Quantile_75_90</th>\n",
       "      <th>Close_lag_30</th>\n",
       "      <th>Close_lag_90</th>\n",
       "      <th>SMA_90_lag_30</th>\n",
       "      <th>SMA_90_lag_90</th>\n",
       "      <th>SMA_30_lag_30</th>\n",
       "      <th>SMA_30_lag_90</th>\n",
       "      <th>EMA_90_lag_30</th>\n",
       "      <th>EMA_90_lag_90</th>\n",
       "      <th>EMA_30_lag_30</th>\n",
       "      <th>EMA_30_lag_90</th>\n",
       "      <th>Volume_lag_30</th>\n",
       "      <th>Volume_lag_90</th>\n",
       "      <th>RSI_lag_30</th>\n",
       "      <th>RSI_lag_90</th>\n",
       "      <th>RSI_90_lag_30</th>\n",
       "      <th>RSI_90_lag_90</th>\n",
       "      <th>RSI_30_lag_30</th>\n",
       "      <th>RSI_30_lag_90</th>\n",
       "      <th>Signal_Line_lag_30</th>\n",
       "      <th>Signal_Line_lag_90</th>\n",
       "      <th>MACD_lag_30</th>\n",
       "      <th>MACD_lag_90</th>\n",
       "      <th>VWAP_lag_30</th>\n",
       "      <th>VWAP_lag_90</th>\n",
       "      <th>%K_lag_30</th>\n",
       "      <th>%K_lag_90</th>\n",
       "      <th>%D_lag_30</th>\n",
       "      <th>%D_lag_90</th>\n",
       "      <th>OBV_lag_30</th>\n",
       "      <th>OBV_lag_90</th>\n",
       "      <th>Momentum_90_lag_30</th>\n",
       "      <th>Momentum_90_lag_90</th>\n",
       "      <th>Momentum_30_lag_30</th>\n",
       "      <th>Momentum_30_lag_90</th>\n",
       "      <th>Std_Dev_90_lag_30</th>\n",
       "      <th>Std_Dev_90_lag_90</th>\n",
       "      <th>Std_Dev_30_lag_30</th>\n",
       "      <th>Std_Dev_30_lag_90</th>\n",
       "      <th>Rolling_Median_90_lag_30</th>\n",
       "      <th>Rolling_Median_90_lag_90</th>\n",
       "      <th>Rolling_Median_30_lag_30</th>\n",
       "      <th>Rolling_Median_30_lag_90</th>\n",
       "      <th>Rolling_Quantile_25_90_lag_30</th>\n",
       "      <th>Rolling_Quantile_25_90_lag_90</th>\n",
       "      <th>Rolling_Quantile_25_30_lag_30</th>\n",
       "      <th>Rolling_Quantile_25_30_lag_90</th>\n",
       "      <th>Rolling_Quantile_75_90_lag_30</th>\n",
       "      <th>Rolling_Quantile_75_90_lag_90</th>\n",
       "      <th>Rolling_Quantile_75_30_lag_30</th>\n",
       "      <th>Rolling_Quantile_75_30_lag_90</th>\n",
       "      <th>Daily_High_90day_avg_lag_30</th>\n",
       "      <th>Daily_High_90day_avg_lag_90</th>\n",
       "      <th>Daily_High_30day_avg_lag_30</th>\n",
       "      <th>Daily_High_30day_avg_lag_90</th>\n",
       "      <th>Daily_Low_90day_avg_lag_30</th>\n",
       "      <th>Daily_Low_90day_avg_lag_90</th>\n",
       "      <th>Daily_Low_30day_avg_lag_30</th>\n",
       "      <th>Daily_Low_30day_avg_lag_90</th>\n",
       "      <th>Volume_90day_avg_lag_30</th>\n",
       "      <th>Volume_90day_avg_lag_90</th>\n",
       "      <th>Volume_30day_avg_lag_30</th>\n",
       "      <th>Volume_30day_avg_lag_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>248034000</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>294247200</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.600333</td>\n",
       "      <td>30.573094</td>\n",
       "      <td>30.551128</td>\n",
       "      <td>30.533038</td>\n",
       "      <td>30.528442</td>\n",
       "      <td>30.520713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.045630</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>30.892965</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>2.942472e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>297898000</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>30.735621</td>\n",
       "      <td>30.672323</td>\n",
       "      <td>30.619765</td>\n",
       "      <td>30.575468</td>\n",
       "      <td>30.564067</td>\n",
       "      <td>30.544764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.132584</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>31.148973</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>0.650002</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>5.921452e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>217088800</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>30.873539</td>\n",
       "      <td>30.776864</td>\n",
       "      <td>30.693974</td>\n",
       "      <td>30.622313</td>\n",
       "      <td>30.603605</td>\n",
       "      <td>30.571692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202627</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.133588</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>31.276489</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>0.407499</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>8.092340e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>252609600</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.018067</td>\n",
       "      <td>30.889306</td>\n",
       "      <td>30.775492</td>\n",
       "      <td>30.674673</td>\n",
       "      <td>30.647995</td>\n",
       "      <td>30.602149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.270153</td>\n",
       "      <td>0.109262</td>\n",
       "      <td>0.160890</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>31.407822</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>0.490002</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>1.061844e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol       Date      Close  Daily_High  Daily_Low     Volume  \\\n",
       "0   AAPL 2015-02-10  30.504999   30.537500  30.040001  248034000   \n",
       "1   AAPL 2015-02-11  31.219999   31.230000  30.625000  294247200   \n",
       "2   AAPL 2015-02-12  31.615000   31.870001  31.392500  297898000   \n",
       "3   AAPL 2015-02-13  31.770000   31.820000  31.412500  217088800   \n",
       "4   AAPL 2015-02-17  31.957500   32.220001  31.730000  252609600   \n",
       "\n",
       "   Volume_14day_avg  Volume_20day_avg  Volume_30day_avg  Volume_50day_avg  \\\n",
       "0      2.480340e+08      2.480340e+08      2.480340e+08      2.480340e+08   \n",
       "1      2.711406e+08      2.711406e+08      2.711406e+08      2.711406e+08   \n",
       "2      2.800597e+08      2.800597e+08      2.800597e+08      2.800597e+08   \n",
       "3      2.643170e+08      2.643170e+08      2.643170e+08      2.643170e+08   \n",
       "4      2.619755e+08      2.619755e+08      2.619755e+08      2.619755e+08   \n",
       "\n",
       "   Volume_60day_avg  Volume_90day_avg  Daily_High_14day_avg  \\\n",
       "0      2.480340e+08      2.480340e+08             30.537500   \n",
       "1      2.711406e+08      2.711406e+08             30.883750   \n",
       "2      2.800597e+08      2.800597e+08             31.212500   \n",
       "3      2.643170e+08      2.643170e+08             31.364375   \n",
       "4      2.619755e+08      2.619755e+08             31.535500   \n",
       "\n",
       "   Daily_High_20day_avg  Daily_High_30day_avg  Daily_High_50day_avg  \\\n",
       "0             30.537500             30.537500             30.537500   \n",
       "1             30.883750             30.883750             30.883750   \n",
       "2             31.212500             31.212500             31.212500   \n",
       "3             31.364375             31.364375             31.364375   \n",
       "4             31.535500             31.535500             31.535500   \n",
       "\n",
       "   Daily_High_60day_avg  Daily_High_90day_avg  Daily_Low_14day_avg  \\\n",
       "0             30.537500             30.537500            30.040001   \n",
       "1             30.883750             30.883750            30.332500   \n",
       "2             31.212500             31.212500            30.685834   \n",
       "3             31.364375             31.364375            30.867500   \n",
       "4             31.535500             31.535500            31.040000   \n",
       "\n",
       "   Daily_Low_20day_avg  Daily_Low_30day_avg  Daily_Low_50day_avg  \\\n",
       "0            30.040001            30.040001            30.040001   \n",
       "1            30.332500            30.332500            30.332500   \n",
       "2            30.685834            30.685834            30.685834   \n",
       "3            30.867500            30.867500            30.867500   \n",
       "4            31.040000            31.040000            31.040000   \n",
       "\n",
       "   Daily_Low_60day_avg  Daily_Low_90day_avg     SMA_14     SMA_20     SMA_30  \\\n",
       "0            30.040001            30.040001  30.504999  30.504999  30.504999   \n",
       "1            30.332500            30.332500  30.862499  30.862499  30.862499   \n",
       "2            30.685834            30.685834  31.113333  31.113333  31.113333   \n",
       "3            30.867500            30.867500  31.277500  31.277500  31.277500   \n",
       "4            31.040000            31.040000  31.413500  31.413500  31.413500   \n",
       "\n",
       "      SMA_50     SMA_60     SMA_90     EMA_14     EMA_20     EMA_30  \\\n",
       "0  30.504999  30.504999  30.504999  30.504999  30.504999  30.504999   \n",
       "1  30.862499  30.862499  30.862499  30.600333  30.573094  30.551128   \n",
       "2  31.113333  31.113333  31.113333  30.735621  30.672323  30.619765   \n",
       "3  31.277500  31.277500  31.277500  30.873539  30.776864  30.693974   \n",
       "4  31.413500  31.413500  31.413500  31.018067  30.889306  30.775492   \n",
       "\n",
       "      EMA_50     EMA_60     EMA_90  RSI  RSI_20  RSI_30  RSI_50  RSI_60  \\\n",
       "0  30.504999  30.504999  30.504999  NaN     NaN     NaN     NaN     NaN   \n",
       "1  30.533038  30.528442  30.520713  NaN     NaN     NaN     NaN     NaN   \n",
       "2  30.575468  30.564067  30.544764  NaN     NaN     NaN     NaN     NaN   \n",
       "3  30.622313  30.603605  30.571692  NaN     NaN     NaN     NaN     NaN   \n",
       "4  30.674673  30.647995  30.602149  NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   RSI_90      MACD  Signal_Line  MACD_Histogram  MACD_rolling_14  \\\n",
       "0     NaN  0.000000     0.000000        0.000000         0.000000   \n",
       "1     NaN  0.057037     0.011407        0.045630         0.028519   \n",
       "2     NaN  0.132584     0.035643        0.096941         0.063207   \n",
       "3     NaN  0.202627     0.069040        0.133588         0.098062   \n",
       "4     NaN  0.270153     0.109262        0.160890         0.132480   \n",
       "\n",
       "   Signal_rolling_14  MACD_Histogram_rolling_14  MACD_rolling_20  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_20  MACD_Histogram_rolling_20  MACD_rolling_30  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_30  MACD_Histogram_rolling_30  MACD_rolling_50  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_50  MACD_Histogram_rolling_50  MACD_rolling_60  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_60  MACD_Histogram_rolling_60  MACD_rolling_90  \\\n",
       "0           0.000000                   0.000000         0.000000   \n",
       "1           0.005704                   0.022815         0.028519   \n",
       "2           0.015683                   0.047524         0.063207   \n",
       "3           0.029022                   0.069040         0.098062   \n",
       "4           0.045070                   0.087410         0.132480   \n",
       "\n",
       "   Signal_rolling_90  MACD_Histogram_rolling_90         %K         %D  \\\n",
       "0           0.000000                   0.000000  93.467084  93.467084   \n",
       "1           0.005704                   0.022815  99.159644  96.313364   \n",
       "2           0.015683                   0.047524  86.065515  92.897414   \n",
       "3           0.029022                   0.069040  94.535498  93.253552   \n",
       "4           0.045070                   0.087410  87.958682  89.519898   \n",
       "\n",
       "       %K_20      %D_20      %K_30      %D_30      %K_50      %D_50  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  93.467084  93.467084   \n",
       "1  99.159644  96.313364  99.159644  96.313364  99.159644  96.313364   \n",
       "2  86.065515  92.897414  86.065515  92.897414  86.065515  92.897414   \n",
       "3  94.535498  93.253552  94.535498  93.253552  94.535498  93.253552   \n",
       "4  87.958682  89.519898  87.958682  89.519898  87.958682  89.519898   \n",
       "\n",
       "       %K_60      %D_60      %K_90      %D_90       VWAP    VWAP_14  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  30.504999  30.504999   \n",
       "1  99.159644  96.313364  99.159644  96.313364  30.892965  30.698982   \n",
       "2  86.065515  92.897414  86.065515  92.897414  31.148973  30.848979   \n",
       "3  94.535498  93.253552  94.535498  93.253552  31.276489  30.955857   \n",
       "4  87.958682  89.519898  87.958682  89.519898  31.407822  31.046250   \n",
       "\n",
       "     VWAP_20    VWAP_30    VWAP_50    VWAP_60    VWAP_90  bb_Middle_Band_14  \\\n",
       "0  30.504999  30.504999  30.504999  30.504999  30.504999          30.504999   \n",
       "1  30.698982  30.698982  30.698982  30.698982  30.698982          30.862499   \n",
       "2  30.848979  30.848979  30.848979  30.848979  30.848979          31.113333   \n",
       "3  30.955857  30.955857  30.955857  30.955857  30.955857          31.277500   \n",
       "4  31.046250  31.046250  31.046250  31.046250  31.046250          31.413500   \n",
       "\n",
       "   Std_Dev_14  Upper_Band_14  Lower_Band_14  bb_Middle_Band_20  Std_Dev_20  \\\n",
       "0         NaN            NaN            NaN          30.504999         NaN   \n",
       "1    0.505581      31.873662      29.851336          30.862499    0.505581   \n",
       "2    0.562635      32.238604      29.988062          31.113333    0.562635   \n",
       "3    0.564661      32.406822      30.148177          31.277500    0.564661   \n",
       "4    0.575858      32.565215      30.261785          31.413500    0.575858   \n",
       "\n",
       "   Upper_Band_20  Lower_Band_20  bb_Middle_Band_30  Std_Dev_30  Upper_Band_30  \\\n",
       "0            NaN            NaN          30.504999         NaN            NaN   \n",
       "1      31.873662      29.851336          30.862499    0.505581      31.873662   \n",
       "2      32.238604      29.988062          31.113333    0.562635      32.238604   \n",
       "3      32.406822      30.148177          31.277500    0.564661      32.406822   \n",
       "4      32.565215      30.261785          31.413500    0.575858      32.565215   \n",
       "\n",
       "   Lower_Band_30  bb_Middle_Band_50  Std_Dev_50  Upper_Band_50  Lower_Band_50  \\\n",
       "0            NaN          30.504999         NaN            NaN            NaN   \n",
       "1      29.851336          30.862499    0.505581      31.873662      29.851336   \n",
       "2      29.988062          31.113333    0.562635      32.238604      29.988062   \n",
       "3      30.148177          31.277500    0.564661      32.406822      30.148177   \n",
       "4      30.261785          31.413500    0.575858      32.565215      30.261785   \n",
       "\n",
       "   bb_Middle_Band_60  Std_Dev_60  Upper_Band_60  Lower_Band_60  \\\n",
       "0          30.504999         NaN            NaN            NaN   \n",
       "1          30.862499    0.505581      31.873662      29.851336   \n",
       "2          31.113333    0.562635      32.238604      29.988062   \n",
       "3          31.277500    0.564661      32.406822      30.148177   \n",
       "4          31.413500    0.575858      32.565215      30.261785   \n",
       "\n",
       "   bb_Middle_Band_90  Std_Dev_90  Upper_Band_90  Lower_Band_90       ATR  \\\n",
       "0          30.504999         NaN            NaN            NaN  0.497499   \n",
       "1          30.862499    0.505581      31.873662      29.851336  0.725000   \n",
       "2          31.113333    0.562635      32.238604      29.988062  0.650002   \n",
       "3          31.277500    0.564661      32.406822      30.148177  0.407499   \n",
       "4          31.413500    0.575858      32.565215      30.261785  0.490002   \n",
       "\n",
       "     ATR_14    ATR_20    ATR_30    ATR_50    ATR_60    ATR_90  \\\n",
       "0  0.497499  0.497499  0.497499  0.497499  0.497499  0.497499   \n",
       "1  0.611250  0.611250  0.611250  0.611250  0.611250  0.611250   \n",
       "2  0.624167  0.624167  0.624167  0.624167  0.624167  0.624167   \n",
       "3  0.570000  0.570000  0.570000  0.570000  0.570000  0.570000   \n",
       "4  0.554000  0.554000  0.554000  0.554000  0.554000  0.554000   \n",
       "\n",
       "   5_day_Fib_23.6%  5_day_Fib_38.2%  5_day_Fib_50.0%  5_day_Fib_61.8%  \\\n",
       "0        30.420091        30.347456        30.288751        30.230046   \n",
       "1        30.949160        30.775420        30.635000        30.494580   \n",
       "2        31.438121        31.170941        30.955001        30.739061   \n",
       "3        31.438121        31.170941        30.955001        30.739061   \n",
       "4        31.705521        31.387241        31.130001        30.872761   \n",
       "\n",
       "   5_day_Fib_78.6%  5_day_Fib_100.0%  5_day_Fib_161.8%  5_day_Fib_261.8%  \\\n",
       "0        30.146466         30.040001         29.732546         29.235047   \n",
       "1        30.294661         30.040001         29.304582         28.114583   \n",
       "2        30.431621         30.040001         28.909061         27.079061   \n",
       "3        30.431621         30.040001         28.909061         27.079061   \n",
       "4        30.506521         30.040001         28.692761         26.512760   \n",
       "\n",
       "   5_day_Fib_423.6%  14_day_Fib_23.6%  14_day_Fib_38.2%  14_day_Fib_50.0%  \\\n",
       "0         28.430093         30.420091         30.347456         30.288751   \n",
       "1         26.189165         30.949160         30.775420         30.635000   \n",
       "2         24.118121         31.438121         31.170941         30.955001   \n",
       "3         24.118121         31.438121         31.170941         30.955001   \n",
       "4         22.985520         31.705521         31.387241         31.130001   \n",
       "\n",
       "   14_day_Fib_61.8%  14_day_Fib_78.6%  14_day_Fib_100.0%  14_day_Fib_161.8%  \\\n",
       "0         30.230046         30.146466          30.040001          29.732546   \n",
       "1         30.494580         30.294661          30.040001          29.304582   \n",
       "2         30.739061         30.431621          30.040001          28.909061   \n",
       "3         30.739061         30.431621          30.040001          28.909061   \n",
       "4         30.872761         30.506521          30.040001          28.692761   \n",
       "\n",
       "   14_day_Fib_261.8%  14_day_Fib_423.6%  30_day_Fib_23.6%  30_day_Fib_38.2%  \\\n",
       "0          29.235047          28.430093         30.420091         30.347456   \n",
       "1          28.114583          26.189165         30.949160         30.775420   \n",
       "2          27.079061          24.118121         31.438121         31.170941   \n",
       "3          27.079061          24.118121         31.438121         31.170941   \n",
       "4          26.512760          22.985520         31.705521         31.387241   \n",
       "\n",
       "   30_day_Fib_50.0%  30_day_Fib_61.8%  30_day_Fib_78.6%  30_day_Fib_100.0%  \\\n",
       "0         30.288751         30.230046         30.146466          30.040001   \n",
       "1         30.635000         30.494580         30.294661          30.040001   \n",
       "2         30.955001         30.739061         30.431621          30.040001   \n",
       "3         30.955001         30.739061         30.431621          30.040001   \n",
       "4         31.130001         30.872761         30.506521          30.040001   \n",
       "\n",
       "   30_day_Fib_161.8%  30_day_Fib_261.8%  30_day_Fib_423.6%           OBV  \\\n",
       "0          29.732546          29.235047          28.430093  0.000000e+00   \n",
       "1          29.304582          28.114583          26.189165  2.942472e+08   \n",
       "2          28.909061          27.079061          24.118121  5.921452e+08   \n",
       "3          28.909061          27.079061          24.118121  8.092340e+08   \n",
       "4          28.692761          26.512760          22.985520  1.061844e+09   \n",
       "\n",
       "   OBV_14day_avg  OBV_20day_avg  OBV_30day_avg  OBV_50day_avg  OBV_60day_avg  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   OBV_90day_avg  Momentum_14  Momentum_20  Momentum_30  Momentum_50  \\\n",
       "0            0.0          0.0          0.0          0.0          0.0   \n",
       "1            0.0          0.0          0.0          0.0          0.0   \n",
       "2            0.0          0.0          0.0          0.0          0.0   \n",
       "3            0.0          0.0          0.0          0.0          0.0   \n",
       "4            0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   Momentum_60  Momentum_90  Rolling_Median_14  Rolling_Quantile_25_14  \\\n",
       "0          0.0          0.0                0.0                     0.0   \n",
       "1          0.0          0.0                0.0                     0.0   \n",
       "2          0.0          0.0                0.0                     0.0   \n",
       "3          0.0          0.0                0.0                     0.0   \n",
       "4          0.0          0.0                0.0                     0.0   \n",
       "\n",
       "   Rolling_Quantile_75_14  Rolling_Median_20  Rolling_Quantile_25_20  \\\n",
       "0                     0.0                0.0                     0.0   \n",
       "1                     0.0                0.0                     0.0   \n",
       "2                     0.0                0.0                     0.0   \n",
       "3                     0.0                0.0                     0.0   \n",
       "4                     0.0                0.0                     0.0   \n",
       "\n",
       "   Rolling_Quantile_75_20  Rolling_Median_30  Rolling_Quantile_25_30  \\\n",
       "0                     0.0                0.0                     0.0   \n",
       "1                     0.0                0.0                     0.0   \n",
       "2                     0.0                0.0                     0.0   \n",
       "3                     0.0                0.0                     0.0   \n",
       "4                     0.0                0.0                     0.0   \n",
       "\n",
       "   Rolling_Quantile_75_30  Rolling_Median_50  Rolling_Quantile_25_50  \\\n",
       "0                     0.0                0.0                     0.0   \n",
       "1                     0.0                0.0                     0.0   \n",
       "2                     0.0                0.0                     0.0   \n",
       "3                     0.0                0.0                     0.0   \n",
       "4                     0.0                0.0                     0.0   \n",
       "\n",
       "   Rolling_Quantile_75_50  Rolling_Median_60  Rolling_Quantile_25_60  \\\n",
       "0                     0.0                0.0                     0.0   \n",
       "1                     0.0                0.0                     0.0   \n",
       "2                     0.0                0.0                     0.0   \n",
       "3                     0.0                0.0                     0.0   \n",
       "4                     0.0                0.0                     0.0   \n",
       "\n",
       "   Rolling_Quantile_75_60  Rolling_Median_90  Rolling_Quantile_25_90  \\\n",
       "0                     0.0                0.0                     0.0   \n",
       "1                     0.0                0.0                     0.0   \n",
       "2                     0.0                0.0                     0.0   \n",
       "3                     0.0                0.0                     0.0   \n",
       "4                     0.0                0.0                     0.0   \n",
       "\n",
       "   Rolling_Quantile_75_90  Close_lag_30  Close_lag_90  SMA_90_lag_30  \\\n",
       "0                     0.0           NaN           NaN            NaN   \n",
       "1                     0.0           NaN           NaN            NaN   \n",
       "2                     0.0           NaN           NaN            NaN   \n",
       "3                     0.0           NaN           NaN            NaN   \n",
       "4                     0.0           NaN           NaN            NaN   \n",
       "\n",
       "   SMA_90_lag_90  SMA_30_lag_30  SMA_30_lag_90  EMA_90_lag_30  EMA_90_lag_90  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   EMA_30_lag_30  EMA_30_lag_90  Volume_lag_30  Volume_lag_90  RSI_lag_30  \\\n",
       "0            NaN            NaN            NaN            NaN         NaN   \n",
       "1            NaN            NaN            NaN            NaN         NaN   \n",
       "2            NaN            NaN            NaN            NaN         NaN   \n",
       "3            NaN            NaN            NaN            NaN         NaN   \n",
       "4            NaN            NaN            NaN            NaN         NaN   \n",
       "\n",
       "   RSI_lag_90  RSI_90_lag_30  RSI_90_lag_90  RSI_30_lag_30  RSI_30_lag_90  \\\n",
       "0         NaN            NaN            NaN            NaN            NaN   \n",
       "1         NaN            NaN            NaN            NaN            NaN   \n",
       "2         NaN            NaN            NaN            NaN            NaN   \n",
       "3         NaN            NaN            NaN            NaN            NaN   \n",
       "4         NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   Signal_Line_lag_30  Signal_Line_lag_90  MACD_lag_30  MACD_lag_90  \\\n",
       "0                 NaN                 NaN          NaN          NaN   \n",
       "1                 NaN                 NaN          NaN          NaN   \n",
       "2                 NaN                 NaN          NaN          NaN   \n",
       "3                 NaN                 NaN          NaN          NaN   \n",
       "4                 NaN                 NaN          NaN          NaN   \n",
       "\n",
       "   VWAP_lag_30  VWAP_lag_90  %K_lag_30  %K_lag_90  %D_lag_30  %D_lag_90  \\\n",
       "0          NaN          NaN        NaN        NaN        NaN        NaN   \n",
       "1          NaN          NaN        NaN        NaN        NaN        NaN   \n",
       "2          NaN          NaN        NaN        NaN        NaN        NaN   \n",
       "3          NaN          NaN        NaN        NaN        NaN        NaN   \n",
       "4          NaN          NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "   OBV_lag_30  OBV_lag_90  Momentum_90_lag_30  Momentum_90_lag_90  \\\n",
       "0         NaN         NaN                 NaN                 NaN   \n",
       "1         NaN         NaN                 NaN                 NaN   \n",
       "2         NaN         NaN                 NaN                 NaN   \n",
       "3         NaN         NaN                 NaN                 NaN   \n",
       "4         NaN         NaN                 NaN                 NaN   \n",
       "\n",
       "   Momentum_30_lag_30  Momentum_30_lag_90  Std_Dev_90_lag_30  \\\n",
       "0                 NaN                 NaN                NaN   \n",
       "1                 NaN                 NaN                NaN   \n",
       "2                 NaN                 NaN                NaN   \n",
       "3                 NaN                 NaN                NaN   \n",
       "4                 NaN                 NaN                NaN   \n",
       "\n",
       "   Std_Dev_90_lag_90  Std_Dev_30_lag_30  Std_Dev_30_lag_90  \\\n",
       "0                NaN                NaN                NaN   \n",
       "1                NaN                NaN                NaN   \n",
       "2                NaN                NaN                NaN   \n",
       "3                NaN                NaN                NaN   \n",
       "4                NaN                NaN                NaN   \n",
       "\n",
       "   Rolling_Median_90_lag_30  Rolling_Median_90_lag_90  \\\n",
       "0                       NaN                       NaN   \n",
       "1                       NaN                       NaN   \n",
       "2                       NaN                       NaN   \n",
       "3                       NaN                       NaN   \n",
       "4                       NaN                       NaN   \n",
       "\n",
       "   Rolling_Median_30_lag_30  Rolling_Median_30_lag_90  \\\n",
       "0                       NaN                       NaN   \n",
       "1                       NaN                       NaN   \n",
       "2                       NaN                       NaN   \n",
       "3                       NaN                       NaN   \n",
       "4                       NaN                       NaN   \n",
       "\n",
       "   Rolling_Quantile_25_90_lag_30  Rolling_Quantile_25_90_lag_90  \\\n",
       "0                            NaN                            NaN   \n",
       "1                            NaN                            NaN   \n",
       "2                            NaN                            NaN   \n",
       "3                            NaN                            NaN   \n",
       "4                            NaN                            NaN   \n",
       "\n",
       "   Rolling_Quantile_25_30_lag_30  Rolling_Quantile_25_30_lag_90  \\\n",
       "0                            NaN                            NaN   \n",
       "1                            NaN                            NaN   \n",
       "2                            NaN                            NaN   \n",
       "3                            NaN                            NaN   \n",
       "4                            NaN                            NaN   \n",
       "\n",
       "   Rolling_Quantile_75_90_lag_30  Rolling_Quantile_75_90_lag_90  \\\n",
       "0                            NaN                            NaN   \n",
       "1                            NaN                            NaN   \n",
       "2                            NaN                            NaN   \n",
       "3                            NaN                            NaN   \n",
       "4                            NaN                            NaN   \n",
       "\n",
       "   Rolling_Quantile_75_30_lag_30  Rolling_Quantile_75_30_lag_90  \\\n",
       "0                            NaN                            NaN   \n",
       "1                            NaN                            NaN   \n",
       "2                            NaN                            NaN   \n",
       "3                            NaN                            NaN   \n",
       "4                            NaN                            NaN   \n",
       "\n",
       "   Daily_High_90day_avg_lag_30  Daily_High_90day_avg_lag_90  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          NaN                          NaN   \n",
       "4                          NaN                          NaN   \n",
       "\n",
       "   Daily_High_30day_avg_lag_30  Daily_High_30day_avg_lag_90  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          NaN                          NaN   \n",
       "4                          NaN                          NaN   \n",
       "\n",
       "   Daily_Low_90day_avg_lag_30  Daily_Low_90day_avg_lag_90  \\\n",
       "0                         NaN                         NaN   \n",
       "1                         NaN                         NaN   \n",
       "2                         NaN                         NaN   \n",
       "3                         NaN                         NaN   \n",
       "4                         NaN                         NaN   \n",
       "\n",
       "   Daily_Low_30day_avg_lag_30  Daily_Low_30day_avg_lag_90  \\\n",
       "0                         NaN                         NaN   \n",
       "1                         NaN                         NaN   \n",
       "2                         NaN                         NaN   \n",
       "3                         NaN                         NaN   \n",
       "4                         NaN                         NaN   \n",
       "\n",
       "   Volume_90day_avg_lag_30  Volume_90day_avg_lag_90  Volume_30day_avg_lag_30  \\\n",
       "0                      NaN                      NaN                      NaN   \n",
       "1                      NaN                      NaN                      NaN   \n",
       "2                      NaN                      NaN                      NaN   \n",
       "3                      NaN                      NaN                      NaN   \n",
       "4                      NaN                      NaN                      NaN   \n",
       "\n",
       "   Volume_30day_avg_lag_90  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_3_month.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e9b57-176d-48e3-9d2b-8c69df3b69c2",
   "metadata": {},
   "source": [
    "RSI\n",
    "MACD and MACD Signal Line\n",
    "Stochastic Oscillator\n",
    "VWAP\n",
    "Bollinger Bands (Upper and Lower)\n",
    "Price Range (High - Low)\n",
    "Momentum and Standard Deviation\n",
    "Fib Levels\n",
    "OBV\n",
    "Quantile Features (Median, Upper/Lower Quantile)\n",
    "Weighted Moving Average (WMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c54763e8-88ac-4442-93eb-be7c0f83a0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Column Pairs:\n",
      "Column a: ['SMA_3', 'SMA_5', 'SMA_7']\n",
      "Column b: ['bb_Middle_Band_3', 'bb_Middle_Band_5', 'bb_Middle_Band_7']\n"
     ]
    }
   ],
   "source": [
    "# Check if we accidentally have the same values in any of these columns (rounded to 4 decimal places)\n",
    "# Get the first 1000 rows of the dataframe\n",
    "# one week\n",
    "df_sample_1_week = stock_data_1_week.head(2500)\n",
    "\n",
    "# Exclude 'Symbol' and 'Date' columns\n",
    "columns = [col for col in df_sample_1_week.columns if col not in ['Symbol', 'Date']]\n",
    "\n",
    "# Lists to store matching column pairs\n",
    "col_a_list = []\n",
    "col_b_list = []\n",
    "\n",
    "# Loop through all pairs of remaining columns and compare their values rounded to 4 decimal places\n",
    "for a in range(len(columns)):\n",
    "    for b in range(a + 1, len(columns)):  # Avoid duplicate comparisons\n",
    "        if (df_sample_1_week[columns[a]].round(4) == df_sample_1_week[columns[b]].round(4)).all():\n",
    "            col_a_list.append(columns[a])\n",
    "            col_b_list.append(columns[b])\n",
    "\n",
    "# Print the two lists\n",
    "print(\"Matching Column Pairs:\")\n",
    "print(\"Column a:\", col_a_list)\n",
    "print(\"Column b:\", col_b_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2500c1a7-533e-44bb-93f6-48ed8b6e4a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Column Pairs:\n",
      "Column c: ['SMA_7', 'SMA_10', 'SMA_14', 'SMA_20', 'SMA_30']\n",
      "Column d: ['bb_Middle_Band_7', 'bb_Middle_Band_10', 'bb_Middle_Band_14', 'bb_Middle_Band_20', 'bb_Middle_Band_30']\n"
     ]
    }
   ],
   "source": [
    "# Check if we accidentally have the same values in any of these columns (rounded to 4 decimal places)\n",
    "# Get the first 1000 rows of the dataframe\n",
    "# one month\n",
    "df_sample_1_month = stock_data_1_month.head(2500)\n",
    "\n",
    "# Exclude 'Symbol' and 'Date' columns\n",
    "columns = [col for col in df_sample_1_month.columns if col not in ['Symbol', 'Date']]\n",
    "\n",
    "# Lists to store matching column pairs\n",
    "col_c_list = []\n",
    "col_d_list = []\n",
    "\n",
    "# Loop through all pairs of remaining columns and compare their values rounded to 4 decimal places\n",
    "for c in range(len(columns)):\n",
    "    for d in range(c + 1, len(columns)):  # Avoid duplicate comparisons\n",
    "        if (df_sample_1_month[columns[c]].round(4) == df_sample_1_month[columns[d]].round(4)).all():\n",
    "            col_c_list.append(columns[c])\n",
    "            col_d_list.append(columns[d])\n",
    "\n",
    "# Print the two lists\n",
    "print(\"Matching Column Pairs:\")\n",
    "print(\"Column c:\", col_c_list)\n",
    "print(\"Column d:\", col_d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c23a9b23-6434-4d0d-9fa8-4d56af2e1781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Column Pairs:\n",
      "Column e: ['SMA_14', 'SMA_20', 'SMA_30', 'SMA_50', 'SMA_60', 'SMA_90']\n",
      "Column f: ['bb_Middle_Band_14', 'bb_Middle_Band_20', 'bb_Middle_Band_30', 'bb_Middle_Band_50', 'bb_Middle_Band_60', 'bb_Middle_Band_90']\n"
     ]
    }
   ],
   "source": [
    "# Check if we accidentally have the same values in any of these columns (rounded to 4 decimal places)\n",
    "# Get the first 1000 rows of the dataframe\n",
    "# three month\n",
    "df_sample_3_month = stock_data_3_month.head(2500)\n",
    "\n",
    "# Exclude 'Symbol' and 'Date' columns\n",
    "columns = [col for col in df_sample_3_month.columns if col not in ['Symbol', 'Date']]\n",
    "\n",
    "# Lists to store matching column pairs\n",
    "col_e_list = []\n",
    "col_f_list = []\n",
    "\n",
    "# Loop through all pairs of remaining columns and compare their values rounded to 4 decimal places\n",
    "for e in range(len(columns)):\n",
    "    for f in range(e + 1, len(columns)):  # Avoid duplicate comparisons\n",
    "        if (df_sample_3_month[columns[e]].round(4) == df_sample_3_month[columns[f]].round(4)).all():\n",
    "            col_e_list.append(columns[e])\n",
    "            col_f_list.append(columns[f])\n",
    "\n",
    "# Print the two lists\n",
    "print(\"Matching Column Pairs:\")\n",
    "print(\"Column e:\", col_e_list)\n",
    "print(\"Column f:\", col_f_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "927720ca-41b7-45ee-86e6-40c2d2786f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(489134, 167)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_1_week.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "11da9df5-1d67-4130-ba84-05b40b03de03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(489134, 206)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_1_month.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b84947c2-3bb0-44ad-9101-bcc108bd9cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(489134, 227)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_3_month.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "888e282b-ff6a-49ee-a6dd-5409e2945e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_columns = col_b_list + col_d_list + col_f_list\n",
    "for df in stock_dataframes:\n",
    "    for col in repeat_columns:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4f6451a8-1507-4b9b-8b54-dfba010202e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Daily_High</th>\n",
       "      <th>Daily_Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volume_3day_avg</th>\n",
       "      <th>Volume_5day_avg</th>\n",
       "      <th>Volume_7day_avg</th>\n",
       "      <th>Daily_High_3day_avg</th>\n",
       "      <th>Daily_High_5day_avg</th>\n",
       "      <th>Daily_High_7day_avg</th>\n",
       "      <th>Daily_Low_3day_avg</th>\n",
       "      <th>Daily_Low_5day_avg</th>\n",
       "      <th>Daily_Low_7day_avg</th>\n",
       "      <th>SMA_3</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_7</th>\n",
       "      <th>EMA_3</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>EMA_7</th>\n",
       "      <th>RSI</th>\n",
       "      <th>RSI_3</th>\n",
       "      <th>RSI_5</th>\n",
       "      <th>RSI_7</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "      <th>MACD_rolling_3</th>\n",
       "      <th>Signal_rolling_3</th>\n",
       "      <th>MACD_Histogram_rolling_3</th>\n",
       "      <th>MACD_rolling_5</th>\n",
       "      <th>Signal_rolling_5</th>\n",
       "      <th>MACD_Histogram_rolling_5</th>\n",
       "      <th>MACD_rolling_7</th>\n",
       "      <th>Signal_rolling_7</th>\n",
       "      <th>MACD_Histogram_rolling_7</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>%K_3</th>\n",
       "      <th>%D_3</th>\n",
       "      <th>%K_5</th>\n",
       "      <th>%D_5</th>\n",
       "      <th>%K_7</th>\n",
       "      <th>%D_7</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>VWAP_3</th>\n",
       "      <th>VWAP_5</th>\n",
       "      <th>VWAP_7</th>\n",
       "      <th>bb_Middle_Band_3</th>\n",
       "      <th>Std_Dev_3</th>\n",
       "      <th>Upper_Band_3</th>\n",
       "      <th>Lower_Band_3</th>\n",
       "      <th>bb_Middle_Band_5</th>\n",
       "      <th>Std_Dev_5</th>\n",
       "      <th>Upper_Band_5</th>\n",
       "      <th>Lower_Band_5</th>\n",
       "      <th>bb_Middle_Band_7</th>\n",
       "      <th>Std_Dev_7</th>\n",
       "      <th>Upper_Band_7</th>\n",
       "      <th>Lower_Band_7</th>\n",
       "      <th>ATR</th>\n",
       "      <th>ATR_3</th>\n",
       "      <th>ATR_5</th>\n",
       "      <th>ATR_7</th>\n",
       "      <th>5_day_Fib_23.6%</th>\n",
       "      <th>5_day_Fib_38.2%</th>\n",
       "      <th>5_day_Fib_50.0%</th>\n",
       "      <th>5_day_Fib_61.8%</th>\n",
       "      <th>5_day_Fib_78.6%</th>\n",
       "      <th>5_day_Fib_100.0%</th>\n",
       "      <th>5_day_Fib_161.8%</th>\n",
       "      <th>5_day_Fib_261.8%</th>\n",
       "      <th>5_day_Fib_423.6%</th>\n",
       "      <th>14_day_Fib_23.6%</th>\n",
       "      <th>14_day_Fib_38.2%</th>\n",
       "      <th>14_day_Fib_50.0%</th>\n",
       "      <th>14_day_Fib_61.8%</th>\n",
       "      <th>14_day_Fib_78.6%</th>\n",
       "      <th>14_day_Fib_100.0%</th>\n",
       "      <th>14_day_Fib_161.8%</th>\n",
       "      <th>14_day_Fib_261.8%</th>\n",
       "      <th>14_day_Fib_423.6%</th>\n",
       "      <th>30_day_Fib_23.6%</th>\n",
       "      <th>30_day_Fib_38.2%</th>\n",
       "      <th>30_day_Fib_50.0%</th>\n",
       "      <th>30_day_Fib_61.8%</th>\n",
       "      <th>30_day_Fib_78.6%</th>\n",
       "      <th>30_day_Fib_100.0%</th>\n",
       "      <th>30_day_Fib_161.8%</th>\n",
       "      <th>30_day_Fib_261.8%</th>\n",
       "      <th>30_day_Fib_423.6%</th>\n",
       "      <th>OBV</th>\n",
       "      <th>OBV_3day_avg</th>\n",
       "      <th>OBV_5day_avg</th>\n",
       "      <th>OBV_7day_avg</th>\n",
       "      <th>Momentum_3</th>\n",
       "      <th>Momentum_5</th>\n",
       "      <th>Momentum_7</th>\n",
       "      <th>Rolling_Median_3</th>\n",
       "      <th>Rolling_Quantile_25_3</th>\n",
       "      <th>Rolling_Quantile_75_3</th>\n",
       "      <th>Rolling_Median_5</th>\n",
       "      <th>Rolling_Quantile_25_5</th>\n",
       "      <th>Rolling_Quantile_75_5</th>\n",
       "      <th>Rolling_Median_7</th>\n",
       "      <th>Rolling_Quantile_25_7</th>\n",
       "      <th>Rolling_Quantile_75_7</th>\n",
       "      <th>Close_lag_3</th>\n",
       "      <th>Close_lag_7</th>\n",
       "      <th>SMA_3_lag_3</th>\n",
       "      <th>SMA_3_lag_7</th>\n",
       "      <th>SMA_7_lag_3</th>\n",
       "      <th>SMA_7_lag_7</th>\n",
       "      <th>EMA_3_lag_3</th>\n",
       "      <th>EMA_3_lag_7</th>\n",
       "      <th>EMA_7_lag_3</th>\n",
       "      <th>EMA_7_lag_7</th>\n",
       "      <th>Volume_lag_3</th>\n",
       "      <th>Volume_lag_7</th>\n",
       "      <th>RSI_lag_3</th>\n",
       "      <th>RSI_lag_7</th>\n",
       "      <th>RSI_3_lag_3</th>\n",
       "      <th>RSI_3_lag_7</th>\n",
       "      <th>RSI_7_lag_3</th>\n",
       "      <th>RSI_7_lag_7</th>\n",
       "      <th>Signal_Line_lag_3</th>\n",
       "      <th>Signal_Line_lag_7</th>\n",
       "      <th>MACD_lag_3</th>\n",
       "      <th>MACD_lag_7</th>\n",
       "      <th>VWAP_lag_3</th>\n",
       "      <th>VWAP_lag_7</th>\n",
       "      <th>%K_lag_3</th>\n",
       "      <th>%K_lag_7</th>\n",
       "      <th>%D_lag_3</th>\n",
       "      <th>%D_lag_7</th>\n",
       "      <th>OBV_lag_3</th>\n",
       "      <th>OBV_lag_7</th>\n",
       "      <th>Momentum_3_lag_3</th>\n",
       "      <th>Momentum_3_lag_7</th>\n",
       "      <th>Momentum_7_lag_3</th>\n",
       "      <th>Momentum_7_lag_7</th>\n",
       "      <th>Std_Dev_3_lag_3</th>\n",
       "      <th>Std_Dev_3_lag_7</th>\n",
       "      <th>Std_Dev_7_lag_3</th>\n",
       "      <th>Std_Dev_7_lag_7</th>\n",
       "      <th>Rolling_Median_3_lag_3</th>\n",
       "      <th>Rolling_Median_3_lag_7</th>\n",
       "      <th>Rolling_Median_7_lag_3</th>\n",
       "      <th>Rolling_Median_7_lag_7</th>\n",
       "      <th>Rolling_Quantile_25_3_lag_3</th>\n",
       "      <th>Rolling_Quantile_25_3_lag_7</th>\n",
       "      <th>Rolling_Quantile_25_7_lag_3</th>\n",
       "      <th>Rolling_Quantile_25_7_lag_7</th>\n",
       "      <th>Rolling_Quantile_75_3_lag_3</th>\n",
       "      <th>Rolling_Quantile_75_3_lag_7</th>\n",
       "      <th>Rolling_Quantile_75_7_lag_3</th>\n",
       "      <th>Rolling_Quantile_75_7_lag_7</th>\n",
       "      <th>Daily_High_3day_avg_lag_3</th>\n",
       "      <th>Daily_High_3day_avg_lag_7</th>\n",
       "      <th>Daily_High_7day_avg_lag_3</th>\n",
       "      <th>Daily_High_7day_avg_lag_7</th>\n",
       "      <th>Daily_Low_3day_avg_lag_3</th>\n",
       "      <th>Daily_Low_3day_avg_lag_7</th>\n",
       "      <th>Daily_Low_7day_avg_lag_3</th>\n",
       "      <th>Daily_Low_7day_avg_lag_7</th>\n",
       "      <th>Volume_3day_avg_lag_3</th>\n",
       "      <th>Volume_3day_avg_lag_7</th>\n",
       "      <th>Volume_7day_avg_lag_3</th>\n",
       "      <th>Volume_7day_avg_lag_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>248034000</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>294247200</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.743333</td>\n",
       "      <td>30.683749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.045630</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>30.892965</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>2.942472e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>297898000</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.238750</td>\n",
       "      <td>31.033888</td>\n",
       "      <td>30.916562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.132584</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>31.148973</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>0.650002</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>5.921452e+08</td>\n",
       "      <td>2.954641e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>31.41750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>217088800</td>\n",
       "      <td>2.697447e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>31.640000</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.143333</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>31.535000</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.504375</td>\n",
       "      <td>31.279259</td>\n",
       "      <td>31.129921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202627</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.133588</td>\n",
       "      <td>0.130750</td>\n",
       "      <td>0.038697</td>\n",
       "      <td>0.092053</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>91.967846</td>\n",
       "      <td>92.397668</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>31.276489</td>\n",
       "      <td>31.106143</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>31.535000</td>\n",
       "      <td>0.283594</td>\n",
       "      <td>32.102187</td>\n",
       "      <td>30.967813</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>0.407499</td>\n",
       "      <td>0.594167</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>8.092340e+08</td>\n",
       "      <td>5.652088e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.265001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.417500</td>\n",
       "      <td>31.69250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248034000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.53750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.53750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248034000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248034000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>252609600</td>\n",
       "      <td>2.558655e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>31.970001</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.511667</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.780834</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.730938</td>\n",
       "      <td>31.505339</td>\n",
       "      <td>31.336816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.270153</td>\n",
       "      <td>0.109262</td>\n",
       "      <td>0.160890</td>\n",
       "      <td>0.201788</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.130473</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>68.277903</td>\n",
       "      <td>82.103755</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>31.407822</td>\n",
       "      <td>31.277761</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.780834</td>\n",
       "      <td>0.171507</td>\n",
       "      <td>32.123848</td>\n",
       "      <td>31.437819</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>0.490002</td>\n",
       "      <td>0.515834</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>1.061844e+09</td>\n",
       "      <td>8.210743e+08</td>\n",
       "      <td>551494000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.737501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.692500</td>\n",
       "      <td>31.86375</td>\n",
       "      <td>31.615</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.683749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>294247200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.892965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>294247200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.88375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.88375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271140600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271140600.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol       Date      Close  Daily_High  Daily_Low     Volume  \\\n",
       "0   AAPL 2015-02-10  30.504999   30.537500  30.040001  248034000   \n",
       "1   AAPL 2015-02-11  31.219999   31.230000  30.625000  294247200   \n",
       "2   AAPL 2015-02-12  31.615000   31.870001  31.392500  297898000   \n",
       "3   AAPL 2015-02-13  31.770000   31.820000  31.412500  217088800   \n",
       "4   AAPL 2015-02-17  31.957500   32.220001  31.730000  252609600   \n",
       "\n",
       "   Volume_3day_avg  Volume_5day_avg  Volume_7day_avg  Daily_High_3day_avg  \\\n",
       "0     2.480340e+08     2.480340e+08     2.480340e+08            30.537500   \n",
       "1     2.711406e+08     2.711406e+08     2.711406e+08            30.883750   \n",
       "2     2.800597e+08     2.800597e+08     2.800597e+08            31.212500   \n",
       "3     2.697447e+08     2.643170e+08     2.643170e+08            31.640000   \n",
       "4     2.558655e+08     2.619755e+08     2.619755e+08            31.970001   \n",
       "\n",
       "   Daily_High_5day_avg  Daily_High_7day_avg  Daily_Low_3day_avg  \\\n",
       "0            30.537500            30.537500           30.040001   \n",
       "1            30.883750            30.883750           30.332500   \n",
       "2            31.212500            31.212500           30.685834   \n",
       "3            31.364375            31.364375           31.143333   \n",
       "4            31.535500            31.535500           31.511667   \n",
       "\n",
       "   Daily_Low_5day_avg  Daily_Low_7day_avg      SMA_3      SMA_5      SMA_7  \\\n",
       "0           30.040001           30.040001  30.504999  30.504999  30.504999   \n",
       "1           30.332500           30.332500  30.862499  30.862499  30.862499   \n",
       "2           30.685834           30.685834  31.113333  31.113333  31.113333   \n",
       "3           30.867500           30.867500  31.535000  31.277500  31.277500   \n",
       "4           31.040000           31.040000  31.780834  31.413500  31.413500   \n",
       "\n",
       "       EMA_3      EMA_5      EMA_7  RSI  RSI_3  RSI_5  RSI_7      MACD  \\\n",
       "0  30.504999  30.504999  30.504999  NaN    NaN    NaN    NaN  0.000000   \n",
       "1  30.862499  30.743333  30.683749  NaN    NaN    NaN    NaN  0.057037   \n",
       "2  31.238750  31.033888  30.916562  NaN  100.0    NaN    NaN  0.132584   \n",
       "3  31.504375  31.279259  31.129921  NaN  100.0    NaN    NaN  0.202627   \n",
       "4  31.730938  31.505339  31.336816  NaN  100.0  100.0    NaN  0.270153   \n",
       "\n",
       "   Signal_Line  MACD_Histogram  MACD_rolling_3  Signal_rolling_3  \\\n",
       "0     0.000000        0.000000        0.000000          0.000000   \n",
       "1     0.011407        0.045630        0.028519          0.005704   \n",
       "2     0.035643        0.096941        0.063207          0.015683   \n",
       "3     0.069040        0.133588        0.130750          0.038697   \n",
       "4     0.109262        0.160890        0.201788          0.071315   \n",
       "\n",
       "   MACD_Histogram_rolling_3  MACD_rolling_5  Signal_rolling_5  \\\n",
       "0                  0.000000        0.000000          0.000000   \n",
       "1                  0.022815        0.028519          0.005704   \n",
       "2                  0.047524        0.063207          0.015683   \n",
       "3                  0.092053        0.098062          0.029022   \n",
       "4                  0.130473        0.132480          0.045070   \n",
       "\n",
       "   MACD_Histogram_rolling_5  MACD_rolling_7  Signal_rolling_7  \\\n",
       "0                  0.000000        0.000000          0.000000   \n",
       "1                  0.022815        0.028519          0.005704   \n",
       "2                  0.047524        0.063207          0.015683   \n",
       "3                  0.069040        0.098062          0.029022   \n",
       "4                  0.087410        0.132480          0.045070   \n",
       "\n",
       "   MACD_Histogram_rolling_7         %K         %D       %K_3       %D_3  \\\n",
       "0                  0.000000  93.467084  93.467084  93.467084  93.467084   \n",
       "1                  0.022815  99.159644  96.313364  99.159644  96.313364   \n",
       "2                  0.047524  86.065515  92.897414  86.065515  92.897414   \n",
       "3                  0.069040  94.535498  93.253552  91.967846  92.397668   \n",
       "4                  0.087410  87.958682  89.519898  68.277903  82.103755   \n",
       "\n",
       "        %K_5       %D_5       %K_7       %D_7       VWAP     VWAP_3  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  30.504999  30.504999   \n",
       "1  99.159644  96.313364  99.159644  96.313364  30.892965  30.698982   \n",
       "2  86.065515  92.897414  86.065515  92.897414  31.148973  30.848979   \n",
       "3  94.535498  93.253552  94.535498  93.253552  31.276489  31.106143   \n",
       "4  87.958682  89.519898  87.958682  89.519898  31.407822  31.277761   \n",
       "\n",
       "      VWAP_5     VWAP_7  bb_Middle_Band_3  Std_Dev_3  Upper_Band_3  \\\n",
       "0  30.504999  30.504999         30.504999        NaN           NaN   \n",
       "1  30.698982  30.698982         30.862499   0.505581     31.873662   \n",
       "2  30.848979  30.848979         31.113333   0.562635     32.238604   \n",
       "3  30.955857  30.955857         31.535000   0.283594     32.102187   \n",
       "4  31.046250  31.046250         31.780834   0.171507     32.123848   \n",
       "\n",
       "   Lower_Band_3  bb_Middle_Band_5  Std_Dev_5  Upper_Band_5  Lower_Band_5  \\\n",
       "0           NaN         30.504999        NaN           NaN           NaN   \n",
       "1     29.851336         30.862499   0.505581     31.873662     29.851336   \n",
       "2     29.988062         31.113333   0.562635     32.238604     29.988062   \n",
       "3     30.967813         31.277500   0.564661     32.406822     30.148177   \n",
       "4     31.437819         31.413500   0.575858     32.565215     30.261785   \n",
       "\n",
       "   bb_Middle_Band_7  Std_Dev_7  Upper_Band_7  Lower_Band_7       ATR  \\\n",
       "0         30.504999        NaN           NaN           NaN  0.497499   \n",
       "1         30.862499   0.505581     31.873662     29.851336  0.725000   \n",
       "2         31.113333   0.562635     32.238604     29.988062  0.650002   \n",
       "3         31.277500   0.564661     32.406822     30.148177  0.407499   \n",
       "4         31.413500   0.575858     32.565215     30.261785  0.490002   \n",
       "\n",
       "      ATR_3     ATR_5     ATR_7  5_day_Fib_23.6%  5_day_Fib_38.2%  \\\n",
       "0  0.497499  0.497499  0.497499        30.420091        30.347456   \n",
       "1  0.611250  0.611250  0.611250        30.949160        30.775420   \n",
       "2  0.624167  0.624167  0.624167        31.438121        31.170941   \n",
       "3  0.594167  0.570000  0.570000        31.438121        31.170941   \n",
       "4  0.515834  0.554000  0.554000        31.705521        31.387241   \n",
       "\n",
       "   5_day_Fib_50.0%  5_day_Fib_61.8%  5_day_Fib_78.6%  5_day_Fib_100.0%  \\\n",
       "0        30.288751        30.230046        30.146466         30.040001   \n",
       "1        30.635000        30.494580        30.294661         30.040001   \n",
       "2        30.955001        30.739061        30.431621         30.040001   \n",
       "3        30.955001        30.739061        30.431621         30.040001   \n",
       "4        31.130001        30.872761        30.506521         30.040001   \n",
       "\n",
       "   5_day_Fib_161.8%  5_day_Fib_261.8%  5_day_Fib_423.6%  14_day_Fib_23.6%  \\\n",
       "0         29.732546         29.235047         28.430093         30.420091   \n",
       "1         29.304582         28.114583         26.189165         30.949160   \n",
       "2         28.909061         27.079061         24.118121         31.438121   \n",
       "3         28.909061         27.079061         24.118121         31.438121   \n",
       "4         28.692761         26.512760         22.985520         31.705521   \n",
       "\n",
       "   14_day_Fib_38.2%  14_day_Fib_50.0%  14_day_Fib_61.8%  14_day_Fib_78.6%  \\\n",
       "0         30.347456         30.288751         30.230046         30.146466   \n",
       "1         30.775420         30.635000         30.494580         30.294661   \n",
       "2         31.170941         30.955001         30.739061         30.431621   \n",
       "3         31.170941         30.955001         30.739061         30.431621   \n",
       "4         31.387241         31.130001         30.872761         30.506521   \n",
       "\n",
       "   14_day_Fib_100.0%  14_day_Fib_161.8%  14_day_Fib_261.8%  14_day_Fib_423.6%  \\\n",
       "0          30.040001          29.732546          29.235047          28.430093   \n",
       "1          30.040001          29.304582          28.114583          26.189165   \n",
       "2          30.040001          28.909061          27.079061          24.118121   \n",
       "3          30.040001          28.909061          27.079061          24.118121   \n",
       "4          30.040001          28.692761          26.512760          22.985520   \n",
       "\n",
       "   30_day_Fib_23.6%  30_day_Fib_38.2%  30_day_Fib_50.0%  30_day_Fib_61.8%  \\\n",
       "0         30.420091         30.347456         30.288751         30.230046   \n",
       "1         30.949160         30.775420         30.635000         30.494580   \n",
       "2         31.438121         31.170941         30.955001         30.739061   \n",
       "3         31.438121         31.170941         30.955001         30.739061   \n",
       "4         31.705521         31.387241         31.130001         30.872761   \n",
       "\n",
       "   30_day_Fib_78.6%  30_day_Fib_100.0%  30_day_Fib_161.8%  30_day_Fib_261.8%  \\\n",
       "0         30.146466          30.040001          29.732546          29.235047   \n",
       "1         30.294661          30.040001          29.304582          28.114583   \n",
       "2         30.431621          30.040001          28.909061          27.079061   \n",
       "3         30.431621          30.040001          28.909061          27.079061   \n",
       "4         30.506521          30.040001          28.692761          26.512760   \n",
       "\n",
       "   30_day_Fib_423.6%           OBV  OBV_3day_avg  OBV_5day_avg  OBV_7day_avg  \\\n",
       "0          28.430093  0.000000e+00  0.000000e+00           0.0           0.0   \n",
       "1          26.189165  2.942472e+08  0.000000e+00           0.0           0.0   \n",
       "2          24.118121  5.921452e+08  2.954641e+08           0.0           0.0   \n",
       "3          24.118121  8.092340e+08  5.652088e+08           0.0           0.0   \n",
       "4          22.985520  1.061844e+09  8.210743e+08   551494000.0           0.0   \n",
       "\n",
       "   Momentum_3  Momentum_5  Momentum_7  Rolling_Median_3  \\\n",
       "0    0.000000         0.0         0.0          0.000000   \n",
       "1    0.000000         0.0         0.0          0.000000   \n",
       "2    0.000000         0.0         0.0         31.219999   \n",
       "3    1.265001         0.0         0.0         31.615000   \n",
       "4    0.737501         0.0         0.0         31.770000   \n",
       "\n",
       "   Rolling_Quantile_25_3  Rolling_Quantile_75_3  Rolling_Median_5  \\\n",
       "0               0.000000                0.00000             0.000   \n",
       "1               0.000000                0.00000             0.000   \n",
       "2              30.862499               31.41750             0.000   \n",
       "3              31.417500               31.69250             0.000   \n",
       "4              31.692500               31.86375            31.615   \n",
       "\n",
       "   Rolling_Quantile_25_5  Rolling_Quantile_75_5  Rolling_Median_7  \\\n",
       "0               0.000000                   0.00               0.0   \n",
       "1               0.000000                   0.00               0.0   \n",
       "2               0.000000                   0.00               0.0   \n",
       "3               0.000000                   0.00               0.0   \n",
       "4              31.219999                  31.77               0.0   \n",
       "\n",
       "   Rolling_Quantile_25_7  Rolling_Quantile_75_7  Close_lag_3  Close_lag_7  \\\n",
       "0                    0.0                    0.0          NaN          NaN   \n",
       "1                    0.0                    0.0          NaN          NaN   \n",
       "2                    0.0                    0.0          NaN          NaN   \n",
       "3                    0.0                    0.0    30.504999          NaN   \n",
       "4                    0.0                    0.0    31.219999          NaN   \n",
       "\n",
       "   SMA_3_lag_3  SMA_3_lag_7  SMA_7_lag_3  SMA_7_lag_7  EMA_3_lag_3  \\\n",
       "0          NaN          NaN          NaN          NaN          NaN   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3    30.504999          NaN    30.504999          NaN    30.504999   \n",
       "4    30.862499          NaN    30.862499          NaN    30.862499   \n",
       "\n",
       "   EMA_3_lag_7  EMA_7_lag_3  EMA_7_lag_7  Volume_lag_3  Volume_lag_7  \\\n",
       "0          NaN          NaN          NaN           NaN           NaN   \n",
       "1          NaN          NaN          NaN           NaN           NaN   \n",
       "2          NaN          NaN          NaN           NaN           NaN   \n",
       "3          NaN    30.504999          NaN   248034000.0           NaN   \n",
       "4          NaN    30.683749          NaN   294247200.0           NaN   \n",
       "\n",
       "   RSI_lag_3  RSI_lag_7  RSI_3_lag_3  RSI_3_lag_7  RSI_7_lag_3  RSI_7_lag_7  \\\n",
       "0        NaN        NaN          NaN          NaN          NaN          NaN   \n",
       "1        NaN        NaN          NaN          NaN          NaN          NaN   \n",
       "2        NaN        NaN          NaN          NaN          NaN          NaN   \n",
       "3        NaN        NaN          NaN          NaN          NaN          NaN   \n",
       "4        NaN        NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Signal_Line_lag_3  Signal_Line_lag_7  MACD_lag_3  MACD_lag_7  VWAP_lag_3  \\\n",
       "0                NaN                NaN         NaN         NaN         NaN   \n",
       "1                NaN                NaN         NaN         NaN         NaN   \n",
       "2                NaN                NaN         NaN         NaN         NaN   \n",
       "3           0.000000                NaN    0.000000         NaN   30.504999   \n",
       "4           0.011407                NaN    0.057037         NaN   30.892965   \n",
       "\n",
       "   VWAP_lag_7   %K_lag_3  %K_lag_7   %D_lag_3  %D_lag_7    OBV_lag_3  \\\n",
       "0         NaN        NaN       NaN        NaN       NaN          NaN   \n",
       "1         NaN        NaN       NaN        NaN       NaN          NaN   \n",
       "2         NaN        NaN       NaN        NaN       NaN          NaN   \n",
       "3         NaN  93.467084       NaN  93.467084       NaN          0.0   \n",
       "4         NaN  99.159644       NaN  96.313364       NaN  294247200.0   \n",
       "\n",
       "   OBV_lag_7  Momentum_3_lag_3  Momentum_3_lag_7  Momentum_7_lag_3  \\\n",
       "0        NaN               NaN               NaN               NaN   \n",
       "1        NaN               NaN               NaN               NaN   \n",
       "2        NaN               NaN               NaN               NaN   \n",
       "3        NaN               0.0               NaN               0.0   \n",
       "4        NaN               0.0               NaN               0.0   \n",
       "\n",
       "   Momentum_7_lag_7  Std_Dev_3_lag_3  Std_Dev_3_lag_7  Std_Dev_7_lag_3  \\\n",
       "0               NaN              NaN              NaN              NaN   \n",
       "1               NaN              NaN              NaN              NaN   \n",
       "2               NaN              NaN              NaN              NaN   \n",
       "3               NaN              NaN              NaN              NaN   \n",
       "4               NaN         0.505581              NaN         0.505581   \n",
       "\n",
       "   Std_Dev_7_lag_7  Rolling_Median_3_lag_3  Rolling_Median_3_lag_7  \\\n",
       "0              NaN                     NaN                     NaN   \n",
       "1              NaN                     NaN                     NaN   \n",
       "2              NaN                     NaN                     NaN   \n",
       "3              NaN                     0.0                     NaN   \n",
       "4              NaN                     0.0                     NaN   \n",
       "\n",
       "   Rolling_Median_7_lag_3  Rolling_Median_7_lag_7  \\\n",
       "0                     NaN                     NaN   \n",
       "1                     NaN                     NaN   \n",
       "2                     NaN                     NaN   \n",
       "3                     0.0                     NaN   \n",
       "4                     0.0                     NaN   \n",
       "\n",
       "   Rolling_Quantile_25_3_lag_3  Rolling_Quantile_25_3_lag_7  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          0.0                          NaN   \n",
       "4                          0.0                          NaN   \n",
       "\n",
       "   Rolling_Quantile_25_7_lag_3  Rolling_Quantile_25_7_lag_7  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          0.0                          NaN   \n",
       "4                          0.0                          NaN   \n",
       "\n",
       "   Rolling_Quantile_75_3_lag_3  Rolling_Quantile_75_3_lag_7  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          0.0                          NaN   \n",
       "4                          0.0                          NaN   \n",
       "\n",
       "   Rolling_Quantile_75_7_lag_3  Rolling_Quantile_75_7_lag_7  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          0.0                          NaN   \n",
       "4                          0.0                          NaN   \n",
       "\n",
       "   Daily_High_3day_avg_lag_3  Daily_High_3day_avg_lag_7  \\\n",
       "0                        NaN                        NaN   \n",
       "1                        NaN                        NaN   \n",
       "2                        NaN                        NaN   \n",
       "3                   30.53750                        NaN   \n",
       "4                   30.88375                        NaN   \n",
       "\n",
       "   Daily_High_7day_avg_lag_3  Daily_High_7day_avg_lag_7  \\\n",
       "0                        NaN                        NaN   \n",
       "1                        NaN                        NaN   \n",
       "2                        NaN                        NaN   \n",
       "3                   30.53750                        NaN   \n",
       "4                   30.88375                        NaN   \n",
       "\n",
       "   Daily_Low_3day_avg_lag_3  Daily_Low_3day_avg_lag_7  \\\n",
       "0                       NaN                       NaN   \n",
       "1                       NaN                       NaN   \n",
       "2                       NaN                       NaN   \n",
       "3                 30.040001                       NaN   \n",
       "4                 30.332500                       NaN   \n",
       "\n",
       "   Daily_Low_7day_avg_lag_3  Daily_Low_7day_avg_lag_7  Volume_3day_avg_lag_3  \\\n",
       "0                       NaN                       NaN                    NaN   \n",
       "1                       NaN                       NaN                    NaN   \n",
       "2                       NaN                       NaN                    NaN   \n",
       "3                 30.040001                       NaN            248034000.0   \n",
       "4                 30.332500                       NaN            271140600.0   \n",
       "\n",
       "   Volume_3day_avg_lag_7  Volume_7day_avg_lag_3  Volume_7day_avg_lag_7  \n",
       "0                    NaN                    NaN                    NaN  \n",
       "1                    NaN                    NaN                    NaN  \n",
       "2                    NaN                    NaN                    NaN  \n",
       "3                    NaN            248034000.0                    NaN  \n",
       "4                    NaN            271140600.0                    NaN  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_1_week.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1ba68b36-27c3-4050-8269-72b376f36ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as CSV file for easy access\n",
    "stock_data_1_week.to_csv('/Users/evancallaghan/flatiron_ds/phase_5/capstone_project/stock_10_yr_200_ta_1_week.csv', index=False)\n",
    "stock_data_1_month.to_csv('/Users/evancallaghan/flatiron_ds/phase_5/capstone_project/stock_10_yr_200_ta_1_month.csv', index=False)\n",
    "stock_data_3_month.to_csv('/Users/evancallaghan/flatiron_ds/phase_5/capstone_project/stock_10_yr_200_ta_3_month.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e7dcb6ac-cf4f-46aa-a706-cf433b219ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Daily_High</th>\n",
       "      <th>Daily_Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volume_3day_avg</th>\n",
       "      <th>Volume_5day_avg</th>\n",
       "      <th>Volume_7day_avg</th>\n",
       "      <th>Daily_High_3day_avg</th>\n",
       "      <th>Daily_High_5day_avg</th>\n",
       "      <th>Daily_High_7day_avg</th>\n",
       "      <th>Daily_Low_3day_avg</th>\n",
       "      <th>Daily_Low_5day_avg</th>\n",
       "      <th>Daily_Low_7day_avg</th>\n",
       "      <th>SMA_3</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_7</th>\n",
       "      <th>EMA_3</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>EMA_7</th>\n",
       "      <th>RSI</th>\n",
       "      <th>RSI_3</th>\n",
       "      <th>RSI_5</th>\n",
       "      <th>RSI_7</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "      <th>MACD_rolling_3</th>\n",
       "      <th>Signal_rolling_3</th>\n",
       "      <th>MACD_Histogram_rolling_3</th>\n",
       "      <th>MACD_rolling_5</th>\n",
       "      <th>Signal_rolling_5</th>\n",
       "      <th>MACD_Histogram_rolling_5</th>\n",
       "      <th>MACD_rolling_7</th>\n",
       "      <th>Signal_rolling_7</th>\n",
       "      <th>MACD_Histogram_rolling_7</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>%K_3</th>\n",
       "      <th>%D_3</th>\n",
       "      <th>%K_5</th>\n",
       "      <th>%D_5</th>\n",
       "      <th>%K_7</th>\n",
       "      <th>%D_7</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>VWAP_3</th>\n",
       "      <th>VWAP_5</th>\n",
       "      <th>VWAP_7</th>\n",
       "      <th>Std_Dev_3</th>\n",
       "      <th>Upper_Band_3</th>\n",
       "      <th>Lower_Band_3</th>\n",
       "      <th>Std_Dev_5</th>\n",
       "      <th>Upper_Band_5</th>\n",
       "      <th>Lower_Band_5</th>\n",
       "      <th>Std_Dev_7</th>\n",
       "      <th>Upper_Band_7</th>\n",
       "      <th>Lower_Band_7</th>\n",
       "      <th>ATR</th>\n",
       "      <th>ATR_3</th>\n",
       "      <th>ATR_5</th>\n",
       "      <th>ATR_7</th>\n",
       "      <th>5_day_Fib_23.6%</th>\n",
       "      <th>5_day_Fib_38.2%</th>\n",
       "      <th>5_day_Fib_50.0%</th>\n",
       "      <th>5_day_Fib_61.8%</th>\n",
       "      <th>5_day_Fib_78.6%</th>\n",
       "      <th>5_day_Fib_100.0%</th>\n",
       "      <th>5_day_Fib_161.8%</th>\n",
       "      <th>5_day_Fib_261.8%</th>\n",
       "      <th>5_day_Fib_423.6%</th>\n",
       "      <th>14_day_Fib_23.6%</th>\n",
       "      <th>14_day_Fib_38.2%</th>\n",
       "      <th>14_day_Fib_50.0%</th>\n",
       "      <th>14_day_Fib_61.8%</th>\n",
       "      <th>14_day_Fib_78.6%</th>\n",
       "      <th>14_day_Fib_100.0%</th>\n",
       "      <th>14_day_Fib_161.8%</th>\n",
       "      <th>14_day_Fib_261.8%</th>\n",
       "      <th>14_day_Fib_423.6%</th>\n",
       "      <th>30_day_Fib_23.6%</th>\n",
       "      <th>30_day_Fib_38.2%</th>\n",
       "      <th>30_day_Fib_50.0%</th>\n",
       "      <th>30_day_Fib_61.8%</th>\n",
       "      <th>30_day_Fib_78.6%</th>\n",
       "      <th>30_day_Fib_100.0%</th>\n",
       "      <th>30_day_Fib_161.8%</th>\n",
       "      <th>30_day_Fib_261.8%</th>\n",
       "      <th>30_day_Fib_423.6%</th>\n",
       "      <th>OBV</th>\n",
       "      <th>OBV_3day_avg</th>\n",
       "      <th>OBV_5day_avg</th>\n",
       "      <th>OBV_7day_avg</th>\n",
       "      <th>Momentum_3</th>\n",
       "      <th>Momentum_5</th>\n",
       "      <th>Momentum_7</th>\n",
       "      <th>Rolling_Median_3</th>\n",
       "      <th>Rolling_Quantile_25_3</th>\n",
       "      <th>Rolling_Quantile_75_3</th>\n",
       "      <th>Rolling_Median_5</th>\n",
       "      <th>Rolling_Quantile_25_5</th>\n",
       "      <th>Rolling_Quantile_75_5</th>\n",
       "      <th>Rolling_Median_7</th>\n",
       "      <th>Rolling_Quantile_25_7</th>\n",
       "      <th>Rolling_Quantile_75_7</th>\n",
       "      <th>Close_lag_3</th>\n",
       "      <th>Close_lag_7</th>\n",
       "      <th>SMA_3_lag_3</th>\n",
       "      <th>SMA_3_lag_7</th>\n",
       "      <th>SMA_7_lag_3</th>\n",
       "      <th>SMA_7_lag_7</th>\n",
       "      <th>EMA_3_lag_3</th>\n",
       "      <th>EMA_3_lag_7</th>\n",
       "      <th>EMA_7_lag_3</th>\n",
       "      <th>EMA_7_lag_7</th>\n",
       "      <th>Volume_lag_3</th>\n",
       "      <th>Volume_lag_7</th>\n",
       "      <th>RSI_lag_3</th>\n",
       "      <th>RSI_lag_7</th>\n",
       "      <th>RSI_3_lag_3</th>\n",
       "      <th>RSI_3_lag_7</th>\n",
       "      <th>RSI_7_lag_3</th>\n",
       "      <th>RSI_7_lag_7</th>\n",
       "      <th>Signal_Line_lag_3</th>\n",
       "      <th>Signal_Line_lag_7</th>\n",
       "      <th>MACD_lag_3</th>\n",
       "      <th>MACD_lag_7</th>\n",
       "      <th>VWAP_lag_3</th>\n",
       "      <th>VWAP_lag_7</th>\n",
       "      <th>%K_lag_3</th>\n",
       "      <th>%K_lag_7</th>\n",
       "      <th>%D_lag_3</th>\n",
       "      <th>%D_lag_7</th>\n",
       "      <th>OBV_lag_3</th>\n",
       "      <th>OBV_lag_7</th>\n",
       "      <th>Momentum_3_lag_3</th>\n",
       "      <th>Momentum_3_lag_7</th>\n",
       "      <th>Momentum_7_lag_3</th>\n",
       "      <th>Momentum_7_lag_7</th>\n",
       "      <th>Std_Dev_3_lag_3</th>\n",
       "      <th>Std_Dev_3_lag_7</th>\n",
       "      <th>Std_Dev_7_lag_3</th>\n",
       "      <th>Std_Dev_7_lag_7</th>\n",
       "      <th>Rolling_Median_3_lag_3</th>\n",
       "      <th>Rolling_Median_3_lag_7</th>\n",
       "      <th>Rolling_Median_7_lag_3</th>\n",
       "      <th>Rolling_Median_7_lag_7</th>\n",
       "      <th>Rolling_Quantile_25_3_lag_3</th>\n",
       "      <th>Rolling_Quantile_25_3_lag_7</th>\n",
       "      <th>Rolling_Quantile_25_7_lag_3</th>\n",
       "      <th>Rolling_Quantile_25_7_lag_7</th>\n",
       "      <th>Rolling_Quantile_75_3_lag_3</th>\n",
       "      <th>Rolling_Quantile_75_3_lag_7</th>\n",
       "      <th>Rolling_Quantile_75_7_lag_3</th>\n",
       "      <th>Rolling_Quantile_75_7_lag_7</th>\n",
       "      <th>Daily_High_3day_avg_lag_3</th>\n",
       "      <th>Daily_High_3day_avg_lag_7</th>\n",
       "      <th>Daily_High_7day_avg_lag_3</th>\n",
       "      <th>Daily_High_7day_avg_lag_7</th>\n",
       "      <th>Daily_Low_3day_avg_lag_3</th>\n",
       "      <th>Daily_Low_3day_avg_lag_7</th>\n",
       "      <th>Daily_Low_7day_avg_lag_3</th>\n",
       "      <th>Daily_Low_7day_avg_lag_7</th>\n",
       "      <th>Volume_3day_avg_lag_3</th>\n",
       "      <th>Volume_3day_avg_lag_7</th>\n",
       "      <th>Volume_7day_avg_lag_3</th>\n",
       "      <th>Volume_7day_avg_lag_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>248034000</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>2.480340e+08</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>0.497499</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>30.420091</td>\n",
       "      <td>30.347456</td>\n",
       "      <td>30.288751</td>\n",
       "      <td>30.230046</td>\n",
       "      <td>30.146466</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>28.430093</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>294247200</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>2.711406e+08</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.883750</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.743333</td>\n",
       "      <td>30.683749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.045630</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>30.892965</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>30.698982</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>0.611250</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>30.949160</td>\n",
       "      <td>30.775420</td>\n",
       "      <td>30.635000</td>\n",
       "      <td>30.494580</td>\n",
       "      <td>30.294661</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>26.189165</td>\n",
       "      <td>2.942472e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>297898000</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>2.800597e+08</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>31.212500</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.238750</td>\n",
       "      <td>31.033888</td>\n",
       "      <td>30.916562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.132584</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>86.065515</td>\n",
       "      <td>92.897414</td>\n",
       "      <td>31.148973</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>30.848979</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>0.562635</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>0.650002</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>0.624167</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>5.921452e+08</td>\n",
       "      <td>2.954641e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>31.41750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>217088800</td>\n",
       "      <td>2.697447e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>2.643170e+08</td>\n",
       "      <td>31.640000</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.364375</td>\n",
       "      <td>31.143333</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>31.535000</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.277500</td>\n",
       "      <td>31.504375</td>\n",
       "      <td>31.279259</td>\n",
       "      <td>31.129921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202627</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.133588</td>\n",
       "      <td>0.130750</td>\n",
       "      <td>0.038697</td>\n",
       "      <td>0.092053</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.098062</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>91.967846</td>\n",
       "      <td>92.397668</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>94.535498</td>\n",
       "      <td>93.253552</td>\n",
       "      <td>31.276489</td>\n",
       "      <td>31.106143</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>30.955857</td>\n",
       "      <td>0.283594</td>\n",
       "      <td>32.102187</td>\n",
       "      <td>30.967813</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>0.407499</td>\n",
       "      <td>0.594167</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>31.438121</td>\n",
       "      <td>31.170941</td>\n",
       "      <td>30.955001</td>\n",
       "      <td>30.739061</td>\n",
       "      <td>30.431621</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>24.118121</td>\n",
       "      <td>8.092340e+08</td>\n",
       "      <td>5.652088e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.265001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.417500</td>\n",
       "      <td>31.69250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248034000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.467084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.53750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.53750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248034000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248034000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>252609600</td>\n",
       "      <td>2.558655e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>2.619755e+08</td>\n",
       "      <td>31.970001</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.535500</td>\n",
       "      <td>31.511667</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.780834</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.413500</td>\n",
       "      <td>31.730938</td>\n",
       "      <td>31.505339</td>\n",
       "      <td>31.336816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.270153</td>\n",
       "      <td>0.109262</td>\n",
       "      <td>0.160890</td>\n",
       "      <td>0.201788</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.130473</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>0.132480</td>\n",
       "      <td>0.045070</td>\n",
       "      <td>0.087410</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>68.277903</td>\n",
       "      <td>82.103755</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>87.958682</td>\n",
       "      <td>89.519898</td>\n",
       "      <td>31.407822</td>\n",
       "      <td>31.277761</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>31.046250</td>\n",
       "      <td>0.171507</td>\n",
       "      <td>32.123848</td>\n",
       "      <td>31.437819</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>0.490002</td>\n",
       "      <td>0.515834</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>31.705521</td>\n",
       "      <td>31.387241</td>\n",
       "      <td>31.130001</td>\n",
       "      <td>30.872761</td>\n",
       "      <td>30.506521</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>22.985520</td>\n",
       "      <td>1.061844e+09</td>\n",
       "      <td>8.210743e+08</td>\n",
       "      <td>551494000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.737501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.692500</td>\n",
       "      <td>31.86375</td>\n",
       "      <td>31.615</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.683749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>294247200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.892965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.159644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.313364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>294247200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.505581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.88375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.88375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271140600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271140600.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol       Date      Close  Daily_High  Daily_Low     Volume  \\\n",
       "0   AAPL 2015-02-10  30.504999   30.537500  30.040001  248034000   \n",
       "1   AAPL 2015-02-11  31.219999   31.230000  30.625000  294247200   \n",
       "2   AAPL 2015-02-12  31.615000   31.870001  31.392500  297898000   \n",
       "3   AAPL 2015-02-13  31.770000   31.820000  31.412500  217088800   \n",
       "4   AAPL 2015-02-17  31.957500   32.220001  31.730000  252609600   \n",
       "\n",
       "   Volume_3day_avg  Volume_5day_avg  Volume_7day_avg  Daily_High_3day_avg  \\\n",
       "0     2.480340e+08     2.480340e+08     2.480340e+08            30.537500   \n",
       "1     2.711406e+08     2.711406e+08     2.711406e+08            30.883750   \n",
       "2     2.800597e+08     2.800597e+08     2.800597e+08            31.212500   \n",
       "3     2.697447e+08     2.643170e+08     2.643170e+08            31.640000   \n",
       "4     2.558655e+08     2.619755e+08     2.619755e+08            31.970001   \n",
       "\n",
       "   Daily_High_5day_avg  Daily_High_7day_avg  Daily_Low_3day_avg  \\\n",
       "0            30.537500            30.537500           30.040001   \n",
       "1            30.883750            30.883750           30.332500   \n",
       "2            31.212500            31.212500           30.685834   \n",
       "3            31.364375            31.364375           31.143333   \n",
       "4            31.535500            31.535500           31.511667   \n",
       "\n",
       "   Daily_Low_5day_avg  Daily_Low_7day_avg      SMA_3      SMA_5      SMA_7  \\\n",
       "0           30.040001           30.040001  30.504999  30.504999  30.504999   \n",
       "1           30.332500           30.332500  30.862499  30.862499  30.862499   \n",
       "2           30.685834           30.685834  31.113333  31.113333  31.113333   \n",
       "3           30.867500           30.867500  31.535000  31.277500  31.277500   \n",
       "4           31.040000           31.040000  31.780834  31.413500  31.413500   \n",
       "\n",
       "       EMA_3      EMA_5      EMA_7  RSI  RSI_3  RSI_5  RSI_7      MACD  \\\n",
       "0  30.504999  30.504999  30.504999  NaN    NaN    NaN    NaN  0.000000   \n",
       "1  30.862499  30.743333  30.683749  NaN    NaN    NaN    NaN  0.057037   \n",
       "2  31.238750  31.033888  30.916562  NaN  100.0    NaN    NaN  0.132584   \n",
       "3  31.504375  31.279259  31.129921  NaN  100.0    NaN    NaN  0.202627   \n",
       "4  31.730938  31.505339  31.336816  NaN  100.0  100.0    NaN  0.270153   \n",
       "\n",
       "   Signal_Line  MACD_Histogram  MACD_rolling_3  Signal_rolling_3  \\\n",
       "0     0.000000        0.000000        0.000000          0.000000   \n",
       "1     0.011407        0.045630        0.028519          0.005704   \n",
       "2     0.035643        0.096941        0.063207          0.015683   \n",
       "3     0.069040        0.133588        0.130750          0.038697   \n",
       "4     0.109262        0.160890        0.201788          0.071315   \n",
       "\n",
       "   MACD_Histogram_rolling_3  MACD_rolling_5  Signal_rolling_5  \\\n",
       "0                  0.000000        0.000000          0.000000   \n",
       "1                  0.022815        0.028519          0.005704   \n",
       "2                  0.047524        0.063207          0.015683   \n",
       "3                  0.092053        0.098062          0.029022   \n",
       "4                  0.130473        0.132480          0.045070   \n",
       "\n",
       "   MACD_Histogram_rolling_5  MACD_rolling_7  Signal_rolling_7  \\\n",
       "0                  0.000000        0.000000          0.000000   \n",
       "1                  0.022815        0.028519          0.005704   \n",
       "2                  0.047524        0.063207          0.015683   \n",
       "3                  0.069040        0.098062          0.029022   \n",
       "4                  0.087410        0.132480          0.045070   \n",
       "\n",
       "   MACD_Histogram_rolling_7         %K         %D       %K_3       %D_3  \\\n",
       "0                  0.000000  93.467084  93.467084  93.467084  93.467084   \n",
       "1                  0.022815  99.159644  96.313364  99.159644  96.313364   \n",
       "2                  0.047524  86.065515  92.897414  86.065515  92.897414   \n",
       "3                  0.069040  94.535498  93.253552  91.967846  92.397668   \n",
       "4                  0.087410  87.958682  89.519898  68.277903  82.103755   \n",
       "\n",
       "        %K_5       %D_5       %K_7       %D_7       VWAP     VWAP_3  \\\n",
       "0  93.467084  93.467084  93.467084  93.467084  30.504999  30.504999   \n",
       "1  99.159644  96.313364  99.159644  96.313364  30.892965  30.698982   \n",
       "2  86.065515  92.897414  86.065515  92.897414  31.148973  30.848979   \n",
       "3  94.535498  93.253552  94.535498  93.253552  31.276489  31.106143   \n",
       "4  87.958682  89.519898  87.958682  89.519898  31.407822  31.277761   \n",
       "\n",
       "      VWAP_5     VWAP_7  Std_Dev_3  Upper_Band_3  Lower_Band_3  Std_Dev_5  \\\n",
       "0  30.504999  30.504999        NaN           NaN           NaN        NaN   \n",
       "1  30.698982  30.698982   0.505581     31.873662     29.851336   0.505581   \n",
       "2  30.848979  30.848979   0.562635     32.238604     29.988062   0.562635   \n",
       "3  30.955857  30.955857   0.283594     32.102187     30.967813   0.564661   \n",
       "4  31.046250  31.046250   0.171507     32.123848     31.437819   0.575858   \n",
       "\n",
       "   Upper_Band_5  Lower_Band_5  Std_Dev_7  Upper_Band_7  Lower_Band_7  \\\n",
       "0           NaN           NaN        NaN           NaN           NaN   \n",
       "1     31.873662     29.851336   0.505581     31.873662     29.851336   \n",
       "2     32.238604     29.988062   0.562635     32.238604     29.988062   \n",
       "3     32.406822     30.148177   0.564661     32.406822     30.148177   \n",
       "4     32.565215     30.261785   0.575858     32.565215     30.261785   \n",
       "\n",
       "        ATR     ATR_3     ATR_5     ATR_7  5_day_Fib_23.6%  5_day_Fib_38.2%  \\\n",
       "0  0.497499  0.497499  0.497499  0.497499        30.420091        30.347456   \n",
       "1  0.725000  0.611250  0.611250  0.611250        30.949160        30.775420   \n",
       "2  0.650002  0.624167  0.624167  0.624167        31.438121        31.170941   \n",
       "3  0.407499  0.594167  0.570000  0.570000        31.438121        31.170941   \n",
       "4  0.490002  0.515834  0.554000  0.554000        31.705521        31.387241   \n",
       "\n",
       "   5_day_Fib_50.0%  5_day_Fib_61.8%  5_day_Fib_78.6%  5_day_Fib_100.0%  \\\n",
       "0        30.288751        30.230046        30.146466         30.040001   \n",
       "1        30.635000        30.494580        30.294661         30.040001   \n",
       "2        30.955001        30.739061        30.431621         30.040001   \n",
       "3        30.955001        30.739061        30.431621         30.040001   \n",
       "4        31.130001        30.872761        30.506521         30.040001   \n",
       "\n",
       "   5_day_Fib_161.8%  5_day_Fib_261.8%  5_day_Fib_423.6%  14_day_Fib_23.6%  \\\n",
       "0         29.732546         29.235047         28.430093         30.420091   \n",
       "1         29.304582         28.114583         26.189165         30.949160   \n",
       "2         28.909061         27.079061         24.118121         31.438121   \n",
       "3         28.909061         27.079061         24.118121         31.438121   \n",
       "4         28.692761         26.512760         22.985520         31.705521   \n",
       "\n",
       "   14_day_Fib_38.2%  14_day_Fib_50.0%  14_day_Fib_61.8%  14_day_Fib_78.6%  \\\n",
       "0         30.347456         30.288751         30.230046         30.146466   \n",
       "1         30.775420         30.635000         30.494580         30.294661   \n",
       "2         31.170941         30.955001         30.739061         30.431621   \n",
       "3         31.170941         30.955001         30.739061         30.431621   \n",
       "4         31.387241         31.130001         30.872761         30.506521   \n",
       "\n",
       "   14_day_Fib_100.0%  14_day_Fib_161.8%  14_day_Fib_261.8%  14_day_Fib_423.6%  \\\n",
       "0          30.040001          29.732546          29.235047          28.430093   \n",
       "1          30.040001          29.304582          28.114583          26.189165   \n",
       "2          30.040001          28.909061          27.079061          24.118121   \n",
       "3          30.040001          28.909061          27.079061          24.118121   \n",
       "4          30.040001          28.692761          26.512760          22.985520   \n",
       "\n",
       "   30_day_Fib_23.6%  30_day_Fib_38.2%  30_day_Fib_50.0%  30_day_Fib_61.8%  \\\n",
       "0         30.420091         30.347456         30.288751         30.230046   \n",
       "1         30.949160         30.775420         30.635000         30.494580   \n",
       "2         31.438121         31.170941         30.955001         30.739061   \n",
       "3         31.438121         31.170941         30.955001         30.739061   \n",
       "4         31.705521         31.387241         31.130001         30.872761   \n",
       "\n",
       "   30_day_Fib_78.6%  30_day_Fib_100.0%  30_day_Fib_161.8%  30_day_Fib_261.8%  \\\n",
       "0         30.146466          30.040001          29.732546          29.235047   \n",
       "1         30.294661          30.040001          29.304582          28.114583   \n",
       "2         30.431621          30.040001          28.909061          27.079061   \n",
       "3         30.431621          30.040001          28.909061          27.079061   \n",
       "4         30.506521          30.040001          28.692761          26.512760   \n",
       "\n",
       "   30_day_Fib_423.6%           OBV  OBV_3day_avg  OBV_5day_avg  OBV_7day_avg  \\\n",
       "0          28.430093  0.000000e+00  0.000000e+00           0.0           0.0   \n",
       "1          26.189165  2.942472e+08  0.000000e+00           0.0           0.0   \n",
       "2          24.118121  5.921452e+08  2.954641e+08           0.0           0.0   \n",
       "3          24.118121  8.092340e+08  5.652088e+08           0.0           0.0   \n",
       "4          22.985520  1.061844e+09  8.210743e+08   551494000.0           0.0   \n",
       "\n",
       "   Momentum_3  Momentum_5  Momentum_7  Rolling_Median_3  \\\n",
       "0    0.000000         0.0         0.0          0.000000   \n",
       "1    0.000000         0.0         0.0          0.000000   \n",
       "2    0.000000         0.0         0.0         31.219999   \n",
       "3    1.265001         0.0         0.0         31.615000   \n",
       "4    0.737501         0.0         0.0         31.770000   \n",
       "\n",
       "   Rolling_Quantile_25_3  Rolling_Quantile_75_3  Rolling_Median_5  \\\n",
       "0               0.000000                0.00000             0.000   \n",
       "1               0.000000                0.00000             0.000   \n",
       "2              30.862499               31.41750             0.000   \n",
       "3              31.417500               31.69250             0.000   \n",
       "4              31.692500               31.86375            31.615   \n",
       "\n",
       "   Rolling_Quantile_25_5  Rolling_Quantile_75_5  Rolling_Median_7  \\\n",
       "0               0.000000                   0.00               0.0   \n",
       "1               0.000000                   0.00               0.0   \n",
       "2               0.000000                   0.00               0.0   \n",
       "3               0.000000                   0.00               0.0   \n",
       "4              31.219999                  31.77               0.0   \n",
       "\n",
       "   Rolling_Quantile_25_7  Rolling_Quantile_75_7  Close_lag_3  Close_lag_7  \\\n",
       "0                    0.0                    0.0          NaN          NaN   \n",
       "1                    0.0                    0.0          NaN          NaN   \n",
       "2                    0.0                    0.0          NaN          NaN   \n",
       "3                    0.0                    0.0    30.504999          NaN   \n",
       "4                    0.0                    0.0    31.219999          NaN   \n",
       "\n",
       "   SMA_3_lag_3  SMA_3_lag_7  SMA_7_lag_3  SMA_7_lag_7  EMA_3_lag_3  \\\n",
       "0          NaN          NaN          NaN          NaN          NaN   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3    30.504999          NaN    30.504999          NaN    30.504999   \n",
       "4    30.862499          NaN    30.862499          NaN    30.862499   \n",
       "\n",
       "   EMA_3_lag_7  EMA_7_lag_3  EMA_7_lag_7  Volume_lag_3  Volume_lag_7  \\\n",
       "0          NaN          NaN          NaN           NaN           NaN   \n",
       "1          NaN          NaN          NaN           NaN           NaN   \n",
       "2          NaN          NaN          NaN           NaN           NaN   \n",
       "3          NaN    30.504999          NaN   248034000.0           NaN   \n",
       "4          NaN    30.683749          NaN   294247200.0           NaN   \n",
       "\n",
       "   RSI_lag_3  RSI_lag_7  RSI_3_lag_3  RSI_3_lag_7  RSI_7_lag_3  RSI_7_lag_7  \\\n",
       "0        NaN        NaN          NaN          NaN          NaN          NaN   \n",
       "1        NaN        NaN          NaN          NaN          NaN          NaN   \n",
       "2        NaN        NaN          NaN          NaN          NaN          NaN   \n",
       "3        NaN        NaN          NaN          NaN          NaN          NaN   \n",
       "4        NaN        NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Signal_Line_lag_3  Signal_Line_lag_7  MACD_lag_3  MACD_lag_7  VWAP_lag_3  \\\n",
       "0                NaN                NaN         NaN         NaN         NaN   \n",
       "1                NaN                NaN         NaN         NaN         NaN   \n",
       "2                NaN                NaN         NaN         NaN         NaN   \n",
       "3           0.000000                NaN    0.000000         NaN   30.504999   \n",
       "4           0.011407                NaN    0.057037         NaN   30.892965   \n",
       "\n",
       "   VWAP_lag_7   %K_lag_3  %K_lag_7   %D_lag_3  %D_lag_7    OBV_lag_3  \\\n",
       "0         NaN        NaN       NaN        NaN       NaN          NaN   \n",
       "1         NaN        NaN       NaN        NaN       NaN          NaN   \n",
       "2         NaN        NaN       NaN        NaN       NaN          NaN   \n",
       "3         NaN  93.467084       NaN  93.467084       NaN          0.0   \n",
       "4         NaN  99.159644       NaN  96.313364       NaN  294247200.0   \n",
       "\n",
       "   OBV_lag_7  Momentum_3_lag_3  Momentum_3_lag_7  Momentum_7_lag_3  \\\n",
       "0        NaN               NaN               NaN               NaN   \n",
       "1        NaN               NaN               NaN               NaN   \n",
       "2        NaN               NaN               NaN               NaN   \n",
       "3        NaN               0.0               NaN               0.0   \n",
       "4        NaN               0.0               NaN               0.0   \n",
       "\n",
       "   Momentum_7_lag_7  Std_Dev_3_lag_3  Std_Dev_3_lag_7  Std_Dev_7_lag_3  \\\n",
       "0               NaN              NaN              NaN              NaN   \n",
       "1               NaN              NaN              NaN              NaN   \n",
       "2               NaN              NaN              NaN              NaN   \n",
       "3               NaN              NaN              NaN              NaN   \n",
       "4               NaN         0.505581              NaN         0.505581   \n",
       "\n",
       "   Std_Dev_7_lag_7  Rolling_Median_3_lag_3  Rolling_Median_3_lag_7  \\\n",
       "0              NaN                     NaN                     NaN   \n",
       "1              NaN                     NaN                     NaN   \n",
       "2              NaN                     NaN                     NaN   \n",
       "3              NaN                     0.0                     NaN   \n",
       "4              NaN                     0.0                     NaN   \n",
       "\n",
       "   Rolling_Median_7_lag_3  Rolling_Median_7_lag_7  \\\n",
       "0                     NaN                     NaN   \n",
       "1                     NaN                     NaN   \n",
       "2                     NaN                     NaN   \n",
       "3                     0.0                     NaN   \n",
       "4                     0.0                     NaN   \n",
       "\n",
       "   Rolling_Quantile_25_3_lag_3  Rolling_Quantile_25_3_lag_7  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          0.0                          NaN   \n",
       "4                          0.0                          NaN   \n",
       "\n",
       "   Rolling_Quantile_25_7_lag_3  Rolling_Quantile_25_7_lag_7  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          0.0                          NaN   \n",
       "4                          0.0                          NaN   \n",
       "\n",
       "   Rolling_Quantile_75_3_lag_3  Rolling_Quantile_75_3_lag_7  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          0.0                          NaN   \n",
       "4                          0.0                          NaN   \n",
       "\n",
       "   Rolling_Quantile_75_7_lag_3  Rolling_Quantile_75_7_lag_7  \\\n",
       "0                          NaN                          NaN   \n",
       "1                          NaN                          NaN   \n",
       "2                          NaN                          NaN   \n",
       "3                          0.0                          NaN   \n",
       "4                          0.0                          NaN   \n",
       "\n",
       "   Daily_High_3day_avg_lag_3  Daily_High_3day_avg_lag_7  \\\n",
       "0                        NaN                        NaN   \n",
       "1                        NaN                        NaN   \n",
       "2                        NaN                        NaN   \n",
       "3                   30.53750                        NaN   \n",
       "4                   30.88375                        NaN   \n",
       "\n",
       "   Daily_High_7day_avg_lag_3  Daily_High_7day_avg_lag_7  \\\n",
       "0                        NaN                        NaN   \n",
       "1                        NaN                        NaN   \n",
       "2                        NaN                        NaN   \n",
       "3                   30.53750                        NaN   \n",
       "4                   30.88375                        NaN   \n",
       "\n",
       "   Daily_Low_3day_avg_lag_3  Daily_Low_3day_avg_lag_7  \\\n",
       "0                       NaN                       NaN   \n",
       "1                       NaN                       NaN   \n",
       "2                       NaN                       NaN   \n",
       "3                 30.040001                       NaN   \n",
       "4                 30.332500                       NaN   \n",
       "\n",
       "   Daily_Low_7day_avg_lag_3  Daily_Low_7day_avg_lag_7  Volume_3day_avg_lag_3  \\\n",
       "0                       NaN                       NaN                    NaN   \n",
       "1                       NaN                       NaN                    NaN   \n",
       "2                       NaN                       NaN                    NaN   \n",
       "3                 30.040001                       NaN            248034000.0   \n",
       "4                 30.332500                       NaN            271140600.0   \n",
       "\n",
       "   Volume_3day_avg_lag_7  Volume_7day_avg_lag_3  Volume_7day_avg_lag_7  \n",
       "0                    NaN                    NaN                    NaN  \n",
       "1                    NaN                    NaN                    NaN  \n",
       "2                    NaN                    NaN                    NaN  \n",
       "3                    NaN            248034000.0                    NaN  \n",
       "4                    NaN            271140600.0                    NaN  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_1_week.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "087e3735-0369-486e-8f4d-3394e79d96f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (385035, 168)\n",
      "Testing data shape: (101109, 168)\n",
      "X_train_1_week_baseline shape: (385035, 164), y_train_1_week_baseline shape: (385035,)\n",
      "X_test_1_week_baseline shape: (101109, 164), y_test_1_week_baseline shape: (101109,)\n",
      "Mean Squared Error on unseen data (post-February 17, 2024): 12898.594739541757\n"
     ]
    }
   ],
   "source": [
    "# BASELINE MODEL\n",
    "\n",
    "# i think this one would actually be the baseline, as i can separate the dates and test only\n",
    "# after feb 10 which is what i want to do\n",
    "# it also contains scaled data, which was better\n",
    "# baseline model\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "stock_data_1_week = stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_baseline = stock_data_1_week[stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_baseline = stock_data_1_week[stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_baseline.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_baseline['Close_Target'] = df_stock_data_train_1_week_baseline.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_baseline['Close_Target'] = df_stock_data_test_1_week_baseline.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_baseline = df_stock_data_train_1_week_baseline.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_baseline = df_stock_data_test_1_week_baseline.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_baseline.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_baseline.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_baseline[numeric_cols_train] = df_stock_data_train_1_week_baseline[numeric_cols_train].fillna(df_stock_data_train_1_week_baseline[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_baseline[numeric_cols_test] = df_stock_data_test_1_week_baseline[numeric_cols_test].fillna(df_stock_data_test_1_week_baseline[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_baseline.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_baseline.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_baseline = df_stock_data_train_1_week_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_baseline = df_stock_data_train_1_week_baseline['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_baseline = df_stock_data_test_1_week_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_baseline = df_stock_data_test_1_week_baseline['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_baseline shape: {X_train_1_week_baseline.shape}, y_train_1_week_baseline shape: {y_train_1_week_baseline.shape}\")\n",
    "print(f\"X_test_1_week_baseline shape: {X_test_1_week_baseline.shape}, y_test_1_week_baseline shape: {y_test_1_week_baseline.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_baseline.shape[0] == 0 or X_test_1_week_baseline.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_week = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_week.fit(X_train_1_week_baseline, y_train_1_week_baseline)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_baseline = model_baseline_1_week.predict(X_test_1_week_baseline)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_week_baseline = mean_squared_error(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test_1_week_baseline}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0a63f8dd-0c9a-4c3b-8b80-e54c451605d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 12898.594739541757\n",
      "Mean Absolute Error on unseen data: 16.56519185539962\n",
      "Root Mean Squared Error on unseen data: 113.57198043329947\n",
      "R-squared on unseen data: 0.8777647895834017\n",
      "Median Absolute Error on unseen data: 2.6649398803710938\n",
      "Durbin-Watson Statistic on unseen data: 0.018538681343526795\n",
      "MAPE on unseen data: 3.44%\n",
      "SMA_3: 27.89%\n",
      "Daily_Low_7day_avg: 17.48%\n",
      "Daily_Low_5day_avg: 12.59%\n",
      "Lower_Band_3: 5.77%\n",
      "EMA_3: 5.57%\n",
      "Daily_Low: 5.13%\n",
      "Upper_Band_5: 3.88%\n",
      "Lower_Band_7: 3.60%\n",
      "Upper_Band_3: 3.46%\n",
      "Daily_High: 3.03%\n",
      "Daily_Low_3day_avg: 2.15%\n",
      "30_day_Fib_100.0%: 1.52%\n",
      "5_day_Fib_261.8%: 1.26%\n",
      "5_day_Fib_161.8%: 1.17%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_week_baseline` are your predictions for the test data and `y_test_1_week_baseline` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_baseline = mean_squared_error(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "mae_1_week_baseline = mean_absolute_error(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "rmse_1_week_baseline = np.sqrt(mse_1_week_baseline)  # Root Mean Squared Error\n",
    "r2_1_week_baseline = r2_score(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_baseline}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_baseline}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_baseline}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_baseline}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_week_baseline = median_absolute_error(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_week_baseline}')\n",
    "\n",
    "dw_stat_1_week_baseline = durbin_watson(y_test_1_week_baseline - y_pred_1_week_baseline)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_baseline}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_baseline = np.mean(np.abs((y_test_1_week_baseline - y_pred_1_week_baseline) / y_test_1_week_baseline)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_baseline:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_baseline = dict(zip(X_train_1_week_baseline.columns, model_baseline_1_week.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_baseline = sorted(feature_importance_1_week_baseline.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_baseline:\n",
    "    if int(importance*100) >= 1:\n",
    "        print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "799369d9-7383-4605-accf-17a8facc976d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with more than 1% contribution:\n",
      "SMA_3: 27.89%\n",
      "Daily_Low_7day_avg: 17.48%\n",
      "Daily_Low_5day_avg: 12.59%\n",
      "Lower_Band_3: 5.77%\n",
      "EMA_3: 5.57%\n",
      "Daily_Low: 5.13%\n",
      "Upper_Band_5: 3.88%\n",
      "Lower_Band_7: 3.60%\n",
      "Upper_Band_3: 3.46%\n",
      "Daily_High: 3.03%\n",
      "Daily_Low_3day_avg: 2.15%\n",
      "30_day_Fib_100.0%: 1.52%\n",
      "5_day_Fib_261.8%: 1.26%\n",
      "5_day_Fib_161.8%: 1.17%\n",
      "List of important features:\n",
      "['SMA_3', 'Daily_Low_7day_avg', 'Daily_Low_5day_avg', 'Lower_Band_3', 'EMA_3', 'Daily_Low', 'Upper_Band_5', 'Lower_Band_7', 'Upper_Band_3', 'Daily_High', 'Daily_Low_3day_avg', '30_day_Fib_100.0%', '5_day_Fib_261.8%', '5_day_Fib_161.8%']\n"
     ]
    }
   ],
   "source": [
    "# we're going to use the scaled data, so the model above will be\n",
    "# our baseline\n",
    "# next we're going to use the same model and use a new dataframe\n",
    "# with features from the baseline model that contributed more than 1%\n",
    "# first we need to get a list of important feautres from our baseline\n",
    "# model and create a new dataframe containing only those features\n",
    "# Get feature importance as a dictionary\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_baseline = dict(zip(X_train_1_week_baseline.columns, model_baseline_1_week.feature_importances_))\n",
    "\n",
    "# Filter features with importance greater than 1%\n",
    "important_features_1_week_baseline = {feature: importance for feature, importance in feature_importance_1_week_baseline.items() if importance > 0.01}\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_important_features_1_week_baseline = sorted(important_features_1_week_baseline.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the features names (keys) into a list\n",
    "important_feature_names_1_week_baseline = [feature for feature, importance in sorted_important_features_1_week_baseline]\n",
    "\n",
    "# Print the sorted important features (optional)\n",
    "print(\"Features with more than 1% contribution:\")\n",
    "for feature in sorted_important_features_1_week_baseline:\n",
    "    print(f\"{feature[0]}: {feature[1] * 100:.2f}%\")\n",
    "\n",
    "# The list of important features that you can use to create a new dataframe\n",
    "print(\"List of important features:\")\n",
    "print(important_feature_names_1_week_baseline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c9f59012-a2f5-443c-8334-0e07686cdee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Daily_High</th>\n",
       "      <th>Daily_Low</th>\n",
       "      <th>Daily_Low_3day_avg</th>\n",
       "      <th>Daily_Low_5day_avg</th>\n",
       "      <th>Daily_Low_7day_avg</th>\n",
       "      <th>SMA_3</th>\n",
       "      <th>EMA_3</th>\n",
       "      <th>Upper_Band_3</th>\n",
       "      <th>Lower_Band_3</th>\n",
       "      <th>Upper_Band_5</th>\n",
       "      <th>Lower_Band_7</th>\n",
       "      <th>5_day_Fib_161.8%</th>\n",
       "      <th>5_day_Fib_261.8%</th>\n",
       "      <th>30_day_Fib_100.0%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.537500</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.040001</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>30.504999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.732546</td>\n",
       "      <td>29.235047</td>\n",
       "      <td>30.040001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>31.219999</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.625000</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.332500</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>30.862499</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>31.873662</td>\n",
       "      <td>29.851336</td>\n",
       "      <td>29.304582</td>\n",
       "      <td>28.114583</td>\n",
       "      <td>30.040001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>31.615000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.392500</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>30.685834</td>\n",
       "      <td>31.113333</td>\n",
       "      <td>31.238750</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>32.238604</td>\n",
       "      <td>29.988062</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>30.040001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>31.820000</td>\n",
       "      <td>31.412500</td>\n",
       "      <td>31.143333</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>30.867500</td>\n",
       "      <td>31.535000</td>\n",
       "      <td>31.504375</td>\n",
       "      <td>32.102187</td>\n",
       "      <td>30.967813</td>\n",
       "      <td>32.406822</td>\n",
       "      <td>30.148177</td>\n",
       "      <td>28.909061</td>\n",
       "      <td>27.079061</td>\n",
       "      <td>30.040001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>31.957500</td>\n",
       "      <td>32.220001</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.511667</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.040000</td>\n",
       "      <td>31.780834</td>\n",
       "      <td>31.730938</td>\n",
       "      <td>32.123848</td>\n",
       "      <td>31.437819</td>\n",
       "      <td>32.565215</td>\n",
       "      <td>30.261785</td>\n",
       "      <td>28.692761</td>\n",
       "      <td>26.512760</td>\n",
       "      <td>30.040001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332597</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2025-02-10</td>\n",
       "      <td>171.910004</td>\n",
       "      <td>172.539993</td>\n",
       "      <td>170.520004</td>\n",
       "      <td>171.816666</td>\n",
       "      <td>171.529999</td>\n",
       "      <td>170.944284</td>\n",
       "      <td>172.486664</td>\n",
       "      <td>172.301034</td>\n",
       "      <td>175.356110</td>\n",
       "      <td>169.617217</td>\n",
       "      <td>176.621746</td>\n",
       "      <td>169.231414</td>\n",
       "      <td>165.604785</td>\n",
       "      <td>158.314791</td>\n",
       "      <td>160.589996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332598</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2025-02-11</td>\n",
       "      <td>174.289993</td>\n",
       "      <td>174.839996</td>\n",
       "      <td>170.710007</td>\n",
       "      <td>170.720001</td>\n",
       "      <td>171.650000</td>\n",
       "      <td>170.948571</td>\n",
       "      <td>172.543330</td>\n",
       "      <td>173.295513</td>\n",
       "      <td>175.606483</td>\n",
       "      <td>169.480176</td>\n",
       "      <td>177.023645</td>\n",
       "      <td>169.886741</td>\n",
       "      <td>166.268171</td>\n",
       "      <td>159.388181</td>\n",
       "      <td>160.589996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332599</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2025-02-12</td>\n",
       "      <td>173.880005</td>\n",
       "      <td>174.119995</td>\n",
       "      <td>171.529999</td>\n",
       "      <td>170.920003</td>\n",
       "      <td>171.538000</td>\n",
       "      <td>171.412857</td>\n",
       "      <td>173.360001</td>\n",
       "      <td>173.587759</td>\n",
       "      <td>175.904714</td>\n",
       "      <td>170.815288</td>\n",
       "      <td>175.821646</td>\n",
       "      <td>170.293576</td>\n",
       "      <td>166.268171</td>\n",
       "      <td>159.388181</td>\n",
       "      <td>160.589996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332600</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>164.929993</td>\n",
       "      <td>166.110001</td>\n",
       "      <td>154.380005</td>\n",
       "      <td>165.540003</td>\n",
       "      <td>167.614001</td>\n",
       "      <td>169.165715</td>\n",
       "      <td>171.033330</td>\n",
       "      <td>169.258876</td>\n",
       "      <td>181.612568</td>\n",
       "      <td>160.454092</td>\n",
       "      <td>178.808753</td>\n",
       "      <td>165.184175</td>\n",
       "      <td>141.327848</td>\n",
       "      <td>120.207853</td>\n",
       "      <td>154.380005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332601</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2025-02-14</td>\n",
       "      <td>157.520004</td>\n",
       "      <td>164.539993</td>\n",
       "      <td>156.339996</td>\n",
       "      <td>160.750000</td>\n",
       "      <td>164.696002</td>\n",
       "      <td>166.915715</td>\n",
       "      <td>165.443334</td>\n",
       "      <td>163.389440</td>\n",
       "      <td>181.827478</td>\n",
       "      <td>149.059190</td>\n",
       "      <td>182.907447</td>\n",
       "      <td>157.154463</td>\n",
       "      <td>141.735730</td>\n",
       "      <td>121.275739</td>\n",
       "      <td>154.380005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>489134 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Symbol       Date       Close  Daily_High   Daily_Low  \\\n",
       "0        AAPL 2015-02-10   30.504999   30.537500   30.040001   \n",
       "1        AAPL 2015-02-11   31.219999   31.230000   30.625000   \n",
       "2        AAPL 2015-02-12   31.615000   31.870001   31.392500   \n",
       "3        AAPL 2015-02-13   31.770000   31.820000   31.412500   \n",
       "4        AAPL 2015-02-17   31.957500   32.220001   31.730000   \n",
       "...       ...        ...         ...         ...         ...   \n",
       "332597    ZTS 2025-02-10  171.910004  172.539993  170.520004   \n",
       "332598    ZTS 2025-02-11  174.289993  174.839996  170.710007   \n",
       "332599    ZTS 2025-02-12  173.880005  174.119995  171.529999   \n",
       "332600    ZTS 2025-02-13  164.929993  166.110001  154.380005   \n",
       "332601    ZTS 2025-02-14  157.520004  164.539993  156.339996   \n",
       "\n",
       "        Daily_Low_3day_avg  Daily_Low_5day_avg  Daily_Low_7day_avg  \\\n",
       "0                30.040001           30.040001           30.040001   \n",
       "1                30.332500           30.332500           30.332500   \n",
       "2                30.685834           30.685834           30.685834   \n",
       "3                31.143333           30.867500           30.867500   \n",
       "4                31.511667           31.040000           31.040000   \n",
       "...                    ...                 ...                 ...   \n",
       "332597          171.816666          171.529999          170.944284   \n",
       "332598          170.720001          171.650000          170.948571   \n",
       "332599          170.920003          171.538000          171.412857   \n",
       "332600          165.540003          167.614001          169.165715   \n",
       "332601          160.750000          164.696002          166.915715   \n",
       "\n",
       "             SMA_3       EMA_3  Upper_Band_3  Lower_Band_3  Upper_Band_5  \\\n",
       "0        30.504999   30.504999           NaN           NaN           NaN   \n",
       "1        30.862499   30.862499     31.873662     29.851336     31.873662   \n",
       "2        31.113333   31.238750     32.238604     29.988062     32.238604   \n",
       "3        31.535000   31.504375     32.102187     30.967813     32.406822   \n",
       "4        31.780834   31.730938     32.123848     31.437819     32.565215   \n",
       "...            ...         ...           ...           ...           ...   \n",
       "332597  172.486664  172.301034    175.356110    169.617217    176.621746   \n",
       "332598  172.543330  173.295513    175.606483    169.480176    177.023645   \n",
       "332599  173.360001  173.587759    175.904714    170.815288    175.821646   \n",
       "332600  171.033330  169.258876    181.612568    160.454092    178.808753   \n",
       "332601  165.443334  163.389440    181.827478    149.059190    182.907447   \n",
       "\n",
       "        Lower_Band_7  5_day_Fib_161.8%  5_day_Fib_261.8%  30_day_Fib_100.0%  \n",
       "0                NaN         29.732546         29.235047          30.040001  \n",
       "1          29.851336         29.304582         28.114583          30.040001  \n",
       "2          29.988062         28.909061         27.079061          30.040001  \n",
       "3          30.148177         28.909061         27.079061          30.040001  \n",
       "4          30.261785         28.692761         26.512760          30.040001  \n",
       "...              ...               ...               ...                ...  \n",
       "332597    169.231414        165.604785        158.314791         160.589996  \n",
       "332598    169.886741        166.268171        159.388181         160.589996  \n",
       "332599    170.293576        166.268171        159.388181         160.589996  \n",
       "332600    165.184175        141.327848        120.207853         154.380005  \n",
       "332601    157.154463        141.735730        121.275739         154.380005  \n",
       "\n",
       "[489134 rows x 17 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns_list = stock_data_1_week.columns\n",
    "imp_feat_columns_list = important_feature_names_1_week_baseline\n",
    "important_features= [\n",
    "    col for col in columns_list\n",
    "    if col in imp_feat_columns_list or col in {'Symbol', 'Date', 'Close'}\n",
    "]\n",
    "\n",
    "stock_data_1_week_if = stock_data_1_week[important_features]\n",
    "stock_data_1_week_if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "41f98683-2901-40b2-96a2-b1edb240cad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in reversed(important_features):\n",
    "    important_features.remove(col)\n",
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "23777ec1-a408-42e0-a454-2d82332eb021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SMA_3',\n",
       " 'Daily_Low_7day_avg',\n",
       " 'Daily_Low_5day_avg',\n",
       " 'Lower_Band_3',\n",
       " 'EMA_3',\n",
       " 'Daily_Low',\n",
       " 'Upper_Band_5',\n",
       " 'Lower_Band_7',\n",
       " 'Upper_Band_3',\n",
       " 'Daily_High',\n",
       " 'Daily_Low_3day_avg',\n",
       " '30_day_Fib_100.0%',\n",
       " '5_day_Fib_261.8%',\n",
       " '5_day_Fib_161.8%']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_feature_names_1_week_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0704f-e9a2-48ab-a5eb-b079ad7a559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = []\n",
    "for col in stock_data_1_week_if.columns.tolist():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872d7c2-824b-4f00-b994-282282e59b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to use the scaled data, so the model above will be\n",
    "# our baseline\n",
    "# next we're going to use the same model and use a new dataframe\n",
    "# with features from the baseline model that contributed more than 1%\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_important_feat_1_week = df_important_feat_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_if.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_if['Close_Target'] = df_stock_data_train_1_week_if.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_if['Close_Target'] = df_stock_data_test_1_week_if.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_if = df_stock_data_train_1_week_if.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_if = df_stock_data_test_1_week_if.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_if.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_if.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_if[numeric_cols_train] = df_stock_data_train_1_week_if[numeric_cols_train].fillna(df_stock_data_train_1_week_if[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_if[numeric_cols_test] = df_stock_data_test_1_week_if[numeric_cols_test].fillna(df_stock_data_test_1_week_if[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_if.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_if.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_if = df_stock_data_train_1_week_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_if = df_stock_data_train_1_week_if['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_if = df_stock_data_test_1_week_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_if = df_stock_data_test_1_week_if['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_if shape: {X_train_1_week_if.shape}, y_train_1_week_if shape: {y_train_1_week_if.shape}\")\n",
    "print(f\"X_test_1_week_if shape: {X_test_1_week_if.shape}, y_test_1_week_if shape: {y_test_1_week_if.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_if.shape[0] == 0 or X_test_1_week_if.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_update1_1_week = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_update1_1_week.fit(X_train_1_week_if, y_train_1_week_if)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_if = model_update1_1_week.predict(X_test_1_week_if)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_week_if = mean_squared_error(y_test_1_week_if, y_pred_1_week_if)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test_1_week_if}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0add6b-88df-4251-bc1b-e95e9785e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_week_if` are your predictions for the test data and `y_test_1_week_if` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_if = mean_squared_error(y_test_1_week_if, y_pred_1_week_if)\n",
    "mae_1_week_if = mean_absolute_error(y_test_1_week_if, y_pred_1_week_if)\n",
    "rmse_1_week_if = np.sqrt(mse_1_week_if)  # Root Mean Squared Error\n",
    "r2_1_week_if = r2_score(y_test_1_week_if, y_pred_1_week_if)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_if}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_if}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_if}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_if}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_week_if = median_absolute_error(y_test_1_week_if, y_pred_1_week_if)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_week_if}')\n",
    "\n",
    "dw_stat_1_week_if = durbin_watson(y_test_1_week_if - y_pred_1_week_if)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_if}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_if = np.mean(np.abs((y_test_1_week_if - y_pred_1_week_if) / y_test_1_week_if)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_if:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_if = dict(zip(X_train_1_week_if.columns, model_update1_1_week.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_if = sorted(feature_importance_1_week_if.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_if:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8deae-7fa7-4ac8-8ac8-600bbcaaa2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing important features led to the degredation of all metrics\n",
    "# we're going to use all metrics again and try adjusting a few of the hyper parameters\n",
    "#\n",
    "# i think this one would actually be the baseline, as i can separate the dates and test only\n",
    "# after feb 10 which is what i want to do\n",
    "# it also contains scaled data, which was better\n",
    "# learning_rate = 0.01\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_lr_01.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_lr_01['Close_Target'] = df_stock_data_train_1_week_lr_01.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_lr_01['Close_Target'] = df_stock_data_test_1_week_lr_01.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_lr_01 = df_stock_data_train_1_week_lr_01.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_lr_01 = df_stock_data_test_1_week_lr_01.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_lr_01.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_lr_01.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_lr_01[numeric_cols_train] = df_stock_data_train_1_week_lr_01[numeric_cols_train].fillna(df_stock_data_train_1_week_lr_01[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_lr_01[numeric_cols_test] = df_stock_data_test_1_week_lr_01[numeric_cols_test].fillna(df_stock_data_test_1_week_lr_01[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_lr_01.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_lr_01.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_lr_01 = df_stock_data_train_1_week_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_lr_01 = df_stock_data_train_1_week_lr_01['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_lr_01 = df_stock_data_test_1_week_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_lr_01 = df_stock_data_test_1_week_lr_01['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_lr_01 shape: {X_train_1_week_lr_01.shape}, y_train_1_week_lr_01 shape: {y_train_1_week_lr_01.shape}\")\n",
    "print(f\"X_test_1_week_lr_01 shape: {X_test_1_week_lr_01.shape}, y_test_1_week_lr_01 shape: {y_test_1_week_lr_01.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_lr_01.shape[0] == 0 or X_test_1_week_lr_01.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_week_LR_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_week_LR_01.fit(X_train_1_week_lr_01, y_train_1_week_lr_01)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_lr_01 = model_baseline_1_week_LR_01.predict(X_test_1_week_lr_01)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_week_lr_01 = mean_squared_error(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test_1_week_lr_01}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f7d07-9367-43ff-ab48-b4f44670b32a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_1_week_lr_01` are your predictions for the test data and `y_test_1_week_lr_01` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_lr_01 = mean_squared_error(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "mae_1_week_lr_01 = mean_absolute_error(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "rmse_1_week_lr_01 = np.sqrt(mse_1_week_lr_01)  # Root Mean Squared Error\n",
    "r2_1_week_lr_01 = r2_score(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_lr_01}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_lr_01}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_lr_01}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_lr_01}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_week_lr_01 = median_absolute_error(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "print(f'Median Absolute Error on unseen data: {medae_week_lr_01}')\n",
    "\n",
    "dw_stat_1_week_lr_01 = durbin_watson(y_test_1_week_lr_01 - y_pred_1_week_lr_01)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_lr_01}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_lr_01 = np.mean(np.abs((y_test_1_week_lr_01 - y_pred_1_week_lr_01) / y_test_1_week_lr_01)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_lr_01:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_lr_01 = dict(zip(X_train_1_week_lr_01.columns, model_baseline_1_week_LR_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_lr_01 = sorted(feature_importance_1_week_lr_01.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_lr_01:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53d222-e712-4709-82eb-3999a328899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing important features led to the degredation of all metrics\n",
    "# we're going to use all metrics again and try adjusting a few of the hyper parameters\n",
    "#\n",
    "# i think this one would actually be the baseline, as i can separate the dates and test only\n",
    "# after feb 10 which is what i want to do\n",
    "# it also contains scaled data, which was better\n",
    "# learning_rate = 0.1\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_lr_1 = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_lr_1 = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_lr_1.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_lr_1['Close_Target'] = df_stock_data_train_1_week_lr_1.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_lr_1['Close_Target'] = df_stock_data_test_1_week_lr_1.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_lr_1 = df_stock_data_train_1_week_lr_1.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_lr_1 = df_stock_data_test_1_week_lr_1.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_lr_1.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_lr_1.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_lr_1[numeric_cols_train] = df_stock_data_train_1_week_lr_1[numeric_cols_train].fillna(df_stock_data_train_1_week_lr_1[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_lr_1[numeric_cols_test] = df_stock_data_test_1_week_lr_1[numeric_cols_test].fillna(df_stock_data_test_1_week_lr_1[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_lr_1.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_lr_1.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_lr_1 = df_stock_data_train_1_week_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_lr_1 = df_stock_data_train_1_week_lr_1['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_lr_1 = df_stock_data_test_1_week_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_lr_1 = df_stock_data_test_1_week_lr_1['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_lr_1 shape: {X_train_1_week_lr_1.shape}, y_train_1_week_lr_1 shape: {y_train_1_week_lr_1.shape}\")\n",
    "print(f\"X_test_1_week_lr_1 shape: {X_test_1_week_lr_1.shape}, y_test_1_week_lr_1 shape: {y_test_1_week_lr_1.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_lr_1.shape[0] == 0 or X_test_1_week_lr_1.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_week_LR_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_week_LR_1.fit(X_train_1_week_lr_1, y_train_1_week_lr_1)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_lr_1 = model_baseline_1_week_LR_1.predict(X_test_1_week_lr_1)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c41240-ce28-461a-8ca2-e17fa249276b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_week_lr_1` are your predictions for the test data and `y_test_1_week_lr_1` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_lr_1 = mean_squared_error(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "mae_1_week_lr_1 = mean_absolute_error(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "rmse_1_week_lr_1 = np.sqrt(mse_1_week_lr_1)  # Root Mean Squared Error\n",
    "r2_1_week_lr_1 = r2_score(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_lr_1}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_lr_1}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_lr_1}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_lr_1}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_week_lr_1 = median_absolute_error(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_week_lr_1}')\n",
    "\n",
    "dw_stat_1_week_lr_1 = durbin_watson(y_test_1_week_lr_1 - y_pred_1_week_lr_1)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_lr_1}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_lr_1 = np.mean(np.abs((y_test_1_week_lr_1 - y_pred_1_week_lr_1) / y_test_1_week_lr_1)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_lr_1:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_lr_1 = dict(zip(X_train_1_week_lr_1.columns, model_baseline_1_week_LR_1.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_lr_1 = sorted(feature_importance_1_week_lr_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_lr_1:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5bf201-dd97-426a-a0e9-ae5951cf55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate outcome: learning_rate=0.01 showed the best improvement\n",
    "# and had better metrics than the baseline, so we'll keep it and now tweak max_depth\n",
    "\n",
    "# max_depth = 3\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_md_3 = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_md_3.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_md_3['Close_Target'] = df_stock_data_train_1_week_md_3.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_md_3['Close_Target'] = df_stock_data_test_1_week_md_3.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_md_3 = df_stock_data_train_1_week_md_3.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_md_3 = df_stock_data_test_1_week_md_3.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_md_3.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_md_3.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_md_3[numeric_cols_train] = df_stock_data_train_1_week_md_3[numeric_cols_train].fillna(df_stock_data_train_1_week_md_3[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_md_3[numeric_cols_test] = df_stock_data_test_1_week_md_3[numeric_cols_test].fillna(df_stock_data_test_1_week_md_3[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_md_3.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_md_3.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_md_3 = df_stock_data_train_1_week_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_md_3 = df_stock_data_train_1_week_md_3['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_md_3 = df_stock_data_test_1_week_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_md_3 = df_stock_data_test_1_week_md_3['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_md_3 shape: {X_train_1_week_md_3.shape}, y_train_1_week_md_3 shape: {y_train_1_week_md_3.shape}\")\n",
    "print(f\"X_test_1_week_md_3 shape: {X_test_1_week_md_3.shape}, y_test_1_week_md_3 shape: {y_test_1_week_md_3.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_md_3.shape[0] == 0 or X_test_1_week_md_3.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_week_MD_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_week_MD_3.fit(X_train_1_week_md_3, y_train_1_week_md_3)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_md_3 = model_1_week_MD_3.predict(X_test_1_week_md_3)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_week_md_3 = mean_squared_error(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test_1_week_md_3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f1ff1-97d5-4986-840e-51abb90c8117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_week_md_3` are your predictions for the test data and `y_test_1_week_md_3` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_md_3 = mean_squared_error(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "mae_1_week_md_3 = mean_absolute_error(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "rmse_1_week_md_3 = np.sqrt(mse_1_week_md_3)  # Root Mean Squared Error\n",
    "r2_1_week_md_3 = r2_score(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_md_3}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_md_3}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_md_3}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_md_3}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_week_md_3 = median_absolute_error(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_week_md_3}')\n",
    "\n",
    "dw_stat_1_week_md_3 = durbin_watson(y_test_1_week_md_3 - y_pred_1_week_md_3)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_md_3}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_md_3 = np.mean(np.abs((y_test_1_week_md_3 - y_pred_1_week_md_3) / y_test_1_week_md_3)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_md_3:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_md_3 = dict(zip(X_train_1_week_md_3.columns, model_1_week_MD_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_md_3 = sorted(feature_importance_1_week_md_3.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_md_3:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e4ee4-0759-442d-b08c-f91e606d533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate outcome: learning_rate=0.01 showed the best improvement\n",
    "# and had better metrics than the baseline, so we'll keep it and now tweak max_depth\n",
    "\n",
    "# max_depth = 7\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_md_7 = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_md_7 = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_md_7.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_md_7['Close_Target'] = df_stock_data_train_1_week_md_7.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_md_7['Close_Target'] = df_stock_data_test_1_week_md_7.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_md_7 = df_stock_data_train_1_week_md_7.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_md_7 = df_stock_data_test_1_week_md_7.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_md_7.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_md_7.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_md_7[numeric_cols_train] = df_stock_data_train_1_week_md_7[numeric_cols_train].fillna(df_stock_data_train_1_week_md_7[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_md_7[numeric_cols_test] = df_stock_data_test_1_week_md_7[numeric_cols_test].fillna(df_stock_data_test_1_week_md_7[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_md_7.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_md_7.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_md_7 = df_stock_data_train_1_week_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_md_7 = df_stock_data_train_1_week_md_7['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_md_7 = df_stock_data_test_1_week_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_md_7 = df_stock_data_test_1_week_md_7['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_md_7 shape: {X_train_1_week_md_7.shape}, y_train_1_week_md_7 shape: {y_train_1_week_md_7.shape}\")\n",
    "print(f\"X_test_1_week_md_7 shape: {X_test_1_week_md_7.shape}, y_test_1_week_md_7 shape: {y_test_1_week_md_7.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_md_7.shape[0] == 0 or X_test_1_week_md_7.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_week_MD_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_week_MD_7.fit(X_train_1_week_md_7, y_train_1_week_md_7)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_md_7 = model_1_week_MD_7.predict(X_test_1_week_md_7)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_week_md_7 = mean_squared_error(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test_1_week_md_7}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef582a0-083b-41ec-931f-8d6721af35c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_week_md_7` are your predictions for the test data and `y_test_1_week_md_7` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_md_7 = mean_squared_error(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "mae_1_week_md_7 = mean_absolute_error(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "rmse_1_week_md_7 = np.sqrt(mse_1_week_md_7)  # Root Mean Squared Error\n",
    "r2_1_week_md_7 = r2_score(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_md_7}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_md_7}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_md_7}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_md_7}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_week_md_7 = median_absolute_error(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_week_md_7}')\n",
    "\n",
    "dw_stat_1_week_md_7 = durbin_watson(y_test_1_week_md_7 - y_pred_1_week_md_7)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_md_7}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_md_7 = np.mean(np.abs((y_test_1_week_md_7 - y_pred_1_week_md_7) / y_test_1_week_md_7)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_md_7:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_md_7 = dict(zip(X_train_1_week_md_7.columns, model_1_week_MD_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_md_7 = sorted(feature_importance_1_week_md_7.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_md_7:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23429a-fd45-457f-a901-ebb2c578277f",
   "metadata": {},
   "source": [
    "best model: learning_rate = 0.01 and max_depth = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b727455-bcda-46ef-98ab-fcafa2f7d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features_1_week_md_3[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features_1_week_md_3[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 6)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=16, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=16, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 1 Week Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe46361-f5ba-45a1-9d2d-63f24006a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will modify our feature set to add bigger lagging indicators.\n",
    "# Create a new dataframe called 'df_stock_data_1_month' as a copy of 'df_stocks_price_ta'\n",
    "df_stock_data_1_month = df_stocks_price_ta.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e5901-1483-4177-b075-cebdf459bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to create lags for (focusing on short-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_5', 'EMA_5', 'Volume', 'EMA_12_MACD', 'SMA_20', 'EMA_20']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 12, 15, 20, 30, 60, 90, 180, 360] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7, 10, 12, 15, 20]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_stock_data_1_month[f'{col}_lag_{lag}'] = df_stock_data_1_month[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_stock_data_1_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da38ef-e130-4b2d-bc33-4e937217952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're going to move onto our next model: 1 month prediction\n",
    "# we'll start at our baseline model and then do the same as we just did\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_baseline = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_baseline = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_baseline.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_baseline['Close_Target'] = df_stock_data_train_1_month_baseline.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_baseline['Close_Target'] = df_stock_data_test_1_month_baseline.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_baseline = df_stock_data_train_1_month_baseline.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_baseline = df_stock_data_test_1_month_baseline.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_baseline[numeric_cols_train] = df_stock_data_train_1_month_baseline[numeric_cols_train].fillna(df_stock_data_train_1_month_baseline[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_baseline[numeric_cols_test] = df_stock_data_test_1_month_baseline[numeric_cols_test].fillna(df_stock_data_test_1_month_baseline[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_baseline.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_baseline.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_baseline = df_stock_data_train_1_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_baseline = df_stock_data_train_1_month_baseline['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_baseline = df_stock_data_test_1_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_baseline = df_stock_data_test_1_month_baseline['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_baseline shape: {X_train_1_month_baseline.shape}, y_train_1_month_baseline shape: {y_train_1_month_baseline.shape}\")\n",
    "print(f\"X_test_1_month_baseline shape: {X_test_1_month_baseline.shape}, y_test_1_month_baseline shape: {y_test_1_month_baseline.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_baseline.shape[0] == 0 or X_test_1_month_baseline.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_month.fit(X_train_1_month_baseline, y_train_1_month_baseline)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_baseline = model_baseline_1_month.predict(X_test_1_month_baseline)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_baseline = mean_squared_error(y_test_1_month_baseline, y_pred_1_month_baseline)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_baseline}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb8f43-31bf-4b04-9166-b2223d530da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_1_month_baseline` are your predictions for the test data and `y_test_1_month_baseline` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_month_baseline = mean_squared_error(y_test_1_month_baseline, y_pred_1_month_baseline)\n",
    "mae_1_month_baseline = mean_absolute_error(y_test_1_month_baseline, y_pred_1_month_baseline)\n",
    "rmse_1_month_baseline = np.sqrt(mse_1_month_baseline)  # Root Mean Squared Error\n",
    "r2_1_month_baseline = r2_score(y_test_1_month_baseline, y_pred_1_month_baseline)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_month_baseline}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_baseline}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_month_baseline}')\n",
    "print(f'R-squared on unseen data: {r2_1_month_baseline}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_baseline = median_absolute_error(y_test_1_month_baseline, y_pred_1_month_baseline)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_baseline}')\n",
    "\n",
    "dw_stat_1_month_baseline = durbin_watson(y_test_1_month_baseline - y_pred_1_month_baseline)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_baseline}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_baseline = np.mean(np.abs((y_test_1_month_baseline - y_pred_1_month_baseline) / y_test_1_month_baseline)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_baseline:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_month_baseline = dict(zip(X_train_1_month_baseline.columns, model_baseline_1_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_baseline = sorted(feature_importance_1_month_baseline.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_baseline:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf0fd0-3c2f-4376-b4a4-593709c8828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the baseline model (1-week prediction)\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_1_month.feature_importances_))\n",
    "\n",
    "# Filter features with importance greater than 1%\n",
    "important_features = {feature: importance for feature, importance in feature_importance.items() if importance > 0.01}\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_important_features = sorted(important_features.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the feature names (keys) into a list\n",
    "important_feature_names = [feature for feature, importance in sorted_important_features]\n",
    "\n",
    "# Print the sorted important features (optional)\n",
    "print(\"Features with more than 1% contribution:\")\n",
    "for feature in sorted_important_features:\n",
    "    print(f\"{feature[0]}: {feature[1] * 100:.2f}%\")\n",
    "\n",
    "# The list of important features that you can use to create a new dataframe\n",
    "print(\"List of important features:\")\n",
    "print(important_feature_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469fa610-eb03-442c-bbf2-bdbc978ded41",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = ['Symbol', 'Date', 'Close', 'Fib_30_High_Max', '30_day_Fib_23',\n",
    "                      'High', 'Low', 'Fib_30_Low_Min', 'Volume', 'EMA_5', '30_day_Fib_50',\n",
    "                      'Fib_5_Low_Min']\n",
    "df_important_feat_1_month = df_stock_data_1_month[important_features]\n",
    "df_important_feat_1_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ad4e4-9e00-4f0a-8232-ed5573072f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline 1 month prediction model with only features contributing over 1%\n",
    "# not as good as baseline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_important_feat_1_month = df_important_feat_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_if = df_important_feat_1_month[df_important_feat_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_if = df_important_feat_1_month[df_important_feat_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_if.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_if['Close_Target'] = df_stock_data_train_1_month_if.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_if['Close_Target'] = df_stock_data_test_1_month_if.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_if = df_stock_data_train_1_month_if.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_if = df_stock_data_test_1_month_if.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_if.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_if.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_if[numeric_cols_train] = df_stock_data_train_1_month_if[numeric_cols_train].fillna(df_stock_data_train_1_month_if[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_if[numeric_cols_test] = df_stock_data_test_1_month_if[numeric_cols_test].fillna(df_stock_data_test_1_month_if[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_if.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_if.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_if = df_stock_data_train_1_month_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_if = df_stock_data_train_1_month_if['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_if = df_stock_data_test_1_month_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_if = df_stock_data_test_1_month_if['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_if shape: {X_train_1_month_if.shape}, y_train_1_month_if shape: {y_train_1_month_if.shape}\")\n",
    "print(f\"X_test_1_month_if shape: {X_test_1_month_if.shape}, y_test_1_month_if shape: {y_test_1_month_if.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_if.shape[0] == 0 or X_test_1_month_if.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_if_1_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_if_1_month.fit(X_train_1_month_if, y_train_1_month_if)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_if = model_baseline_if_1_month.predict(X_test_1_month_if)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_if = mean_squared_error(y_test_1_month_if, y_pred_1_month_if)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_if}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20339820-f94b-4c03-83cd-3bf8ac18595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_month_if` are your predictions for the test data and `y_test_1_month_if` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_month_if = mean_squared_error(y_test_1_month_if, y_pred_1_month_if)\n",
    "mae_1_month_if = mean_absolute_error(y_test_1_month_if, y_pred_1_month_if)\n",
    "rmse_1_month_if = np.sqrt(mse_1_month_if)  # Root Mean Squared Error\n",
    "r2_1_month_if = r2_score(y_test_1_month_if, y_pred_1_month_if)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_month_if}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_if}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_month_if}')\n",
    "print(f'R-squared on unseen data: {r2_1_month_if}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_if = median_absolute_error(y_test_1_month_if, y_pred_1_month_if)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_if}')\n",
    "\n",
    "dw_stat_1_month_if = durbin_watson(y_test_1_month_if - y_pred_1_month_if)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_if}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_if = np.mean(np.abs((y_test_1_month_if - y_pred_1_month_if) / y_test_1_month_if)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_if:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_month_if = dict(zip(X_train_1_month_if.columns, model_baseline_if_1_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_if = sorted(feature_importance_1_month_if.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_if:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1861cb51-cd63-4973-944a-35e4679656d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 month baseline model with learning_rate=0.1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_lr_1 = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_lr_1 = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_lr_1.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_lr_1['Close_Target'] = df_stock_data_train_1_month_lr_1.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_lr_1['Close_Target'] = df_stock_data_test_1_month_lr_1.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_lr_1 = df_stock_data_train_1_month_lr_1.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_lr_1 = df_stock_data_test_1_month_lr_1.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_lr_1[numeric_cols_train] = df_stock_data_train_1_month_lr_1[numeric_cols_train].fillna(df_stock_data_train_1_month_lr_1[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_lr_1[numeric_cols_test] = df_stock_data_test_1_month_lr_1[numeric_cols_test].fillna(df_stock_data_test_1_month_lr_1[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_lr_1.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_lr_1.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_lr_1 = df_stock_data_train_1_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_lr_1 = df_stock_data_train_1_month_lr_1['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_lr_1 = df_stock_data_test_1_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_lr_1 = df_stock_data_test_1_month_lr_1['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_lr_1 shape: {X_train_1_month_lr_1.shape}, y_train_1_month_lr_1 shape: {y_train_1_month_lr_1.shape}\")\n",
    "print(f\"X_test_1_month_lr_1 shape: {X_test_1_month_lr_1.shape}, y_test_1_month_lr_1 shape: {y_test_1_month_lr_1.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_lr_1.shape[0] == 0 or X_test_1_month_lr_1.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_tr_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_tr_01.fit(X_train_1_month_lr_1, y_train_1_month_lr_1)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_lr_1 = model_1_month_tr_01.predict(X_test_1_month_lr_1)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_lr_1 = mean_squared_error(y_test_1_month_lr_1, y_pred_1_month_lr_1)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_lr_1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a25ea7-1c21-4032-9fcd-8e1963941b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_month_lr_1` are your predictions for the test data and `y_test_1_month_lr_1` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_month_lr_1 = mean_squared_error(y_test_1_month_lr_1, y_pred_1_month_lr_1)\n",
    "mae_1_month_lr_1 = mean_absolute_error(y_test_1_month_lr_1, y_pred_1_month_lr_1)\n",
    "rmse_1_month_lr_1 = np.sqrt(mse_1_month_lr_1)  # Root Mean Squared Error\n",
    "r2_1_month_lr_1 = r2_score(y_test_1_month_lr_1, y_pred_1_month_lr_1)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_month_lr_1}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_lr_1}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_month_lr_1}')\n",
    "print(f'R-squared on unseen data: {r2_1_month_lr_1}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_lr_1 = median_absolute_error(y_test_1_month_lr_1, y_pred_1_month_lr_1)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_lr_1}')\n",
    "\n",
    "dw_stat_1_month_lr_1 = durbin_watson(y_test_1_month_lr_1 - y_pred_1_month_lr_1)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_lr_1}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_lr_1 = np.mean(np.abs((y_test_1_month_lr_1 - y_pred_1_month_lr_1) / y_test_1_month_lr_1)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_lr_1:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_month_lr_1 = dict(zip(X_train_1_month_lr_1.columns, model_1_month_tr_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_lr_1 = sorted(feature_importance_1_month_lr_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_lr_1:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241485c-b2a1-41a6-ae31-03bf3500db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 month baseline model with learning_rate=0.01\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_lr_01 = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_lr_01 = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_lr_01.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_lr_01['Close_Target'] = df_stock_data_train_1_month_lr_01.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_lr_01['Close_Target'] = df_stock_data_test_1_month_lr_01.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_lr_01 = df_stock_data_train_1_month_lr_01.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_lr_01 = df_stock_data_test_1_month_lr_01.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_lr_01[numeric_cols_train] = df_stock_data_train_1_month_lr_01[numeric_cols_train].fillna(df_stock_data_train_1_month_lr_01[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_lr_01[numeric_cols_test] = df_stock_data_test_1_month_lr_01[numeric_cols_test].fillna(df_stock_data_test_1_month_lr_01[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_lr_01.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_lr_01.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_lr_01 = df_stock_data_train_1_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_lr_01 = df_stock_data_train_1_month_lr_01['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_lr_01 = df_stock_data_test_1_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_lr_01 = df_stock_data_test_1_month_lr_01['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_lr_01 shape: {X_train_1_month_lr_01.shape}, y_train_1_month_lr_01 shape: {y_train_1_month_lr_01.shape}\")\n",
    "print(f\"X_test_1_month_lr_01 shape: {X_test_1_month_lr_01.shape}, y_test_1_month_lr_01 shape: {y_test_1_month_lr_01.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_lr_01.shape[0] == 0 or X_test_1_month_lr_01.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_tr_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_tr_1.fit(X_train_1_month_lr_01, y_train_1_month_lr_01)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_lr_01 = model_1_month_tr_1.predict(X_test_1_month_lr_01)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_lr_01 = mean_squared_error(y_test_1_month_lr_01, y_pred_1_month_lr_01)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_lr_01}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2f654-7efe-4059-8f53-bbcfdfedf986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_1_month_lr_01` are your predictions for the test data and `y_test_1_month_lr_01` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_month_lr_01 = mean_squared_error(y_test_1_month_lr_01, y_pred_1_month_lr_01)\n",
    "mae_1_month_lr_01 = mean_absolute_error(y_test_1_month_lr_01, y_pred_1_month_lr_01)\n",
    "rmse_month_lr_01 = np.sqrt(mse_month_lr_01)  # Root Mean Squared Error\n",
    "r2_month_lr_01 = r2_score(y_test_1_month_lr_01, y_pred_1_month_lr_01)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_month_lr_01}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_lr_01}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_month_lr_01}')\n",
    "print(f'R-squared on unseen data: {r2_month_lr_01}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_lr_01 = median_absolute_error(y_test_1_month_lr_01, y_pred_1_month_lr_01)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_lr_01}')\n",
    "\n",
    "dw_stat_1_month_lr_01 = durbin_watson(y_test_1_month_lr_01 - y_pred_1_month_lr_01)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_lr_01}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_lr_01 = np.mean(np.abs((y_test_1_month_lr_01 - y_pred_1_month_lr_01) / y_test_1_month_lr_01)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_lr_01:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_month_lr_01 = dict(zip(X_train_1_month_lr_01.columns, model_1_month_tr_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_lr_01 = sorted(feature_importance_month_lr_01.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_lr_01:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3e270-fd95-4daa-86ef-fce7ca7ead0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with learning_rate = 0.01 is best again, so we keep that parameter\n",
    "# now we'll do max depth\n",
    "# max depth = 3\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_md_3 = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_md_3 = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_md_3.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_md_3['Close_Target'] = df_stock_data_train_1_month_md_3.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_md_3['Close_Target'] = df_stock_data_test_1_month_md_3.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_md_3 = df_stock_data_train_1_month_md_3.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_md_3 = df_stock_data_test_1_month_md_3.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_md_3[numeric_cols_train] = df_stock_data_train_1_month_md_3[numeric_cols_train].fillna(df_stock_data_train_1_month_md_3[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_md_3[numeric_cols_test] = df_stock_data_test_1_month_md_3[numeric_cols_test].fillna(df_stock_data_test_1_month_md_3[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_md_3.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_md_3.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_md_3 = df_stock_data_train_1_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_md_3 = df_stock_data_train_1_month_md_3['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_md_3 = df_stock_data_test_1_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_md_3 = df_stock_data_test_1_month_md_3['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_md_3 shape: {X_train_1_month_md_3.shape}, y_train_1_month_md_3 shape: {y_train_1_month_md_3.shape}\")\n",
    "print(f\"X_test_1_month_md_3 shape: {X_test_1_month_md_3.shape}, y_test_1_month_md_3 shape: {y_test_1_month_md_3.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_md_3.shape[0] == 0 or X_test_1_month_md_3.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_md_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_md_3.fit(X_train_1_month_md_3, y_train_1_month_md_3)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_md_3 = model_1_month_md_3.predict(X_test_1_month_md_3)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_md_3 = mean_squared_error(y_test_1_month_md_3, y_pred_1_month_md_3)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_md_3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb1a2c-a665-4731-b977-0d3e517b69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_1_month_md_3` are your predictions for the test data and `y_test_1_month_md_3` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_month_md_3 = mean_squared_error(y_test_1_month_md_3, y_pred_1_month_md_3)\n",
    "mae_1_month_md_3 = mean_absolute_error(y_test_1_month_md_3, y_pred_1_month_md_3)\n",
    "rmse_1_month_md_3 = np.sqrt(mse_1_month_md_3)  # Root Mean Squared Error\n",
    "r2_1_month_md_3 = r2_score(y_test_1_month_md_3, y_pred_1_month_md_3)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_month_md_3}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_md_3}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_month_md_3}')\n",
    "print(f'R-squared on unseen data: {r2_1_month_md_3}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_md_3 = median_absolute_error(y_test_1_month_md_3, y_pred_1_month_md_3)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_md_3}')\n",
    "\n",
    "dw_stat_1_month_md_3 = durbin_watson(y_test_1_month_md_3 - y_pred_1_month_md_3)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_md_3}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_md_3 = np.mean(np.abs((y_test_1_month_md_3 - y_pred_1_month_md_3) / y_test_1_month_md_3)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_md_3:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_month_md_3 = dict(zip(X_train_1_month_md_3.columns, model_1_month_md_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_md_3 = sorted(feature_importance_1_month_md_3.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_md_3:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c640a6d-da1c-4cb3-9f2b-20c872bf533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with learning_rate = 0.01 is best again, so we keep that parameter\n",
    "# now we'll do max depth\n",
    "# max depth = 7\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_md_7 = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_md_7 = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_md_7.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_md_7['Close_Target'] = df_stock_data_train_1_month_md_7.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_md_7['Close_Target'] = df_stock_data_test_1_month_md_7.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_md_7 = df_stock_data_train_1_month_md_7.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_md_7 = df_stock_data_test_1_month_md_7.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_md_7[numeric_cols_train] = df_stock_data_train_1_month_md_7[numeric_cols_train].fillna(df_stock_data_train_1_month_md_7[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_md_7[numeric_cols_test] = df_stock_data_test_1_month_md_7[numeric_cols_test].fillna(df_stock_data_test_1_month_md_7[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_md_7.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_md_7.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_md_7 = df_stock_data_train_1_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_md_7 = df_stock_data_train_1_month_md_7['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_md_7 = df_stock_data_test_1_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_md_7 = df_stock_data_test_1_month_md_7['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_md_7 shape: {X_train_1_month_md_7.shape}, y_train_1_month_md_7 shape: {y_train_1_month_md_7.shape}\")\n",
    "print(f\"X_test_1_month_md_7 shape: {X_test_1_month_md_7.shape}, y_test_1_month_md_7 shape: {y_test_1_month_md_7.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_md_7.shape[0] == 0 or X_test_1_month_md_7.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_md_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_md_7.fit(X_train_1_month_md_7, y_train_1_month_md_7)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_md_7 = model_1_month_md_7.predict(X_test_1_month_md_7)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_md_7 = mean_squared_error(y_test_1_month_md_7, y_pred_1_month_md_7)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_md_7}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f51e8b-000d-4892-9bd6-24ddb35a35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_month_md_7` are your predictions for the test data and `y_test_1_month_md_7` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_month_md_7 = mean_squared_error(y_test_1_month_md_7, y_pred_1_month_md_7)\n",
    "mae_1_month_md_7 = mean_absolute_error(y_test_1_month_md_7, y_pred_1_month_md_7)\n",
    "rmse_1_month_md_7 = np.sqrt(mse_1_month_md_7)  # Root Mean Squared Error\n",
    "r2_1_month_md_7 = r2_score(y_test_1_month_md_7, y_pred_1_month_md_7)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_month_md_7}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_md_7}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_month_md_7}')\n",
    "print(f'R-squared on unseen data: {r2_1_month_md_7}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_md_7 = median_absolute_error(y_test_1_month_md_7, y_pred_1_month_md_7)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_md_7}')\n",
    "\n",
    "dw_stat_1_month_md_7 = durbin_watson(y_test_1_month_md_7 - y_pred_1_month_md_7)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_md_7}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_md_7 = np.mean(np.abs((y_test_1_month_md_7 - y_pred_1_month_md_7) / y_test_1_month_md_7)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_md_7:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_month_md_7 = dict(zip(X_train_1_month_md_7.columns, model_1_month_md_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_md_7 = sorted(feature_importance_1_month_md_7.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_md_7:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df734c-0863-4b15-91e7-7f6c3b4ca6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features_1_month_md_7[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features_1_month_md_7[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 6)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=16, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=16, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 1 Month Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2163b5-dd6b-4c0f-94d3-8df2186bca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will modify our feature set to add bigger lagging indicators.\n",
    "# Create a new dataframe called 'df_stock_data_3_month' as a copy of 'df_stocks_price_ta'\n",
    "df_stock_data_3_month = df_stocks_price_ta.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc29a6-e7e2-42b4-a34e-828d6db188ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to create lags for (focusing on mid-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_5', 'EMA_5', 'Volume', 'SMA_20',\n",
    "       'SMA_50', 'EMA_5', 'EMA_20', 'EMA_50',  'EMA_12_MACD',\n",
    "       'EMA_26_MACD']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 12, 15, 20, 30, 60, 90, 180, 360] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7, 10, 15, 20, 25, 30, 40, 50, 60, 75, 90]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_stock_data_3_month.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01894bf1-679f-4c3d-a95e-3fcb8be05a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're going to move onto our next model: 3 month prediction\n",
    "# we'll start at our baseline model and then do the same as we just did\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_baseline = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_baseline = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_baseline.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_baseline['Close_Target'] = df_stock_data_train_3_month_baseline.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_baseline['Close_Target'] = df_stock_data_test_3_month_baseline.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_baseline = df_stock_data_train_3_month_baseline.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_baseline = df_stock_data_test_3_month_baseline.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_baseline[numeric_cols_train] = df_stock_data_train_3_month_baseline[numeric_cols_train].fillna(df_stock_data_train_3_month_baseline[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_baseline[numeric_cols_test] = df_stock_data_test_3_month_baseline[numeric_cols_test].fillna(df_stock_data_test_3_month_baseline[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_baseline.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_baseline.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_baseline = df_stock_data_train_3_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_baseline = df_stock_data_train_3_month_baseline['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_baseline = df_stock_data_test_3_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_baseline = df_stock_data_test_3_month_baseline['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_baseline shape: {X_train_3_month_baseline.shape}, y_train_3_month_baseline shape: {y_train_3_month_baseline.shape}\")\n",
    "print(f\"X_test_3_month_baseline shape: {X_test_3_month_baseline.shape}, y_test_3_month_baseline shape: {y_test_3_month_baseline.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_baseline.shape[0] == 0 or X_test_3_month_baseline.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_3_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_3_month.fit(X_train_3_month_baseline, y_train_3_month_baseline)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_baseline = model_baseline_3_month.predict(X_test_3_month_baseline)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_baseline = mean_squared_error(y_test_3_month_baseline, y_pred_3_month_baseline)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_baseline}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfae4e6-6f28-4d01-9880-4878ce427875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_3_month_baseline` are your predictions for the test data and `y_test_3_month_baseline` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_baseline = mean_squared_error(y_test_3_month_baseline, y_pred_3_month_baseline)\n",
    "mae_3_month_baseline = mean_absolute_error(y_test_3_month_baseline, y_pred_3_month_baseline)\n",
    "rmse_3_month_baseline = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2_3_month_baseline = r2_score(y_test_3_month_baseline, y_pred_3_month_baseline)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_baseline}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_baseline}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_baseline}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_baseline}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_baseline = median_absolute_error(y_test_3_month_baseline, y_pred_3_month_baseline)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_baseline}')\n",
    "\n",
    "dw_stat_3_month_baseline = durbin_watson(y_test_3_month_baseline - y_pred_3_month_baseline)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_baseline}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_baseline = np.mean(np.abs((y_test_3_month_baseline - y_pred_3_month_baseline) / y_test_3_month_baseline)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_baseline:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_baseline = dict(zip(X_train_3_month_baseline.columns, model_baseline_3_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_baseline = sorted(feature_importance_3_month_baseline.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_baseline:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ebb9a-2e8c-4b94-a240-f74295bb8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the baseline model (1-week prediction)\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_3_month.feature_importances_))\n",
    "\n",
    "# Filter features with importance greater than 1%\n",
    "important_features = {feature: importance for feature, importance in feature_importance.items() if importance > 0.01}\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_important_features = sorted(important_features.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the feature names (keys) into a list\n",
    "important_feature_names = [feature for feature, importance in sorted_important_features]\n",
    "\n",
    "# Print the sorted important features (optional)\n",
    "print(\"Features with more than 1% contribution:\")\n",
    "for feature in sorted_important_features:\n",
    "    print(f\"{feature[0]}: {feature[1] * 100:.2f}%\")\n",
    "\n",
    "# The list of important features that you can use to create a new dataframe\n",
    "print(\"List of important features:\")\n",
    "print(important_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03ff90-2e8c-489c-8482-f189fd3cbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = ['Symbol', 'Date', 'Close', 'Fib_30_Low_Min', '30_day_Fib_38',\n",
    "                      '5_day-Fib_61', '5_day-Fib_23', 'EMA_5', 'Volume',\n",
    "                      'EMA_12_MACD', 'High', 'Low', '30_day_Fib_61',\n",
    "                      'ATR_Prev_Close', 'Fib_5_Low_Min']\n",
    "df_important_feat_3_month = df_stock_data_3_month[important_features]\n",
    "df_important_feat_3_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907f0d3-cb76-4323-a038-1468b2b071ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 month prediction with only important featuers\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_important_feat_3_month = df_important_feat_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_if = df_important_feat_3_month[df_important_feat_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_if = df_important_feat_3_month[df_important_feat_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_if.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_if['Close_Target'] = df_stock_data_train_3_month_if.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_if['Close_Target'] = df_stock_data_test_3_month_if.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_if = df_stock_data_train_3_month_if.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_if = df_stock_data_test_3_month_if.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_if.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_if.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_if[numeric_cols_train] = df_stock_data_train_3_month_if[numeric_cols_train].fillna(df_stock_data_train_3_month_if[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_if[numeric_cols_test] = df_stock_data_test_3_month_if[numeric_cols_test].fillna(df_stock_data_test_3_month_if[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_if.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_if.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_if = df_stock_data_train_3_month_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_if = df_stock_data_train_3_month_if['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_if = df_stock_data_test_3_month_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_if = df_stock_data_test_3_month_if['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_if shape: {X_train_3_month_if.shape}, y_train_3_month_if shape: {y_train_3_month_if.shape}\")\n",
    "print(f\"X_test_3_month_if shape: {X_test_3_month_if.shape}, y_test_3_month_if shape: {y_test_3_month_if.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_if.shape[0] == 0 or X_test_3_month_if.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_if_3_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_if_3_month.fit(X_train_3_month_if, y_train_3_month_if)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_if = model_baseline_if_3_month.predict(X_test_3_month_if)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_if = mean_squared_error(y_test_3_month_if, y_pred_3_month_if)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_if}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52695914-d9ee-493e-8795-ad01a148d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_3_month_if` are your predictions for the test data and `y_test_3_month_if` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_if = mean_squared_error(y_test_3_month_if, y_pred_3_month_if)\n",
    "mae_3_month_if = mean_absolute_error(y_test_3_month_if, y_pred_3_month_if)\n",
    "rmse_3_month_if = np.sqrt(mse_3_month_if)  # Root Mean Squared Error\n",
    "r2_3_month_if = r2_score(y_test_3_month_if, y_pred_3_month_if)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_if}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_if}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_if}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_if}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_if = median_absolute_error(y_test_3_month_if, y_pred_3_month_if)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_if}')\n",
    "\n",
    "dw_stat_3_month_if = durbin_watson(y_test_3_month_if - y_pred_3_month_if)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_if}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_if = np.mean(np.abs((y_test_3_month_if - y_pred_3_month_if) / y_test_3_month_if)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_if:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_if = dict(zip(X_train_3_month_if.columns, model_baseline_if_3_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_if = sorted(feature_importance_3_month_if.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_if:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8314d71a-686c-4902-8101-1bc25fa85b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = 0.1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_lr_1 = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_lr_1 = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_lr_1.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_lr_1['Close_Target'] = df_stock_data_train_3_month_lr_1.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_lr_1['Close_Target'] = df_stock_data_test_3_month_lr_1.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_lr_1 = df_stock_data_train_3_month_lr_1.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_lr_1 = df_stock_data_test_3_month_lr_1.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_lr_1[numeric_cols_train] = df_stock_data_train_3_month_lr_1[numeric_cols_train].fillna(df_stock_data_train_3_month_lr_1[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_lr_1[numeric_cols_test] = df_stock_data_test_3_month_lr_1[numeric_cols_test].fillna(df_stock_data_test_3_month_lr_1[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_lr_1.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_lr_1.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_lr_1 = df_stock_data_train_3_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_lr_1 = df_stock_data_train_3_month_lr_1['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_lr_1 = df_stock_data_test_3_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_lr_1 = df_stock_data_test_3_month_lr_1['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_lr_1 shape: {X_train_3_month_lr_1.shape}, y_train_3_month_lr_1 shape: {y_train_3_month_lr_1.shape}\")\n",
    "print(f\"X_test_3_month_lr_1 shape: {X_test_3_month_lr_1.shape}, y_test_3_month_lr_1 shape: {y_test_3_month_lr_1.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_lr_1.shape[0] == 0 or X_test_3_month_lr_1.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_tf_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_tf_1.fit(X_train_3_month_lr_1, y_train_3_month_lr_1)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_lr_1 = model_3_month_tf_1.predict(X_test_3_month_lr_1)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_lr_1 = mean_squared_error(y_test_3_month_lr_1, y_pred_3_month_lr_1)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_lr_1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da607ba9-e76b-48e3-94b4-700d86cc9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_3_month_lr_1` are your predictions for the test data and `y_test_3_month_lr_1` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_lr_1 = mean_squared_error(y_test_3_month_lr_1, y_pred_3_month_lr_1)\n",
    "mae_3_month_lr_1 = mean_absolute_error(y_test_3_month_lr_1, y_pred_3_month_lr_1)\n",
    "rmse_3_month_lr_1 = np.sqrt(mse_3_month_lr_1)  # Root Mean Squared Error\n",
    "r2_3_month_lr_1 = r2_score(y_test_3_month_lr_1, y_pred_3_month_lr_1)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_lr_1}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_lr_1}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_lr_1}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_lr_1}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_lr_1 = median_absolute_error(y_test_3_month_lr_1, y_pred_3_month_lr_1)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_lr_1}')\n",
    "\n",
    "dw_stat_3_month_lr_1 = durbin_watson(y_test_3_month_lr_1 - y_pred_3_month_lr_1)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_lr_1}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_lr_1 = np.mean(np.abs((y_test_3_month_lr_1 - y_pred_3_month_lr_1) / y_test_3_month_lr_1)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_lr_1:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_lr_1 = dict(zip(X_train_3_month_lr_1.columns, model_3_month_tf_1.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_lr_1 = sorted(feature_importance_3_month_lr_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_lr_1:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c086d1f-6b98-4cc0-94b4-d7a2a347ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = 0.01\n",
    "# this is the best one again\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_lr_01 = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_lr_01 = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_lr_01.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_lr_01['Close_Target'] = df_stock_data_train_3_month_lr_01.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_lr_01['Close_Target'] = df_stock_data_test_3_month_lr_01.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_lr_01 = df_stock_data_train_3_month_lr_01.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_lr_01 = df_stock_data_test_3_month_lr_01.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_lr_01[numeric_cols_train] = df_stock_data_train_3_month_lr_01[numeric_cols_train].fillna(df_stock_data_train_3_month_lr_01[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_lr_01[numeric_cols_test] = df_stock_data_test_3_month_lr_01[numeric_cols_test].fillna(df_stock_data_test_3_month_lr_01[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_lr_01.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_lr_01.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_lr_01 = df_stock_data_train_3_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_lr_01 = df_stock_data_train_3_month_lr_01['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_lr_01 = df_stock_data_test_3_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_lr_01 = df_stock_data_test_3_month_lr_01['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_lr_01 shape: {X_train_3_month_lr_01.shape}, y_train_3_month_lr_01 shape: {y_train_3_month_lr_01.shape}\")\n",
    "print(f\"X_test_3_month_lr_01 shape: {X_test_3_month_lr_01.shape}, y_test_3_month_lr_01 shape: {y_test_3_month_lr_01.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_lr_01.shape[0] == 0 or X_test_3_month_lr_01.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_tf_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_tf_01.fit(X_train_3_month_lr_01, y_train_3_month_lr_01)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_lr_01 = model_3_month_tf_01.predict(X_test_3_month_lr_01)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_lr_01 = mean_squared_error(y_test_3_month_lr_01, y_pred_3_month_lr_01)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_lr_01}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e4f7c-a236-4bdf-b030-16da772eb9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_3_month_lr_01` are your predictions for the test data and `y_test_3_month_lr_01` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_lr_01 = mean_squared_error(y_test_3_month_lr_01, y_pred_3_month_lr_01)\n",
    "mae_3_month_lr_01 = mean_absolute_error(y_test_3_month_lr_01, y_pred_3_month_lr_01)\n",
    "rmse_3_month_lr_01 = np.sqrt(mse_3_month_lr_01)  # Root Mean Squared Error\n",
    "r2_3_month_lr_01 = r2_score(y_test_3_month_lr_01, y_pred_3_month_lr_01)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_lr_01}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_lr_01}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_lr_01}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_lr_01}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_lr_01 = median_absolute_error(y_test_3_month_lr_01, y_pred_3_month_lr_01)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_lr_01}')\n",
    "\n",
    "dw_stat_3_month_lr_01 = durbin_watson(y_test_3_month_lr_01 - y_pred_3_month_lr_01)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_lr_01}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_lr_01 = np.mean(np.abs((y_test_3_month_lr_01 - y_pred_3_month_lr_01) / y_test_3_month_lr_01)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_lr_01:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_lr_01 = dict(zip(X_train_3_month_lr_01.columns, model_3_month_tf_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_lr_01 = sorted(feature_importance_3_month_lr_01.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_lr_01:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348432af-38f0-446c-a678-3764e9a2fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = 0.01\n",
    "# max depth 3\n",
    "# this is actually the best one for 3 months now\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_md_3 = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_md_3 = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_md_3.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_md_3['Close_Target'] = df_stock_data_train_3_month_md_3.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_md_3['Close_Target'] = df_stock_data_test_3_month_md_3.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_md_3 = df_stock_data_train_3_month_md_3.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_md_3 = df_stock_data_test_3_month_md_3.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_md_3[numeric_cols_train] = df_stock_data_train_3_month_md_3[numeric_cols_train].fillna(df_stock_data_train_3_month_md_3[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_md_3[numeric_cols_test] = df_stock_data_test_3_month_md_3[numeric_cols_test].fillna(df_stock_data_test_3_month_md_3[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_md_3.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_md_3.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_md_3 = df_stock_data_train_3_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_md_3 = df_stock_data_train_3_month_md_3['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_md_3 = df_stock_data_test_3_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_md_3 = df_stock_data_test_3_month_md_3['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_md_3 shape: {X_train_3_month_md_3.shape}, y_train_3_month_md_3 shape: {y_train_3_month_md_3.shape}\")\n",
    "print(f\"X_test_3_month_md_3 shape: {X_test_3_month_md_3.shape}, y_test_3_month_md_3 shape: {y_test_3_month_md_3.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_md_3.shape[0] == 0 or X_test_3_month_md_3.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_md_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_md_3.fit(X_train_3_month_md_3, y_train_3_month_md_3)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_md_3 = model_3_month_md_3.predict(X_test_3_month_md_3)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_md_3 = mean_squared_error(y_test_3_month_md_3, y_pred_3_month_md_3)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_md_3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89181266-05d0-4c3d-8f06-93fa693c9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_3_month_md_3` are your predictions for the test data and `y_test_3_month_md_3` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_md_3 = mean_squared_error(y_test_3_month_md_3, y_pred_3_month_md_3)\n",
    "mae_3_month_md_3 = mean_absolute_error(y_test_3_month_md_3, y_pred_3_month_md_3)\n",
    "rmse_3_month_md_3 = np.sqrt(mse_3_month_md_3)  # Root Mean Squared Error\n",
    "r2_3_month_md_3 = r2_score(y_test_3_month_md_3, y_pred_3_month_md_3)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_md_3}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_md_3}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_md_3}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_md_3}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_md_3 = median_absolute_error(y_test_3_month_md_3, y_pred_3_month_md_3)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_md_3}')\n",
    "\n",
    "dw_stat_3_month_md_3 = durbin_watson(y_test_3_month_md_3 - y_pred_3_month_md_3)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_md_3}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_md_3 = np.mean(np.abs((y_test_3_month_md_3 - y_pred_3_month_md_3) / y_test_3_month_md_3)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_md_3:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_md_3 = dict(zip(X_train_3_month_md_3.columns, model_3_month_md_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_md_3 = sorted(feature_importance_3_month_md_3.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_md_3:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86699263-cf02-41ab-8ce1-a26c9531fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = 0.01\n",
    "# max depth 7\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_md_7 = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_md_7 = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_md_7.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_md_7['Close_Target'] = df_stock_data_train_3_month_md_7.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_md_7['Close_Target'] = df_stock_data_test_3_month_md_7.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_md_7 = df_stock_data_train_3_month_md_7.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_md_7 = df_stock_data_test_3_month_md_7.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_md_7[numeric_cols_train] = df_stock_data_train_3_month_md_7[numeric_cols_train].fillna(df_stock_data_train_3_month_md_7[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_md_7[numeric_cols_test] = df_stock_data_test_3_month_md_7[numeric_cols_test].fillna(df_stock_data_test_3_month_md_7[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_md_7.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_md_7.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_md_7 = df_stock_data_train_3_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_md_7 = df_stock_data_train_3_month_md_7['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_md_7 = df_stock_data_test_3_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_md_7 = df_stock_data_test_3_month_md_7['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_md_7 shape: {X_train_3_month_md_7.shape}, y_train_3_month_md_7 shape: {y_train_3_month_md_7.shape}\")\n",
    "print(f\"X_test_3_month_md_7 shape: {X_test_3_month_md_7.shape}, y_test_3_month_md_7 shape: {y_test_3_month_md_7.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_md_7.shape[0] == 0 or X_test_3_month_md_7.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_md_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_md_7.fit(X_train_3_month_md_7, y_train_3_month_md_7)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_md_7 = model_3_month_md_7.predict(X_test_3_month_md_7)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_md_7 = mean_squared_error(y_test_3_month_md_7, y_pred_3_month_md_7)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_md_7}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd94b99-7fca-49a4-bcf0-716ad08f459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_3_month_md_7` are your predictions for the test data and `y_test_3_month_md_7` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_md_7 = mean_squared_error(y_test_3_month_md_7, y_pred_3_month_md_7)\n",
    "mae_3_month_md_7 = mean_absolute_error(y_test_3_month_md_7, y_pred_3_month_md_7)\n",
    "rmse_3_month_md_7 = np.sqrt(mse_3_month_md_7)  # Root Mean Squared Error\n",
    "r2_3_month_md_7 = r2_score(y_test_3_month_md_7, y_pred_3_month_md_7)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_md_7}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_md_7}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_md_7}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_md_7}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_md_7 = median_absolute_error(y_test_3_month_md_7, y_pred_3_month_md_7)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_md_7}')\n",
    "\n",
    "dw_stat_3_month_md_7 = durbin_watson(y_test_3_month_md_7 - y_pred_3_month_md_7)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_md_7}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_md_7 = np.mean(np.abs((y_test_3_month_md_7 - y_pred_3_month_md_7) / y_test_3_month_md_7)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_md_7:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_md_7 = dict(zip(X_train_3_month_md_7.columns, model_3_month_md_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_md_7 = sorted(feature_importance_3_month_md_7.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_md_7:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2757dd-30da-423a-80a6-7b9bc4eeec73",
   "metadata": {},
   "source": [
    "best model: learning_rate = 0.01 and max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a7d87-e136-4beb-985c-b1989d1d6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features_3_month_md_3[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features_3_month_md_3[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 3)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=18, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=18, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 3 Month Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c48d5c5-d7c5-49d8-b601-312cbc13048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will modify our feature set to add bigger lagging indicators.\n",
    "# Create a new dataframe called 'df_stock_data_6_month' as a copy of 'df_stocks_price_ta'\n",
    "df_stock_data_6_month = df_stocks_price_ta.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e314a56-c23b-42b9-a9f0-bbf269d95b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to create lags for (focusing on mid-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_5', 'EMA_5', 'Volume', 'SMA_20',\n",
    "       'SMA_50', 'EMA_5', 'EMA_20', 'EMA_50',  'EMA_12_MACD',\n",
    "       'EMA_26_MACD']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 12, 15, 20, 30, 60, 90, 180, 360] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7, 10, 15, 20, 25, 30, 40, 50, 60, 75, 90, 180]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_stock_data_6_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f34d1-1235-42aa-8716-fe89165ce334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're going to move onto our next model: 6 month prediction\n",
    "# we'll start at our baseline model and then do the same as we just did\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_6_month_baseline = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_6_month_baseline = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_6_month_baseline.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_6_month_baseline['Close_Target'] = df_stock_data_train_6_month_baseline.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test_6_month_baseline['Close_Target'] = df_stock_data_test_6_month_baseline.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_6_month_baseline = df_stock_data_train_6_month_baseline.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_6_month_baseline = df_stock_data_test_6_month_baseline.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_6_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_6_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_6_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_6_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_6_month_baseline[numeric_cols_train] = df_stock_data_train_6_month_baseline[numeric_cols_train].fillna(df_stock_data_train_6_month_baseline[numeric_cols_train].median())\n",
    "df_stock_data_test_6_month_baseline[numeric_cols_test] = df_stock_data_test_6_month_baseline[numeric_cols_test].fillna(df_stock_data_test_6_month_baseline[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_6_month_baseline.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_6_month_baseline.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_6_month_baseline = df_stock_data_train_6_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_6_month_baseline = df_stock_data_train_6_month_baseline['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_6_month_baseline = df_stock_data_test_6_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_6_month_baseline = df_stock_data_test_6_month_baseline['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_6_month_baseline shape: {X_train_6_month_baseline.shape}, y_train_6_month_baseline shape: {y_train_6_month_baseline.shape}\")\n",
    "print(f\"X_test_6_month_baseline shape: {X_test_6_month_baseline.shape}, y_test_6_month_baseline shape: {y_test_6_month_baseline.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_6_month_baseline.shape[0] == 0 or X_test_6_month_baseline.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month.fit(X_train_6_month_baseline, y_train_6_month_baseline)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_6_month_baseline = model_baseline_6_month.predict(X_test_6_month_baseline)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_6_month_baseline = mean_squared_error(y_test_6_month_baseline, y_pred_6_month_baseline)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_6_month_baseline}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09aee03-9fcd-4fdb-88d5-0c9e0e73b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_6_month_baseline` are your predictions for the test data and `y_test_6_month_baseline` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_6_month_baseline = mean_squared_error(y_test_6_month_baseline, y_pred_6_month_baseline)\n",
    "mae_6_month_baseline = mean_absolute_error(y_test_6_month_baseline, y_pred_6_month_baseline)\n",
    "rmse_6_month_baseline = np.sqrt(mse_6_month_baseline)  # Root Mean Squared Error\n",
    "r2_6_month_baseline = r2_score(y_test_6_month_baseline, y_pred_6_month_baseline)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_6_month_baseline}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_6_month_baseline}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_6_month_baseline}')\n",
    "print(f'R-squared on unseen data: {r2_6_month_baseline}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_6_month_baseline = median_absolute_error(y_test_6_month_baseline, y_pred_6_month_baseline)\n",
    "print(f'Median Absolute Error on unseen data: {medae_6_month_baseline}')\n",
    "\n",
    "dw_stat_6_month_baseline = durbin_watson(y_test_6_month_baseline - y_pred_6_month_baseline)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_6_month_baseline}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_6_month_baseline = np.mean(np.abs((y_test_6_month_baseline - y_pred_6_month_baseline) / y_test_6_month_baseline)) * 100\n",
    "print(f'MAPE on unseen data: {mape_6_month_baseline:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_6_month_baseline = dict(zip(X_train_6_month_baseline.columns, model_baseline_6_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_6_month_baseline = sorted(feature_importance_6_month_baseline.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_6_month_baseline:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28bc7d-0e99-4862-9908-d092c1b0408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_6_month_lr_1 = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_6_month_lr_1 = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_6_month_lr_1.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_6_month_lr_1['Close_Target'] = df_stock_data_train_6_month_lr_1.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test_6_month_lr_1['Close_Target'] = df_stock_data_test_6_month_lr_1.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_6_month_lr_1 = df_stock_data_train_6_month_lr_1.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_6_month_lr_1 = df_stock_data_test_6_month_lr_1.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_6_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_6_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_6_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_6_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_6_month_lr_1[numeric_cols_train] = df_stock_data_train_6_month_lr_1[numeric_cols_train].fillna(df_stock_data_train_6_month_lr_1[numeric_cols_train].median())\n",
    "df_stock_data_test_6_month_lr_1[numeric_cols_test] = df_stock_data_test_6_month_lr_1[numeric_cols_test].fillna(df_stock_data_test_6_month_lr_1[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_6_month_lr_1.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_6_month_lr_1.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_6_month_lr_1 = df_stock_data_train_6_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_6_month_lr_1 = df_stock_data_train_6_month_lr_1['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_6_month_lr_1 = df_stock_data_test_6_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_6_month_lr_1 = df_stock_data_test_6_month_lr_1['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_6_month_lr_1 shape: {X_train_6_month_lr_1.shape}, y_train_6_month_lr_1 shape: {y_train_6_month_lr_1.shape}\")\n",
    "print(f\"X_test_6_month_lr_1 shape: {X_test_6_month_lr_1.shape}, y_test_6_month_lr_1 shape: {y_test_6_month_lr_1.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_6_month_lr_1.shape[0] == 0 or X_test_6_month_lr_1.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_lr_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_lr_1.fit(X_train_6_month_lr_1, y_train_6_month_lr_1)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_6_month_lr_1 = model_baseline_6_month_lr_1.predict(X_test_6_month_lr_1)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test_6_month_lr_1, y_pred_6_month_lr_1)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0079e37-5c7f-40ee-ba59-50bde2f72d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_6_month_lr_1` are your predictions for the test data and `y_test_6_month_lr_1` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_6_month_lr_1 = mean_squared_error(y_test_6_month_lr_1, y_pred_6_month_lr_1)\n",
    "mae_6_month_lr_1 = mean_absolute_error(y_test_6_month_lr_1, y_pred_6_month_lr_1)\n",
    "rmse_6_month_lr_1 = np.sqrt(mse_6_month_lr_1)  # Root Mean Squared Error\n",
    "r2_6_month_lr_1 = r2_score(y_test_6_month_lr_1, y_pred_6_month_lr_1)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_6_month_lr_1}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_6_month_lr_1}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_6_month_lr_1}')\n",
    "print(f'R-squared on unseen data: {r2_6_month_lr_1}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_6_month_lr_1 = median_absolute_error(y_test_6_month_lr_1, y_pred_6_month_lr_1)\n",
    "print(f'Median Absolute Error on unseen data: {medae_6_month_lr_1}')\n",
    "\n",
    "dw_stat_6_month_lr_1 = durbin_watson(y_test_6_month_lr_1 - y_pred_6_month_lr_1)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_6_month_lr_1}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_6_month_lr_1 = np.mean(np.abs((y_test_6_month_lr_1 - y_pred_6_month_lr_1) / y_test_6_month_lr_1)) * 100\n",
    "print(f'MAPE on unseen data: {mape_6_month_lr_1:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_6_month_lr_1 = dict(zip(X_train_6_month_lr_1.columns, model_baseline_6_month_lr_1.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_6_month_lr_1 = sorted(feature_importance_6_month_lr_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_6_month_lr_1:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368cfb0-0392-4be4-a586-29b4ce0bc69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.01\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_6_month_lr_01 = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_6_month_lr_01 = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_6_month_lr_01.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_6_month_lr_01['Close_Target'] = df_stock_data_train_6_month_lr_01.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test_6_month_lr_01['Close_Target'] = df_stock_data_test_6_month_lr_01.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_6_month_lr_01 = df_stock_data_train_6_month_lr_01.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_6_month_lr_01 = df_stock_data_test_6_month_lr_01.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_6_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_6_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_6_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_6_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_6_month_lr_01[numeric_cols_train] = df_stock_data_train_6_month_lr_01[numeric_cols_train].fillna(df_stock_data_train_6_month_lr_01[numeric_cols_train].median())\n",
    "df_stock_data_test_6_month_lr_01[numeric_cols_test] = df_stock_data_test_6_month_lr_01[numeric_cols_test].fillna(df_stock_data_test_6_month_lr_01[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_6_month_lr_01.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_6_month_lr_01.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_6_month_lr_01 = df_stock_data_train_6_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_6_month_lr_01 = df_stock_data_train_6_month_lr_01['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_6_month_lr_01 = df_stock_data_test_6_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_6_month_lr_01 = df_stock_data_test_6_month_lr_01['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_6_month_lr_01 shape: {X_train_6_month_lr_01.shape}, y_train_6_month_lr_01 shape: {y_train_6_month_lr_01.shape}\")\n",
    "print(f\"X_test_6_month_lr_01 shape: {X_test_6_month_lr_01.shape}, y_test_6_month_lr_01 shape: {y_test_6_month_lr_01.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_6_month_lr_01.shape[0] == 0 or X_test_6_month_lr_01.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_lr_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_lr_01.fit(X_train_6_month_lr_01, y_train_6_month_lr_01)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_6_month_lr_01 = model_baseline_6_month_lr_01.predict(X_test_6_month_lr_01)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_6_month_lr_01 = mean_squared_error(y_test_6_month_lr_01, y_pred_6_month_lr_01)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_6_month_lr_01}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac22b48-ec53-40ad-bbab-4b9a39f7abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_6_month_lr_01` are your predictions for the test data and `y_test_6_month_lr_01` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_6_month_lr_01 = mean_squared_error(y_test_6_month_lr_01, y_pred_6_month_lr_01)\n",
    "mae_6_month_lr_01 = mean_absolute_error(y_test_6_month_lr_01, y_pred_6_month_lr_01)\n",
    "rmse_6_month_lr_01 = np.sqrt(mse_6_month_lr_01)  # Root Mean Squared Error\n",
    "r2_6_month_lr_01 = r2_score(y_test_6_month_lr_01, y_pred_6_month_lr_01)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_6_month_lr_01}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_6_month_lr_01}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_6_month_lr_01}')\n",
    "print(f'R-squared on unseen data: {r2_6_month_lr_01}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_6_month_lr_01 = median_absolute_error(y_test_6_month_lr_01, y_pred_6_month_lr_01)\n",
    "print(f'Median Absolute Error on unseen data: {medae_6_month_lr_01}')\n",
    "\n",
    "dw_stat_6_month_lr_01 = durbin_watson(y_test_6_month_lr_01 - y_pred_6_month_lr_01)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_6_month_lr_01}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_6_month_lr_01 = np.mean(np.abs((y_test_6_month_lr_01 - y_pred_6_month_lr_01) / y_test_6_month_lr_01)) * 100\n",
    "print(f'MAPE on unseen data: {mape_6_month_lr_01:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_6_month_lr_01 = dict(zip(X_train_6_month_lr_01.columns, model_baseline_6_month_lr_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_6_month_lr_01 = sorted(feature_importance_6_month_lr_01.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_6_month_lr_01:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be07e445-4b9a-4531-bf37-ff3d8725a3d3",
   "metadata": {},
   "source": [
    "model with learning_rate 0.01 performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447c125-546d-49b9-bf7f-43ba6018f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.01\n",
    "# max_depth = 3\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_6_month_md_3 = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_6_month_md_3 = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_6_month_md_3.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_6_month_md_3['Close_Target'] = df_stock_data_train_6_month_md_3.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test_6_month_md_3['Close_Target'] = df_stock_data_test_6_month_md_3.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_6_month_md_3 = df_stock_data_train_6_month_md_3.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_6_month_md_3 = df_stock_data_test_6_month_md_3.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_6_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_6_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_6_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_6_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_6_month_md_3[numeric_cols_train] = df_stock_data_train_6_month_md_3[numeric_cols_train].fillna(df_stock_data_train_6_month_md_3[numeric_cols_train].median())\n",
    "df_stock_data_test_6_month_md_3[numeric_cols_test] = df_stock_data_test_6_month_md_3[numeric_cols_test].fillna(df_stock_data_test_6_month_md_3[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_6_month_md_3.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_6_month_md_3.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_6_month_md_3 = df_stock_data_train_6_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_6_month_md_3 = df_stock_data_train_6_month_md_3['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_6_month_md_3 = df_stock_data_test_6_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_6_month_md_3 = df_stock_data_test_6_month_md_3['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_6_month_md_3 shape: {X_train_6_month_md_3.shape}, y_train_6_month_md_3 shape: {y_train_6_month_md_3.shape}\")\n",
    "print(f\"X_test_6_month_md_3 shape: {X_test_6_month_md_3.shape}, y_test_6_month_md_3 shape: {y_test_6_month_md_3.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_6_month_md_3.shape[0] == 0 or X_test_6_month_md_3.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_md_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_md_3.fit(X_train_6_month_md_3, y_train_6_month_md_3)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_6_month_md_3 = model_baseline_6_month_md_3.predict(X_test_6_month_md_3)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_6_month_md_3 = mean_squared_error(y_test_6_month_md_3, y_pred_6_month_md_3)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_6_month_md_3}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ac369-2efa-460f-b7d2-733d5302933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_6_month_md_3` are your predictions for the test data and `y_test_6_month_md_3` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_6_month_md_3 = mean_squared_error(y_test_6_month_md_3, y_pred_6_month_md_3)\n",
    "mae_6_month_md_3 = mean_absolute_error(y_test_6_month_md_3, y_pred_6_month_md_3)\n",
    "rmse_6_month_md_3 = np.sqrt(mse_6_month_md_3)  # Root Mean Squared Error\n",
    "r2_6_month_md_3 = r2_score(y_test_6_month_md_3, y_pred_6_month_md_3)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_6_month_md_3}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_6_month_md_3}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_6_month_md_3}')\n",
    "print(f'R-squared on unseen data: {r2_6_month_md_3}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_6_month_md_3 = median_absolute_error(y_test_6_month_md_3, y_pred_6_month_md_3)\n",
    "print(f'Median Absolute Error on unseen data: {medae_6_month_md_3}')\n",
    "\n",
    "dw_stat_6_month_md_3 = durbin_watson(y_test_6_month_md_3 - y_pred_6_month_md_3)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_6_month_md_3}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_6_month_md_3 = np.mean(np.abs((y_test_6_month_md_3 - y_pred_6_month_md_3) / y_test_6_month_md_3)) * 100\n",
    "print(f'MAPE on unseen data: {mape_6_month_md_3:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_6_month_md_3 = dict(zip(X_train_6_month_md_3.columns, model_baseline_6_month_md_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_6_month_md_3 = sorted(feature_importance_6_month_md_3.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_6_month_md_3:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba1716-99b9-4fad-a2bc-74cbb5e5f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.01\n",
    "# max_depth = 7\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_6_month_md_7 = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_6_month_md_7 = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_6_month_md_7.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_6_month_md_7['Close_Target'] = df_stock_data_train_6_month_md_7.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test_6_month_md_7['Close_Target'] = df_stock_data_test_6_month_md_7.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_6_month_md_7 = df_stock_data_train_6_month_md_7.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_6_month_md_7 = df_stock_data_test_6_month_md_7.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_6_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_6_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_6_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_6_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_6_month_md_7[numeric_cols_train] = df_stock_data_train_6_month_md_7[numeric_cols_train].fillna(df_stock_data_train_6_month_md_7[numeric_cols_train].median())\n",
    "df_stock_data_test_6_month_md_7[numeric_cols_test] = df_stock_data_test_6_month_md_7[numeric_cols_test].fillna(df_stock_data_test_6_month_md_7[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_6_month_md_7.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_6_month_md_7.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_6_month_md_7 = df_stock_data_train_6_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_6_month_md_7 = df_stock_data_train_6_month_md_7['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_6_month_md_7 = df_stock_data_test_6_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_6_month_md_7 = df_stock_data_test_6_month_md_7['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_6_month_md_7 shape: {X_train_6_month_md_7.shape}, y_train_6_month_md_7 shape: {y_train_6_month_md_7.shape}\")\n",
    "print(f\"X_test_6_month_md_7 shape: {X_test_6_month_md_7.shape}, y_test_6_month_md_7 shape: {y_test_6_month_md_7.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_6_month_md_7.shape[0] == 0 or X_test_6_month_md_7.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_md_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_md_7.fit(X_train_6_month_md_7, y_train_6_month_md_7)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_6_month_md_7 = model_baseline_6_month_md_7.predict(X_test_6_month_md_7)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_6_month_md_7 = mean_squared_error(y_test_6_month_md_7, y_pred_6_month_md_7)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_6_month_md_7}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b5029-bb9b-4f8d-8e6b-111edd9a4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_6_month_md_7` are your predictions for the test data and `y_test_6_month_md_7` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_6_month_md_7 = mean_squared_error(y_test_6_month_md_7, y_pred_6_month_md_7)\n",
    "mae_6_month_md_7 = mean_absolute_error(y_test_6_month_md_7, y_pred_6_month_md_7)\n",
    "rmse_6_month_md_7 = np.sqrt(mse_6_month_md_7)  # Root Mean Squared Error\n",
    "r2_6_month_md_7 = r2_score(y_test_6_month_md_7, y_pred_6_month_md_7)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_6_month_md_7}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_6_month_md_7}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_6_month_md_7}')\n",
    "print(f'R-squared on unseen data: {r2_6_month_md_7}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_6_month_md_7 = median_absolute_error(y_test_6_month_md_7, y_pred_6_month_md_7)\n",
    "print(f'Median Absolute Error on unseen data: {medae_6_month_md_7}')\n",
    "\n",
    "dw_stat_6_month_md_7 = durbin_watson(y_test_6_month_md_7 - y_pred_6_month_md_7)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_6_month_md_7}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_6_month_md_7 = np.mean(np.abs((y_test_6_month_md_7 - y_pred_6_month_md_7) / y_test_6_month_md_7)) * 100\n",
    "print(f'MAPE on unseen data: {mape_6_month_md_7:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_6_month_md_7 = dict(zip(X_train_6_month_md_7.columns, model_baseline_6_month_md_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_6_month_md_7 = sorted(feature_importance_6_month_md_7.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_6_month_md_7:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed1f03-4d19-4618-812d-a4795292834d",
   "metadata": {},
   "source": [
    "best model: learning_rate = 0.01 and max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521210ee-88e2-428b-baf3-8a851df6cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features_6_month_md_3[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features_6_month_md_3[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 3)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=18, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=18, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 6 Month Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c7256-6210-45ca-bdf0-2f820a860f73",
   "metadata": {},
   "source": [
    "Best model metrics\n",
    "\n",
    "most important metrics to visualize:\n",
    "\n",
    "1. Root Mean Squared Error\n",
    "2. R-squared\n",
    "3. MAPE\n",
    "4. Median Absolute Error\n",
    "\n",
    "1 week:\n",
    "\n",
    "Mean Squared Error on unseen data: 7604.147083529955\n",
    "Mean Absolute Error on unseen data: 10.044193260395863\n",
    "Root Mean Squared Error on unseen data: 87.20176078227982\n",
    "R-squared on unseen data: 0.9656152295864199\n",
    "Median Absolute Error on unseen data: 1.588897705078125\n",
    "Durbin-Watson Statistic on unseen data: 0.1674710246723756\n",
    "MAPE on unseen data: 3.96%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Fib_10_Low_Min: 63.23%\n",
    "Fib_30_High_Max: 14.79%\n",
    "Low: 4.90%\n",
    "5_day-Fib_38: 4.25%\n",
    "High: 3.05%\n",
    "5_day-Fib_61: 1.76%\n",
    "10_day_Fib_23: 1.33%\n",
    "Fib_30_Low_Min: 1.28%\n",
    "Volume: 1.07%\n",
    "Fib_5_Low_Min: 0.89%\n",
    "5_day-Fib_50: 0.66%\n",
    "EMA_5: 0.49%\n",
    "SMA_5: 0.44%\n",
    "30_day_Fib_38: 0.42%\n",
    "5_day-Fib_23: 0.24%\n",
    "Std_Dev: 0.20%\n",
    "Cumulative_Price_Volume: 0.20%\n",
    "EMA_12_MACD: 0.08%\n",
    "SMA_20: 0.08%\n",
    "30_day_Fib_50: 0.06%\n",
    "VWAP: 0.05%\n",
    "30_day_Fib_61: 0.05%\n",
    "SMA_5_lag_7: 0.05%\n",
    "ATR_True_Range: 0.05%\n",
    "SMA_50: 0.05%\n",
    "ATR_Prev_Close: 0.05%\n",
    "Cumulative_Volume: 0.05%\n",
    "Fib_5_High_Max: 0.04%\n",
    "EMA_20: 0.04%\n",
    "ATR: 0.03%\n",
    "Upper_Band: 0.03%\n",
    "Fib_10_High_Max: 0.01%\n",
    "Lower_Band: 0.01%\n",
    "Volume_lag_1: 0.01%\n",
    "EMA_26_MACD: 0.01%\n",
    "EMA_50: 0.01%\n",
    "10_day_Fib_61: 0.01%\n",
    "\n",
    "\n",
    "1 month:\n",
    "\n",
    "Mean Squared Error on unseen data: 10844.724459170979\n",
    "Mean Absolute Error on unseen data: 15.639166248218757\n",
    "Root Mean Squared Error on unseen data: 104.13800679469037\n",
    "R-squared on unseen data: 0.9513849924746814\n",
    "Median Absolute Error on unseen data: 3.326946258544922\n",
    "Durbin-Watson Statistic on unseen data: 0.0750817215794377\n",
    "MAPE on unseen data: 7.82%\n",
    "\n",
    "30_day_Fib_23: 52.41%\n",
    "Fib_30_High_Max: 32.44%\n",
    "Fib_5_Low_Min: 2.25%\n",
    "Low: 2.08%\n",
    "10_day_Fib_23: 1.96%\n",
    "High: 1.77%\n",
    "Volume: 1.43%\n",
    "5_day-Fib_50: 1.11%\n",
    "30_day_Fib_50: 0.68%\n",
    "10_day_Fib_38: 0.62%\n",
    "5_day-Fib_23: 0.48%\n",
    "Fib_10_High_Max: 0.28%\n",
    "Fib_10_Low_Min: 0.25%\n",
    "Std_Dev: 0.21%\n",
    "VWAP: 0.21%\n",
    "ATR_Prev_Close: 0.20%\n",
    "EMA_5: 0.19%\n",
    "Fib_5_High_Max: 0.19%\n",
    "Cumulative_Price_Volume: 0.17%\n",
    "Lower_Band: 0.16%\n",
    "EMA_26_MACD: 0.14%\n",
    "30_day_Fib_61: 0.11%\n",
    "SMA_5: 0.11%\n",
    "SMA_20_lag_1: 0.06%\n",
    "Upper_Band: 0.05%\n",
    "5_day-Fib_61: 0.04%\n",
    "ATR_True_Range: 0.04%\n",
    "Cumulative_Volume: 0.04%\n",
    "30_day_Fib_38: 0.03%\n",
    "10_day_Fib_61: 0.03%\n",
    "EMA_50: 0.02%\n",
    "Fib_30_Low_Min: 0.02%\n",
    "ATR: 0.02%\n",
    "Volume_lag_1: 0.02%\n",
    "Close_lag_1: 0.02%\n",
    "EMA_20_lag_20: 0.02%\n",
    "SMA_50: 0.01%\n",
    "EMA_12_MACD_lag_15: 0.01%\n",
    "EMA_12_MACD_lag_12: 0.01%\n",
    "\n",
    "\n",
    "3 months:\n",
    "\n",
    "Mean Squared Error on unseen data: 25855.13009374476\n",
    "Mean Absolute Error on unseen data: 25.29591435606357\n",
    "Root Mean Squared Error on unseen data: 160.79530494931984\n",
    "R-squared on unseen data: 0.8883720519645201\n",
    "Median Absolute Error on unseen data: 6.283714294433594\n",
    "Durbin-Watson Statistic on unseen data: 0.017109218495924613\n",
    "MAPE on unseen data: 15.52%\n",
    "\n",
    "Fib_30_Low_Min: 27.17%\n",
    "30_day_Fib_38: 14.80%\n",
    "5_day-Fib_61: 11.17%\n",
    "Upper_Band: 8.25%\n",
    "Fib_10_Low_Min: 3.10%\n",
    "10_day_Fib_50: 2.76%\n",
    "Close_lag_1: 2.34%\n",
    "Volume: 1.87%\n",
    "EMA_5: 1.82%\n",
    "Fib_5_Low_Min: 1.69%\n",
    "Fib_5_High_Max: 1.51%\n",
    "30_day_Fib_23: 1.50%\n",
    "High: 1.48%\n",
    "EMA_50_lag_25: 1.37%\n",
    "30_day_Fib_50: 1.29%\n",
    "ATR_High_Low: 1.20%\n",
    "EMA_26_MACD: 1.11%\n",
    "EMA_50: 1.08%\n",
    "SMA_50: 1.02%\n",
    "ATR: 1.00%\n",
    "10_day_Fib_61: 0.87%\n",
    "Low: 0.86%\n",
    "EMA_12_MACD: 0.82%\n",
    "Close_lag_90: 0.76%\n",
    "5_day-Fib_23: 0.76%\n",
    "EMA_20: 0.73%\n",
    "SMA_20_lag_90: 0.60%\n",
    "EMA_12_MACD_lag_90: 0.58%\n",
    "10_day_Fib_38: 0.57%\n",
    "VWAP: 0.55%\n",
    "30_day_Fib_61: 0.54%\n",
    "Volume_lag_50: 0.53%\n",
    "ATR_True_Range: 0.39%\n",
    "ATR_Prev_Close: 0.32%\n",
    "SMA_5_lag_90: 0.31%\n",
    "EMA_50_lag_30: 0.30%\n",
    "Lower_Band: 0.28%\n",
    "Fib_30_High_Max: 0.24%\n",
    "Std_Dev: 0.23%\n",
    "Volume_lag_3: 0.16%\n",
    "Volume_lag_25: 0.15%\n",
    "SMA_50_lag_60: 0.13%\n",
    "EMA_26_MACD_lag_40: 0.13%\n",
    "Cumulative_Price_Volume: 0.13%\n",
    "EMA_50_lag_20: 0.12%\n",
    "Volume_lag_75: 0.11%\n",
    "EMA_12_MACD_lag_20: 0.10%\n",
    "SMA_50_lag_10: 0.07%\n",
    "Volume_lag_60: 0.07%\n",
    "EMA_20_lag_15: 0.06%\n",
    "EMA_50_lag_50: 0.05%\n",
    "SMA_50_lag_3: 0.05%\n",
    "EMA_5_lag_40: 0.04%\n",
    "SMA_50_lag_5: 0.04%\n",
    "EMA_50_lag_75: 0.04%\n",
    "Cumulative_Volume: 0.04%\n",
    "SMA_5_lag_60: 0.04%\n",
    "Fib_10_High_Max: 0.04%\n",
    "EMA_50_lag_60: 0.03%\n",
    "SMA_50_lag_30: 0.03%\n",
    "SMA_5_lag_25: 0.03%\n",
    "Volume_lag_90: 0.03%\n",
    "SMA_20_lag_25: 0.03%\n",
    "SMA_50_lag_7: 0.03%\n",
    "ATR_High_Close: 0.02%\n",
    "EMA_50_lag_90: 0.02%\n",
    "SMA_20_lag_40: 0.02%\n",
    "Signal_Line: 0.02%\n",
    "Volume_lag_1: 0.02%\n",
    "SMA_50_lag_20: 0.02%\n",
    "SMA_20_lag_30: 0.02%\n",
    "EMA_26_MACD_lag_50: 0.02%\n",
    "SMA_5_lag_50: 0.02%\n",
    "SMA_50_lag_15: 0.02%\n",
    "Volume_lag_40: 0.02%\n",
    "SMA_50_lag_25: 0.01%\n",
    "EMA_50_lag_15: 0.01%\n",
    "EMA_12_MACD_lag_40: 0.01%\n",
    "MACD: 0.01%\n",
    "SMA_20_lag_20: 0.01%\n",
    "EMA_12_MACD_lag_15: 0.01%\n",
    "Volume_lag_5: 0.01%\n",
    "SMA_5: 0.01%\n",
    "SMA_5_lag_15: 0.01%\n",
    "EMA_26_MACD_lag_30: 0.01%\n",
    "Close_lag_20: 0.01%\n",
    "EMA_20_lag_50: 0.01%\n",
    "Close_lag_25: 0.01%\n",
    "Volume_lag_20: 0.01%\n",
    "Close_lag_40: 0.01%\n",
    "EMA_5_lag_50: 0.01%\n",
    "SMA_5_lag_40: 0.01%\n",
    "SMA_50_lag_90: 0.01%\n",
    "5_day-Fib_50: 0.01%\n",
    "SMA_50_lag_40: 0.01%\n",
    "EMA_5_lag_75: 0.01%\n",
    "SMA_20_lag_50: 0.01%\n",
    "Close_lag_10: 0.01%\n",
    "EMA_12_MACD_lag_5: 0.01%\n",
    "\n",
    "\n",
    "6 months:\n",
    "\n",
    "Mean Squared Error on unseen data: 31411.174267588787\n",
    "Mean Absolute Error on unseen data: 34.84146322021573\n",
    "Root Mean Squared Error on unseen data: 177.23197868214638\n",
    "R-squared on unseen data: 0.871485858716382\n",
    "Median Absolute Error on unseen data: 8.994926452636719\n",
    "Durbin-Watson Statistic on unseen data: 0.020845241129731375\n",
    "MAPE on unseen data: 21.51%\n",
    "\n",
    "Fib_10_High_Max: 17.93%\n",
    "Fib_30_High_Max: 13.03%\n",
    "Fib_5_Low_Min: 12.74%\n",
    "ATR_High_Low: 5.86%\n",
    "10_day_Fib_50: 4.75%\n",
    "EMA_50_lag_25: 4.42%\n",
    "EMA_5: 2.79%\n",
    "Middle_Band: 2.77%\n",
    "Volume: 2.74%\n",
    "EMA_12_MACD: 2.23%\n",
    "EMA_50: 2.11%\n",
    "Fib_5_High_Max: 2.10%\n",
    "5_day-Fib_61: 1.93%\n",
    "Low: 1.78%\n",
    "High: 1.60%\n",
    "30_day_Fib_61: 1.47%\n",
    "VWAP: 1.45%\n",
    "Fib_30_Low_Min: 1.18%\n",
    "EMA_26_MACD_lag_40: 1.09%\n",
    "SMA_50: 1.05%\n",
    "Fib_10_Low_Min: 1.02%\n",
    "SMA_50_lag_90: 1.01%\n",
    "SMA_5: 0.98%\n",
    "EMA_50_lag_30: 0.97%\n",
    "ATR: 0.92%\n",
    "SMA_50_lag_20: 0.76%\n",
    "EMA_20: 0.69%\n",
    "Close_lag_90: 0.65%\n",
    "5_day-Fib_23: 0.58%\n",
    "SMA_50_lag_25: 0.55%\n",
    "ATR_Prev_Close: 0.54%\n",
    "Lower_Band: 0.53%\n",
    "Std_Dev: 0.43%\n",
    "ATR_True_Range: 0.38%\n",
    "Volume_lag_75: 0.35%\n",
    "SMA_5_lag_60: 0.31%\n",
    "30_day_Fib_23: 0.29%\n",
    "EMA_12_MACD_lag_3: 0.21%\n",
    "10_day_Fib_38: 0.21%\n",
    "Volume_lag_60: 0.20%\n",
    "SMA_50_lag_75: 0.20%\n",
    "Volume_lag_1: 0.18%\n",
    "EMA_5_lag_75: 0.17%\n",
    "SMA_50_lag_3: 0.17%\n",
    "Close_lag_180: 0.17%\n",
    "Cumulative_Price_Volume: 0.16%\n",
    "Volume_lag_50: 0.15%\n",
    "5_day-Fib_50: 0.15%\n",
    "EMA_50_lag_1: 0.14%\n",
    "EMA_50_lag_90: 0.13%\n",
    "Volume_lag_30: 0.13%\n",
    "10_day_Fib_61: 0.12%\n",
    "EMA_50_lag_40: 0.09%\n",
    "EMA_20_lag_180: 0.08%\n",
    "SMA_50_lag_40: 0.08%\n",
    "Volume_lag_25: 0.08%\n",
    "EMA_50_lag_75: 0.06%\n",
    "Volume_lag_3: 0.05%\n",
    "Cumulative_Volume: 0.04%\n",
    "Volume_lag_5: 0.04%\n",
    "EMA_20_lag_90: 0.04%\n",
    "Signal_Line: 0.04%\n",
    "EMA_50_lag_180: 0.04%\n",
    "Volume_lag_40: 0.04%\n",
    "EMA_26_MACD: 0.03%\n",
    "EMA_26_MACD_lag_180: 0.03%\n",
    "Upper_Band: 0.03%\n",
    "EMA_12_MACD_lag_180: 0.03%\n",
    "EMA_20_lag_3: 0.03%\n",
    "EMA_5_lag_40: 0.03%\n",
    "EMA_12_MACD_lag_40: 0.03%\n",
    "SMA_50_lag_180: 0.02%\n",
    "SMA_20_lag_15: 0.02%\n",
    "Volume_lag_20: 0.02%\n",
    "SMA_20_lag_30: 0.02%\n",
    "EMA_12_MACD_lag_25: 0.02%\n",
    "EMA_5_lag_60: 0.02%\n",
    "SMA_5_lag_7: 0.02%\n",
    "Volume_lag_180: 0.02%\n",
    "EMA_5_lag_50: 0.02%\n",
    "EMA_50_lag_15: 0.02%\n",
    "SMA_50_lag_50: 0.02%\n",
    "SMA_20_lag_60: 0.02%\n",
    "SMA_20_lag_40: 0.02%\n",
    "EMA_26_MACD_lag_10: 0.01%\n",
    "Close_lag_20: 0.01%\n",
    "5_day-Fib_38: 0.01%\n",
    "Close_lag_50: 0.01%\n",
    "MACD: 0.01%\n",
    "EMA_20_lag_40: 0.01%\n",
    "Close_lag_10: 0.01%\n",
    "EMA_20_lag_50: 0.01%\n",
    "SMA_5_lag_30: 0.01%\n",
    "SMA_50_lag_10: 0.01%\n",
    "Volume_lag_10: 0.01%\n",
    "MACD_Histogram: 0.01%\n",
    "EMA_50_lag_20: 0.01%\n",
    "RSI: 0.01%\n",
    "10_day_Fib_23: 0.01%\n",
    "%D: 0.01%\n",
    "Close_lag_5: 0.01%\n",
    "Volume_lag_90: 0.01%\n",
    "Volume_lag_15: 0.01%\n",
    "SMA_20_lag_3: 0.01%\n",
    "EMA_20_lag_15: 0.01%\n",
    "SMA_5_lag_90: 0.01%\n",
    "EMA_12_MACD_lag_50: 0.01%\n",
    "SMA_20_lag_75: 0.01%\n",
    "EMA_20_lag_75: 0.01%\n",
    "EMA_50_lag_60: 0.01%\n",
    "SMA_50_lag_1: 0.01%\n",
    "SMA_50_lag_60: 0.01%\n",
    "EMA_26_MACD_lag_60: 0.01%\n",
    "Close_lag_1: 0.01%\n",
    "Volume_lag_7: 0.01%\n",
    "SMA_20_lag_90: 0.01%\n",
    "Close_lag_75: 0.01%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f93f01-f1de-4a9f-934c-268d7f07bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root mean squared error graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define time horizons and RMSE values\n",
    "time_horizons = [\"1 Week\", \"1 Month\", \"3 Months\", \"6 Months\"]\n",
    "rmse_values = [rmse_1_week_md_7, rmse_1_month_md_7, rmse_3_month_md_3, rmse_6_month_md_3]\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(time_horizons, rmse_values, marker='o', linestyle='-', color='b', linewidth=2, markersize=8)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Prediction Horizon\")\n",
    "plt.ylabel(\"Root Mean Squared Error (RMSE)\")\n",
    "plt.title(\"RMSE Across Different Prediction Timelines\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Display plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6095d-cb9d-43ac-9edf-515b8024d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r squared graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define time horizons and R-squared values\n",
    "time_horizons = [\"1 Week\", \"1 Month\", \"3 Months\", \"6 Months\"]\n",
    "r_squared_values = [r2_1_week_md_7, r2_1_month_md_7, r2_3_month_md_3, r2_6_month_md_3]\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(time_horizons, r_squared_values, marker='o', linestyle='-', color='g', linewidth=2, markersize=8)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Prediction Horizon\")\n",
    "plt.ylabel(\"R-squared\")\n",
    "plt.title(\"R-squared Across Different Prediction Timelines\")\n",
    "plt.ylim(0.85, 1.0)  # Setting limits for better visualization\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Display plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a011625-fa36-4063-8140-728f7ae6c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define time horizons and MAPE values\n",
    "time_horizons = [\"1 Week\", \"1 Month\", \"3 Months\", \"6 Months\"]\n",
    "mape_values = [mape_1_week_md_7, mape_1_month_md_7, mape_3_month_md_3, mape_6_month_md_3]\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(time_horizons, mape_values, marker='o', linestyle='-', color='r', linewidth=2, markersize=8)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Prediction Horizon\")\n",
    "plt.ylabel(\"MAPE (%)\")\n",
    "plt.title(\"Mean Absolute Percentage Error (MAPE) Over Different Prediction Horizons\")\n",
    "plt.ylim(0, 30)  # Adjusted for better visualization\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Display plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdb9b9-0380-4efd-8729-6179a4431923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median Absolute Error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define time horizons and Median Absolute Error values\n",
    "time_horizons = [\"1 Week\", \"1 Month\", \"3 Months\", \"6 Months\"]\n",
    "medae_values = [medae_1_week_md_7, medae_1_month_md_7, medae_3_month_md_3, medae_6_month_md_3]\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(time_horizons, medae_values, marker='o', linestyle='-', color='b', linewidth=2, markersize=8)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Prediction Horizon\")\n",
    "plt.ylabel(\"Median Absolute Error\")\n",
    "plt.title(\"Median Absolute Error Over Different Prediction Horizons\")\n",
    "plt.ylim(0, 15)  # Adjusted for better visualization\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Display plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892988f-54e9-4307-96a0-358e7c30bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Week Price Prediction: Available Testing Dates\n",
    "# Prints the first available date and the last available date \n",
    "# where 'Close' and 'Close_Target' values exist for a given stock ticker.\n",
    "\n",
    "def print_stock_date_range(df_test, symbol):\n",
    " \n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "    \n",
    "    # Filter DataFrame for the given symbol and ensure 'Close' and 'Close_Target' are not NaN\n",
    "    df_filtered = df_test[(df_test['Symbol'] == symbol) & \n",
    "                          (df_test['Close'].notna()) & \n",
    "                          (df_test['Close_Target'].notna())].reset_index(drop=True)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No available data for symbol: {symbol}\")\n",
    "        return\n",
    "\n",
    "    # Extract first and last available dates\n",
    "    first_date = df_filtered['Date'].min()\n",
    "    last_date = df_filtered['Date'].max()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Symbol: {symbol}\"),\n",
    "    print(f\"First Available Date: {first_date.strftime('%Y-%m-%d')}\"),\n",
    "    print(f\"Last Available Date: {last_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_date_range(df_stock_data_test_1_week_md_7, 'NVDA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0e8dc-a9d3-42fa-81bc-e9d71650621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Week Price Prediction\n",
    "# Prints the stock symbol, specified date, actual close price ('Close'), \n",
    "# and predicted price ('Close_Target'). If the date is not available, \n",
    "# it finds the next available future date. If the entered date is the last date, \n",
    "# it looks for the closest previous available date.\n",
    "import pandas as pd\n",
    "\n",
    "def print_stock_prediction_by_date(df_test, symbol, date):\n",
    "\n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "    \n",
    "    # Convert date input to datetime for accurate comparisons\n",
    "    date = pd.to_datetime(date)\n",
    "\n",
    "    # Filter DataFrame for the given stock symbol\n",
    "    df_filtered = df_test[df_test['Symbol'] == symbol].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data available for symbol: {symbol}\")\n",
    "        return\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_filtered['Date'] = pd.to_datetime(df_filtered['Date'])\n",
    "\n",
    "    # Ensure sorting by date for correct traversal\n",
    "    df_filtered = df_filtered.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    # Try to find the exact date\n",
    "    if date in df_filtered['Date'].values:\n",
    "        closest_date = date\n",
    "    else:\n",
    "        # Find the next available future date\n",
    "        future_dates = df_filtered[df_filtered['Date'] > date]\n",
    "        if not future_dates.empty:\n",
    "            closest_date = future_dates['Date'].iloc[0]  # Next available future date\n",
    "        else:\n",
    "            # If no future date exists, get the closest past date\n",
    "            past_dates = df_filtered[df_filtered['Date'] < date]\n",
    "            if not past_dates.empty:\n",
    "                closest_date = past_dates['Date'].iloc[-1]  # Last available past date\n",
    "            else:\n",
    "                print(f\"No available dates found for symbol: {symbol}\")\n",
    "                return\n",
    "\n",
    "    # Get row for the closest available date\n",
    "    row = df_filtered[df_filtered['Date'] == closest_date].iloc[0]\n",
    "    actual_close = row['Close']\n",
    "    predicted_price = row['Close_Target']\n",
    "\n",
    "    # Calculate the future date (120 trading days = 6 months)\n",
    "    future_date = closest_date + pd.DateOffset(days=7)\n",
    "    \n",
    "    # To ensure it represents 120 trading days, we might need to filter out weekends\n",
    "    # and filter out time\n",
    "    future_trading_date = future_date\n",
    "    trading_days = pd.date_range(closest_date, future_date, freq='B')  # 'B' for business days (weekdays)\n",
    "    future_trading_date = (trading_days[-1] if len(trading_days) > 0 else future_date).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    #Calculate percent error of actual price vs. predicted price\n",
    "    percent_error = ((abs(actual_close - predicted_price)) / abs(actual_close)) * 100\n",
    "    formatted_percent_error = f'{percent_error:.2f}%'\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Symbol: {symbol}\")\n",
    "    print(f\"Date: {closest_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Close Price: {actual_close:.2f}\")\n",
    "    print(f\"Predicted Price Date: {future_trading_date}\")\n",
    "    print(f\"Predicted Price: {predicted_price:.2f}\")\n",
    "    print(f\"Percent Error: {formatted_percent_error}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_prediction_by_date(df_stock_data_test_1_week_md_7, 'NVDA', '2024-03-21')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b863a-125b-44d7-853d-25aa5228770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_data_test_1_week_md_3[1990:1995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d06c7-63ec-4579-ab9c-2924e85c49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Month Price Prediction: Available Testing Dates\n",
    "# Prints the first available date and the last available date \n",
    "# where 'Close' and 'Close_Target' values exist for a given stock ticker.\n",
    "\n",
    "def print_stock_date_range(df_test, symbol):\n",
    " \n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "\n",
    "    # Ensure the 'Date' column is in datetime format\n",
    "    df_test['Date'] = pd.to_datetime(df_test['Date']) \n",
    "    \n",
    "    # Filter DataFrame for the given symbol and ensure 'Close' and 'Close_Target' are not NaN\n",
    "    df_filtered = df_test[(df_test['Symbol'] == symbol) & \n",
    "                          (df_test['Close'].notna()) & \n",
    "                          (df_test['Close_Target'].notna())].reset_index(drop=True)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No available data for symbol: {symbol}\")\n",
    "        return\n",
    "\n",
    "    # Extract first and last available dates\n",
    "    first_date = df_filtered['Date'].min()\n",
    "    last_date = df_filtered['Date'].max()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Symbol: {symbol}\"),\n",
    "    print(f\"First Available Date: {first_date.strftime('%Y-%m-%d')}\"),\n",
    "    print(f\"Last Available Date: {last_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_date_range(df_stock_data_test_1_month_md_7, 'AAPL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f155f-03b7-4034-8739-9edf87492b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Month Price Prediction\n",
    "# Prints the stock symbol, specified date, actual close price ('Close'), \n",
    "# and predicted price ('Close_Target'). If the date is not available, \n",
    "# it finds the next available future date. If the entered date is the last date, \n",
    "# it looks for the closest previous available date.\n",
    "import pandas as pd\n",
    "\n",
    "def print_stock_prediction_by_date(df_test, symbol, date):\n",
    "\n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "    \n",
    "    # Convert date input to datetime for accurate comparisons\n",
    "    date = pd.to_datetime(date)\n",
    "\n",
    "    # Filter DataFrame for the given stock symbol\n",
    "    df_filtered = df_test[df_test['Symbol'] == symbol].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data available for symbol: {symbol}\")\n",
    "        return\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_filtered['Date'] = pd.to_datetime(df_filtered['Date'])\n",
    "\n",
    "    # Ensure sorting by date for correct traversal\n",
    "    df_filtered = df_filtered.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    # Try to find the exact date\n",
    "    if date in df_filtered['Date'].values:\n",
    "        closest_date = date\n",
    "    else:\n",
    "        # Find the next available future date\n",
    "        future_dates = df_filtered[df_filtered['Date'] > date]\n",
    "        if not future_dates.empty:\n",
    "            closest_date = future_dates['Date'].iloc[0]  # Next available future date\n",
    "        else:\n",
    "            # If no future date exists, get the closest past date\n",
    "            past_dates = df_filtered[df_filtered['Date'] < date]\n",
    "            if not past_dates.empty:\n",
    "                closest_date = past_dates['Date'].iloc[-1]  # Last available past date\n",
    "            else:\n",
    "                print(f\"No available dates found for symbol: {symbol}\")\n",
    "                return\n",
    "\n",
    "    # Get row for the closest available date\n",
    "    row = df_filtered[df_filtered['Date'] == closest_date].iloc[0]\n",
    "    actual_close = row['Close']\n",
    "    predicted_price = row['Close_Target']\n",
    "\n",
    "    # Calculate the future date (120 trading days = 6 months)\n",
    "    future_date = closest_date + pd.DateOffset(days=120)\n",
    "    \n",
    "    # To ensure it represents 120 trading days, we might need to filter out weekends\n",
    "    # and filter out time\n",
    "    future_trading_date = future_date\n",
    "    trading_days = pd.date_range(closest_date, future_date, freq='B')  # 'B' for business days (weekdays)\n",
    "    future_trading_date = (trading_days[-1] if len(trading_days) > 0 else future_date).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    #Calculate percent error of actual price vs. predicted price\n",
    "    percent_error = ((abs(actual_close - predicted_price)) / abs(actual_close)) * 100\n",
    "    formatted_percent_error = f'{percent_error:.2f}%'\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Symbol: {symbol}\")\n",
    "    print(f\"Date: {closest_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Close Price: {actual_close:.2f}\")\n",
    "    print(f\"Predicted Price Date: {future_trading_date}\")\n",
    "    print(f\"Predicted Price: {predicted_price:.2f}\")\n",
    "    print(f\"Percent Error: {formatted_percent_error}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_prediction_by_date(df_stock_data_test_1_month_md_3, 'MSFT', '2024-02-09')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05a66d-1af3-49d4-9d0f-a6621195d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Month Price Prediction: Available Testing Dates\n",
    "# Prints the first available date and the last available date \n",
    "# where 'Close' and 'Close_Target' values exist for a given stock ticker.\n",
    "\n",
    "def print_stock_date_range(df_test, symbol):\n",
    " \n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Filter DataFrame for the given symbol and ensure 'Close' and 'Close_Target' are not NaN\n",
    "    df_filtered = df_test[(df_test['Symbol'] == symbol) & \n",
    "                          (df_test['Close'].notna()) & \n",
    "                          (df_test['Close_Target'].notna())].reset_index(drop=True)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No available data for symbol: {symbol}\")\n",
    "        return\n",
    "\n",
    "    # Extract first and last available dates\n",
    "    first_date = df_filtered['Date'].min()\n",
    "    last_date = df_filtered['Date'].max()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Symbol: {symbol}\"),\n",
    "    print(f\"First Available Date: {first_date.strftime('%Y-%m-%d')}\"),\n",
    "    print(f\"Last Available Date: {last_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_date_range(df_stock_data_test_3_month_md_3, 'AAPL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75b408-60d7-4d08-9f84-0a689e28a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Month Price Prediction\n",
    "# Prints the stock symbol, specified date, actual close price ('Close'), \n",
    "# and predicted price ('Close_Target'). If the date is not available, \n",
    "# it finds the next available future date. If the entered date is the last date, \n",
    "# it looks for the closest previous available date.\n",
    "import pandas as pd\n",
    "\n",
    "def print_stock_prediction_by_date(df_test, symbol, date):\n",
    "\n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "    \n",
    "    # Convert date input to datetime for accurate comparisons\n",
    "    date = pd.to_datetime(date)\n",
    "\n",
    "    # Filter DataFrame for the given stock symbol\n",
    "    df_filtered = df_test[df_test['Symbol'] == symbol].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data available for symbol: {symbol}\")\n",
    "        return\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_filtered['Date'] = pd.to_datetime(df_filtered['Date'])\n",
    "\n",
    "    # Ensure sorting by date for correct traversal\n",
    "    df_filtered = df_filtered.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    # Try to find the exact date\n",
    "    if date in df_filtered['Date'].values:\n",
    "        closest_date = date\n",
    "    else:\n",
    "        # Find the next available future date\n",
    "        future_dates = df_filtered[df_filtered['Date'] > date]\n",
    "        if not future_dates.empty:\n",
    "            closest_date = future_dates['Date'].iloc[0]  # Next available future date\n",
    "        else:\n",
    "            # If no future date exists, get the closest past date\n",
    "            past_dates = df_filtered[df_filtered['Date'] < date]\n",
    "            if not past_dates.empty:\n",
    "                closest_date = past_dates['Date'].iloc[-1]  # Last available past date\n",
    "            else:\n",
    "                print(f\"No available dates found for symbol: {symbol}\")\n",
    "                return\n",
    "\n",
    "    # Get row for the closest available date\n",
    "    row = df_filtered[df_filtered['Date'] == closest_date].iloc[0]\n",
    "    actual_close = row['Close']\n",
    "    predicted_price = row['Close_Target']\n",
    "\n",
    "    # Calculate the future date (120 trading days = 6 months)\n",
    "    future_date = closest_date + pd.DateOffset(days=120)\n",
    "    \n",
    "    # To ensure it represents 120 trading days, we might need to filter out weekends\n",
    "    # and filter out time\n",
    "    future_trading_date = future_date\n",
    "    trading_days = pd.date_range(closest_date, future_date, freq='B')  # 'B' for business days (weekdays)\n",
    "    future_trading_date = (trading_days[-1] if len(trading_days) > 0 else future_date).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    #Calculate percent error of actual price vs. predicted price\n",
    "    percent_error = ((abs(actual_close - predicted_price)) / abs(actual_close)) * 100\n",
    "    formatted_percent_error = f'{percent_error:.2f}%'\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Symbol: {symbol}\")\n",
    "    print(f\"Date: {closest_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Close Price: {actual_close:.2f}\")\n",
    "    print(f\"Predicted Price Date: {future_trading_date}\")\n",
    "    print(f\"Predicted Price: {predicted_price:.2f}\")\n",
    "    print(f\"Percent Error: {formatted_percent_error}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_prediction_by_date(df_stock_data_test_3_month_md_3, 'AAPL', '2024-02-09')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113decb-1636-4a34-822c-0753eea942f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Month Price Prediction: Available Testing Dates\n",
    "# Prints the first available date and the last available date \n",
    "# where 'Close' and 'Close_Target' values exist for a given stock ticker.\n",
    "\n",
    "def print_stock_date_range(df_test, symbol):\n",
    " \n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "\n",
    "    # Ensure the 'Date' column is in datetime format\n",
    "    df_test['Date'] = pd.to_datetime(df_test['Date']) \n",
    "    \n",
    "    # Filter DataFrame for the given symbol and ensure 'Close' and 'Close_Target' are not NaN\n",
    "    df_filtered = df_test[(df_test['Symbol'] == symbol) & \n",
    "                          (df_test['Close'].notna()) & \n",
    "                          (df_test['Close_Target'].notna())].reset_index(drop=True)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No available data for symbol: {symbol}\")\n",
    "        return\n",
    "\n",
    "    # Extract first and last available dates\n",
    "    first_date = df_filtered['Date'].min()\n",
    "    last_date = df_filtered['Date'].max()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Symbol: {symbol}\"),\n",
    "    print(f\"First Available Date: {first_date.strftime('%Y-%m-%d')}\"),\n",
    "    print(f\"Last Available Date: {last_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_date_range(df_stock_data_test_6_month_md_3, 'MSFT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81b6db-3b38-4d60-a6e4-185853e6732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Month Price Prediction\n",
    "# Prints the stock symbol, specified date, actual close price ('Close'), \n",
    "# and predicted price ('Close_Target'). If the date is not available, \n",
    "# it finds the next available future date. If the entered date is the last date, \n",
    "# it looks for the closest previous available date.\n",
    "import pandas as pd\n",
    "\n",
    "def print_stock_prediction_by_date(df_test, symbol, date):\n",
    "\n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "    \n",
    "    # Convert date input to datetime for accurate comparisons\n",
    "    date = pd.to_datetime(date)\n",
    "\n",
    "    # Filter DataFrame for the given stock symbol\n",
    "    df_filtered = df_test[df_test['Symbol'] == symbol].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data available for symbol: {symbol}\")\n",
    "        return\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_filtered['Date'] = pd.to_datetime(df_filtered['Date'])\n",
    "\n",
    "    # Ensure sorting by date for correct traversal\n",
    "    df_filtered = df_filtered.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    # Try to find the exact date\n",
    "    if date in df_filtered['Date'].values:\n",
    "        closest_date = date\n",
    "    else:\n",
    "        # Find the next available future date\n",
    "        future_dates = df_filtered[df_filtered['Date'] > date]\n",
    "        if not future_dates.empty:\n",
    "            closest_date = future_dates['Date'].iloc[0]  # Next available future date\n",
    "        else:\n",
    "            # If no future date exists, get the closest past date\n",
    "            past_dates = df_filtered[df_filtered['Date'] < date]\n",
    "            if not past_dates.empty:\n",
    "                closest_date = past_dates['Date'].iloc[-1]  # Last available past date\n",
    "            else:\n",
    "                print(f\"No available dates found for symbol: {symbol}\")\n",
    "                return\n",
    "\n",
    "    # Get row for the closest available date\n",
    "    row = df_filtered[df_filtered['Date'] == closest_date].iloc[0]\n",
    "    actual_close = row['Close']\n",
    "    predicted_price = row['Close_Target']\n",
    "\n",
    "    # Calculate the future date (120 trading days = 6 months)\n",
    "    future_date = closest_date + pd.DateOffset(days=120)\n",
    "    \n",
    "    # To ensure it represents 120 trading days, we might need to filter out weekends\n",
    "    # and filter out time\n",
    "    future_trading_date = future_date\n",
    "    trading_days = pd.date_range(closest_date, future_date, freq='B')  # 'B' for business days (weekdays)\n",
    "    future_trading_date = (trading_days[-1] if len(trading_days) > 0 else future_date).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    #Calculate percent error of actual price vs. predicted price\n",
    "    percent_error = ((abs(actual_close - predicted_price)) / abs(actual_close)) * 100\n",
    "    formatted_percent_error = f'{percent_error:.2f}%'\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Symbol: {symbol}\")\n",
    "    print(f\"Date: {closest_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Close Price: {actual_close:.2f}\")\n",
    "    print(f\"Predicted Price Date: {future_trading_date}\")\n",
    "    print(f\"Predicted Price: {predicted_price:.2f}\")\n",
    "    print(f\"Percent Error: {formatted_percent_error}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_prediction_by_date(df_stock_data_test_6_month_md_3, 'AAPL', '2024-02-09')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40d771-c51b-437c-b9fe-772ebdd07c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned[1500:1505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f85c6-9f05-472e-b63d-98945d413315",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
