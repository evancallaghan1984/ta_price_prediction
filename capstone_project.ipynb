{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce9d5af-0fda-41db-a2fc-22a5d7e0f586",
   "metadata": {},
   "source": [
    "# Technical Analysis Indicator Price Prediction\n",
    "The goal of this project is to analyze the predictive power of the top 10 most popular TA indicators and see how well they do to predict price over a 30 day period. I am going to find the value of the indicators on day 1 (30 trading days ago) and then find the daily closing price for 30 days later and measure how well the indicator predicted the price.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b635c3-9842-408e-9f61-0c982d9c49a2",
   "metadata": {},
   "source": [
    "first we'll find the top 500 stocks by market cap from nasdaq and pull them into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e2975-7d47-4313-93a9-26bcc74988d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas library for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = '/Users/evancallaghan/Downloads/nasdaq_screener_1726538993372.csv' \n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Inspect the DataFrame to understand its structure\n",
    "print(df.head())\n",
    "\n",
    "# Filter DataFrame to only show the columns 'Symbol', 'Name', and 'Market Cap'\n",
    "df = df[['Symbol', 'Name', 'Market Cap']]\n",
    "\n",
    "# Convert 'Market Cap' to numeric if it's not already\n",
    "# Remove commas, dollar signs, and replace these symbols with empty spaces\n",
    "df['Market Cap'] = df['Market Cap'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "\n",
    "# Sort the DataFrame by Market Cap in descending order\n",
    "df_sorted = df.sort_values(by='Market Cap', ascending=False).head(1000)                                                                        \n",
    "df_sorted.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434eced7-138d-4dbe-a4d3-c9c92f32fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the DataFrame and drop the old index\n",
    "df_sorted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update the index to start from 1 instead of 0\n",
    "df_sorted.index = df_sorted.index + 1\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674b213-6fd9-428b-aa96-56497343cb2f",
   "metadata": {},
   "source": [
    "remove all stocks except common stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fe5b5-1215-423f-9acf-fbcbae90e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no leading or trailing whitespaces in the 'Name' column\n",
    "df_sorted['Name'] = df_sorted['Name'].str.strip()\n",
    "\n",
    "# List of terms to filter out\n",
    "terms_to_drop = [\"Capital Stock\", \"Depository Shares\", \"Global Notes\", \"ADS\", \n",
    "                 \"Registry Shares\", \"Depositary Shares\"\n",
    "]\n",
    "\n",
    "# Create a regex pattern to match any of the terms\n",
    "# //b ensures that the match occues only at the start or end of a word\n",
    "# pipe '|' ensures that if any of the terms in 'terms_to_drop' are seen, \n",
    "# there is a match\n",
    "pattern = '|'.join([f\"\\\\b{term}\\\\b\" for term in terms_to_drop])\n",
    "\n",
    "# Apply filtering based on the updated pattern\n",
    "df_filtered = df_sorted[~df_sorted['Name'].str.contains(pattern, case=False, \n",
    "                                                        na=False)\n",
    "]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41bc92-091c-4d0d-9ad7-c143c72bd072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the DataFrame and drop the old index\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update the index to start from 1 instead of 0\n",
    "df_filtered.index = df_filtered.index + 1\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ae08f-7207-4e72-94bc-beb65e8f66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[595:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd41b6a-5b30-413e-9090-5e6a9e99e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f48fa-2e14-4f6e-ae73-f451fa6fd1be",
   "metadata": {},
   "source": [
    "below are the 10 technical indicators we are going to use for this project.\n",
    "1. Relative Strength Index (RSI)\n",
    "2. Moving Average Convergence Divergence (MACD)\n",
    "3. Stochastic Oscillator\n",
    "4. Simple Moving Average (SMA)\n",
    "5. Exponential Moving Average (EMA)\n",
    "6. Volume Weighted Average Price (VWAP)\n",
    "7. Bollinger Bands\n",
    "8. Average True Range (ATR)\n",
    "9. Fibonacci Retracement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972bb04-97c8-4a0a-90c4-9e7f5e7fad8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10 year historical data top 600 stocks\n",
    "# Pulls data from yahoo finance into CSV files\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to download stock data for a single stock\n",
    "def download_stock_data(ticker, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Add a delay to avoid rate limiting\n",
    "            time.sleep(1)  # Increase delay to 2 seconds between requests\n",
    "            \n",
    "            print(f\"Downloading data for {ticker}, attempt {attempt + 1}\")\n",
    "            data = yf.download(ticker, start=\"2015-02-10\", end=\"2025-02-17\", interval=\"1d\")[['Close', 'High', 'Low', 'Volume']]\n",
    "            \n",
    "            if data.empty:\n",
    "                print(f\"Warning: No data found for {ticker}\")\n",
    "                return None  # Return None if data is empty\n",
    "\n",
    "            # Explicitly add 'Date' as a column before resetting the index\n",
    "            data['Date'] = data.index\n",
    "\n",
    "            # Reset the index and make 'Date' a normal column\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            data['Ticker'] = ticker\n",
    "            print(f\"Downloaded data for {ticker}:\\n{data.head()}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading data for {ticker}: {e}\")\n",
    "            time.sleep(1)  # Longer delay before retrying in case of failure\n",
    "    return None  # Return None after retries if still fails\n",
    "\n",
    "# List of tickers from your df_filtered dataframe\n",
    "tickers = df_filtered['Symbol'].head(600).astype(str).tolist()  # Ensure tickers are strings\n",
    "\n",
    "# Batch size for processing tickers in smaller chunks\n",
    "batch_size = 100  # Reduce batch size to avoid rate limits\n",
    "\n",
    "# Directory to save CSV files\n",
    "output_dir = \"/Users/evancallaghan/flatiron_ds/phase_5/capstone_project\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a function to download data for a batch of tickers in parallel\n",
    "def download_batch(batch_tickers, batch_index):\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:  # Use fewer threads to reduce load\n",
    "        results = list(executor.map(lambda ticker: download_stock_data(ticker), batch_tickers))\n",
    "\n",
    "    # Remove None values and ensure we have valid data\n",
    "    valid_results = [(batch_tickers[i], results[i]) for i in range(len(batch_tickers)) if results[i] is not None]\n",
    "    \n",
    "    # Add ticker info to the valid results\n",
    "    for ticker, df in valid_results:\n",
    "        df['Ticker'] = ticker  # Explicitly add a column for ticker\n",
    "    \n",
    "    # Combine all the valid stock data into a single DataFrame\n",
    "    if valid_results:\n",
    "        df_batch = pd.concat([df for _, df in valid_results], ignore_index=False)  # Don't lose index info\n",
    "        print(f\"Saving batch {batch_index} data to CSV.\")\n",
    "        # Save the batch to a CSV file\n",
    "        df_batch.to_csv(f\"{output_dir}/top600_10yr_stock_price_data_{batch_index}.csv\", index=False)\n",
    "    else:\n",
    "        print(f\"No data downloaded for batch {batch_index}.\")\n",
    "\n",
    "# Split tickers into batches\n",
    "for i in range(0, len(tickers), batch_size):\n",
    "    batch_tickers = tickers[i:i + batch_size]\n",
    "    batch_index = (i // batch_size) + 1  # Batch index starts from 1\n",
    "    download_batch(batch_tickers, batch_index)\n",
    "\n",
    "print(\"All batches processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab381910-c48d-4d2d-9426-96842e88517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if we will use thiis\n",
    "\n",
    "# 5 year historical data top 600 stocks\n",
    "# Pulls data from yahoo finance into CSV files\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to download stock data for a single stock\n",
    "def download_stock_data(ticker, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Add a delay to avoid rate limiting\n",
    "            time.sleep(1)  # Increase delay to 2 seconds between requests\n",
    "            \n",
    "            print(f\"Downloading data for {ticker}, attempt {attempt + 1}\")\n",
    "            data = yf.download(ticker, start=\"2020-02-10\", end=\"2025-02-17\", interval=\"1d\")[['Close', 'High', 'Low', 'Volume']]\n",
    "            \n",
    "            if data.empty:\n",
    "                print(f\"Warning: No data found for {ticker}\")\n",
    "                return None  # Return None if data is empty\n",
    "\n",
    "            # Explicitly add 'Date' as a column before resetting the index\n",
    "            data['Date'] = data.index\n",
    "\n",
    "            # Reset the index and make 'Date' a normal column\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            data['Ticker'] = ticker\n",
    "            print(f\"Downloaded data for {ticker}:\\n{data.head()}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading data for {ticker}: {e}\")\n",
    "            time.sleep(1)  # Longer delay before retrying in case of failure\n",
    "    return None  # Return None after retries if still fails\n",
    "\n",
    "# List of tickers from your df_filtered dataframe\n",
    "tickers = df_filtered['Symbol'].head(600).astype(str).tolist()  # Ensure tickers are strings\n",
    "\n",
    "# Batch size for processing tickers in smaller chunks\n",
    "batch_size = 100  # Reduce batch size to avoid rate limits\n",
    "\n",
    "# Directory to save CSV files\n",
    "output_dir = \"/Users/evancallaghan/flatiron_ds/phase_5/capstone_project\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a function to download data for a batch of tickers in parallel\n",
    "def download_batch(batch_tickers, batch_index):\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:  # Use fewer threads to reduce load\n",
    "        results = list(executor.map(lambda ticker: download_stock_data(ticker), batch_tickers))\n",
    "\n",
    "    # Remove None values and ensure we have valid data\n",
    "    valid_results = [(batch_tickers[i], results[i]) for i in range(len(batch_tickers)) if results[i] is not None]\n",
    "    \n",
    "    # Add ticker info to the valid results\n",
    "    for ticker, df in valid_results:\n",
    "        df['Ticker'] = ticker  # Explicitly add a column for ticker\n",
    "    \n",
    "    # Combine all the valid stock data into a single DataFrame\n",
    "    if valid_results:\n",
    "        df_batch = pd.concat([df for _, df in valid_results], ignore_index=False)  # Don't lose index info\n",
    "        print(f\"Saving batch {batch_index} data to CSV.\")\n",
    "        # Save the batch to a CSV file\n",
    "        df_batch.to_csv(f\"{output_dir}/top600_5yr_stock_price_data_{batch_index}.csv\", index=False)\n",
    "    else:\n",
    "        print(f\"No data downloaded for batch {batch_index}.\")\n",
    "\n",
    "# Split tickers into batches\n",
    "for i in range(0, len(tickers), batch_size):\n",
    "    batch_tickers = tickers[i:i + batch_size]\n",
    "    batch_index = (i // batch_size) + 1  # Batch index starts from 1\n",
    "    download_batch(batch_tickers, batch_index)\n",
    "\n",
    "print(\"All batches processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b1a6ef-b1f9-4587-ac6a-865665dc5d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to hold DataFrames\n",
    "df_list = []\n",
    "\n",
    "# List of specific file indices\n",
    "file_indices = range(1, 7)\n",
    "\n",
    "# Loop through the specific CSV file indices\n",
    "for i in file_indices:\n",
    "    # Construct the file path for each batch\n",
    "    csv_file_path = f'/Users/evancallaghan/flatiron_ds/phase_5/capstone_project/top600_10yr_stock_price_data_{i}.csv'\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list along the rows (axis=0)\n",
    "df_all = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92f501-4ae0-42fe-ac19-0b367ea5e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e050b0-c4b5-4d9a-ae51-2f857455b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[['Ticker', 'Date', 'Close', 'High', 'Low', 'Volume']]\n",
    "df_all = df_all.rename(columns={'Ticker': 'Symbol',\n",
    "                               'High': 'Daily_High',\n",
    "                               'Low': 'Daily_Low'})\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a6efa-5a3c-4573-93a6-b5eee084e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all['Date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc5b5a-3c22-49fc-9f90-ae8f9da72463",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Date'] = pd.to_datetime(df_all['Date'], errors='coerce')\n",
    "print(df_all['Date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96f7a3-17ed-43c4-95b0-caa2a6be9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540232d0-08c4-48a9-a32d-3f5151d39cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stocks = df_all['Symbol'].unique().tolist()\n",
    "df_all = df_all[df_all['Symbol'].isin(top_stocks[:200])]\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb11e4-340d-4508-8491-41605f0109b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all['Symbol'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0400075f-d82e-4a7c-82d0-59c55a0b19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have to change this for now, but I can change the dataframe name later\n",
    "df_all_cleaned = df_all.copy()\n",
    "df_all_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171add33-1c6d-41f2-b3d1-bc32e8086752",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba23c49-644a-4f8d-988b-eb3f58001025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't thinkn I need this, can delete\n",
    "\n",
    "# Check for non-numeric values in the 'Close' column\n",
    "# non_numeric_values = df_all_cleaned[~df_all_cleaned['Close'].apply(pd.to_numeric, errors='coerce').notna()]\n",
    "# print(non_numeric_values[['Date', 'Symbol', 'Close']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf6bc5-13e8-46fa-94b9-3b3279d4fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't thinkn I need this, can delete\n",
    "\n",
    "# df_all_cleaned = df_all_cleaned[pd.to_numeric(df_all_cleaned['Close'], errors='coerce').notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3e059-7a9e-44f3-b5ee-4a988b72621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_1_week = df_all_cleaned.copy()\n",
    "stock_data_1_month = df_all_cleaned.copy()\n",
    "stock_data_3_month = df_all_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6085b4-fabf-4368-bc6b-5a349db53bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows for my TA indicator calculations\n",
    "\n",
    "one_week_window = [3, 5, 7]\n",
    "one_month_window = [7, 10, 14, 20, 30]\n",
    "three_month_window = [14, 20, 30, 50, 60, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568d69e-d075-4d9a-ae32-feeaf402868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Volume MA\n",
    "\n",
    "# 1 week\n",
    "for window in one_week_window:\n",
    "    stock_data_1_week[f'Volume_{window}day_avg'] = stock_data_1_week.groupby('Symbol')['Volume'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "\n",
    "# 1 month\n",
    "for window in one_month_window:\n",
    "    stock_data_1_month[f'Volume_{window}day_avg'] = stock_data_1_month.groupby('Symbol')['Volume'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "\n",
    "# 3 month\n",
    "for window in three_month_window:\n",
    "    stock_data_3_month[f'Volume_{window}day_avg'] = stock_data_3_month.groupby('Symbol')['Volume'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e5dc1-bd10-4229-a475-7ef2b0f44639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily High Price MA\n",
    "\n",
    "# 1 week\n",
    "for window in one_week_window:\n",
    "    stock_data_1_week[f'Daily_High_{window}day_avg'] = stock_data_1_week.groupby('Symbol')['Daily_High'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "# 1 month\n",
    "for window in one_month_window:\n",
    "    stock_data_1_month[f'Daily_High_{window}day_avg'] = stock_data_1_month.groupby('Symbol')['Daily_High'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "# 3 month\n",
    "for window in three_month_window:\n",
    "    stock_data_3_month[f'Daily_High_{window}day_avg'] = stock_data_3_month.groupby('Symbol')['Daily_High'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b1430-ce37-4ba8-85ac-d21962ec5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Low Price MA\n",
    "\n",
    "# 1 week\n",
    "for window in one_week_window:\n",
    "    stock_data_1_week[f'Daily_Low_{window}day_avg'] = stock_data_1_week.groupby('Symbol')['Daily_Low'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "# 1 month\n",
    "for window in one_month_window:\n",
    "    stock_data_1_month[f'Daily_Low_{window}day_avg'] = stock_data_1_month.groupby('Symbol')['Daily_Low'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "# 3 month\n",
    "for window in three_month_window:\n",
    "    stock_data_3_month[f'Daily_Low_{window}day_avg'] = stock_data_3_month.groupby('Symbol')['Daily_Low'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909fe0e-16fa-46c0-b6ca-39b48978bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Moving Average\n",
    "\n",
    "# 1 week\n",
    "for window in one_week_window:\n",
    "    \n",
    "    stock_data_1_week[f'SMA_{window}'] = stock_data_1_week.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "# 1 month\n",
    "for window in one_month_window:\n",
    "    \n",
    "    stock_data_1_month[f'SMA_{window}'] = stock_data_1_month.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "# 3 month\n",
    "for window in three_month_window:\n",
    "    \n",
    "    stock_data_3_month[f'SMA_{window}'] = stock_data_3_month.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e77e20-54e1-4ed0-9ff2-a9780e9b2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Moving Average\n",
    "\n",
    "# 1 week\n",
    "for window in one_week_window:\n",
    "    stock_data_1_week[f'EMA_{window}'] = stock_data_1_week.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.ewm(span=window, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "# 1 month\n",
    "for window in one_month_window:\n",
    "    stock_data_1_month[f'EMA_{window}'] = stock_data_1_month.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.ewm(span=window, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "# 3 month\n",
    "for window in three_month_window:\n",
    "    stock_data_3_month[f'EMA_{window}'] = stock_data_3_month.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.ewm(span=window, adjust=False).mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0301330-a6b3-4916-bdf1-4f7c376f34a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RSI\n",
    "\n",
    "# Define a function to calculate RSI\n",
    "def calculate_rsi(df, window=14):\n",
    "    # Calculate price changes\n",
    "    delta = df['Close'].diff()\n",
    "\n",
    "    # Separate gains and losses\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "    # Calculate the rolling average of gains and losses\n",
    "    avg_gain = gain.rolling(window=window).mean()\n",
    "    avg_loss = loss.rolling(window=window).mean()\n",
    "\n",
    "    # Calculate Relative Strength (RS)\n",
    "    rs = avg_gain / avg_loss\n",
    "\n",
    "    # Calculate RSI\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi\n",
    "\n",
    "# Apply the function to the dataframe to calculate RSI\n",
    "for df in [stock_data_1_week, stock_data_1_month, stock_data_3_month]:\n",
    "    df['RSI'] = calculate_rsi(df)\n",
    "\n",
    "# 1 week\n",
    "for window in one_week_window:\n",
    "\n",
    "    stock_data_1_week[f'RSI_{window}'] = stock_data_1_week.groupby('Symbol', group_keys=False).apply(\n",
    "        lambda x: calculate_rsi(x, window=window)\n",
    "    )\n",
    "\n",
    "# 1 month\n",
    "for window in one_month_window:\n",
    "\n",
    "    stock_data_1_month[f'RSI_{window}'] = stock_data_1_month.groupby('Symbol', group_keys=False).apply(\n",
    "        lambda x: calculate_rsi(x, window=window)\n",
    "    )\n",
    "\n",
    "# 3 month\n",
    "for window in three_month_window:\n",
    "\n",
    "    stock_data_3_month[f'RSI_{window}'] = stock_data_3_month.groupby('Symbol', group_keys=False).apply(\n",
    "        lambda x: calculate_rsi(x, window=window)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92445ec-095c-40c6-b225-6e4d7c93e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window mapping for loops\n",
    "window_mapping = {\n",
    "    \"stock_data_1_week\": (stock_data_1_week, one_week_window),\n",
    "    \"stock_data_1_month\": (stock_data_1_month, one_month_window),\n",
    "    \"stock_data_3_month\": (stock_data_3_month, three_month_window)\n",
    "}\n",
    "\n",
    "# List of dataframes for loops\n",
    "stock_dataframes = [stock_data_1_week, stock_data_1_month, stock_data_3_month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9627f-e119-4714-93e1-a5c0c073f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACD\n",
    "\n",
    "for df in [stock_data_1_week, stock_data_1_month, stock_data_3_month]:\n",
    "    \n",
    "    df['EMA_12_MACD'] = df.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.ewm(span=12, adjust=False).mean()\n",
    "    )\n",
    "    df['EMA_26_MACD'] = df.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.ewm(span=26, adjust=False).mean()\n",
    "    )\n",
    "\n",
    "    df['MACD'] = df['EMA_12_MACD'] - df['EMA_26_MACD']\n",
    "    df['Signal_Line'] = df.groupby('Symbol')['MACD'].transform(\n",
    "        lambda x: x.ewm(span=9, adjust=False).mean()\n",
    "    )\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['Signal_Line']\n",
    "\n",
    "\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    for window in windows:\n",
    "    # Apply rolling average to the MACD line \n",
    "        df[f'MACD_rolling_{window}'] = df.groupby('Symbol')['MACD'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "        # Apply rolling average to the Signal Line\n",
    "        df[f'Signal_rolling_{window}'] = df.groupby('Symbol')['Signal_Line'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "        df[f'MACD_Histogram_rolling_{window}'] = df.groupby('Symbol')['MACD_Histogram'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "    df.drop(columns=['EMA_12_MACD', 'EMA_26_MACD'], inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb009eb-0610-489b-85d2-063669c8e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None  # Show all columns\n",
    "\n",
    "stock_data_3_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe0cbc-984c-4585-b08d-ec460754adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic oscillator\n",
    "\n",
    "for df in stock_dataframes:\n",
    "    df['Stoch_Lowest_Low_14'] = df.groupby('Symbol')['Daily_Low'].transform(\n",
    "        lambda x: x.rolling(window=14, min_periods=1).min()\n",
    "    )\n",
    "    df['Stoch_Highest_High_14'] = df.groupby('Symbol')['Daily_High'].transform(\n",
    "        lambda x: x.rolling(window=14, min_periods=1).max()\n",
    "    )\n",
    "    df['%K'] = ((df['Close'] - df['Stoch_Lowest_Low_14']) / (df['Stoch_Highest_High_14'] - df['Stoch_Lowest_Low_14'])) * 100\n",
    "    \n",
    "\n",
    "# Calculate the %D (3-day Simple Moving Average of %K)\n",
    "# Regardless of window size, each DF gets the %D (3day SMA of %k)\n",
    "\n",
    "for df in stock_dataframes:\n",
    "    df['%D_3'] = df.groupby('Symbol')['%K'].transform(\n",
    "        lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    for window in windows:\n",
    "\n",
    "        df[f'%D_{window}'] = df.groupby('Symbol')['%K'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )  \n",
    "    # Drop intermediate columns if you don't need them\n",
    "    df.drop(columns=['Stoch_Lowest_Low_14', 'Stoch_Highest_High_14'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2efea-5d80-47cf-bca5-f389b95b7f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VWAP\n",
    "\n",
    "# Calculate Volume Weighted Average Price (VWAP) per symbol\n",
    "def calculate_vwap(df):\n",
    "    # Ensure 'Close' and 'Volume' are numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "    df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')\n",
    "\n",
    "    # Calculate cumulative price-volume product for VWAP\n",
    "    df['Cumulative_Price_Volume'] = df.groupby('Symbol')['Close'].transform(\n",
    "    lambda x: (x * df.loc[x.index, 'Volume']).cumsum()\n",
    "    )\n",
    "    # Calculate cumulative volume for VWAP\n",
    "    df['Cumulative_Volume'] = df.groupby('Symbol')['Volume'].transform(\n",
    "    lambda x: x.cumsum()\n",
    "    )\n",
    "    # Calculate VWAP as the ratio of cumulative sums for each group (symbol)\n",
    "    df['VWAP'] = df['Cumulative_Price_Volume'] / df['Cumulative_Volume']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Add VWAP and VWAP window averages to dataframes\n",
    "i = 0\n",
    "while i < len(stock_dataframes):\n",
    "    df = stock_dataframes[i]\n",
    "    \n",
    "    calculate_vwap(df)\n",
    "    for name, (df_map, windows) in window_mapping.items():\n",
    "        if df is df_map:\n",
    "            for window in windows:\n",
    "    \n",
    "                df[f'VWAP_{window}'] = df.groupby('Symbol')['VWAP'].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "                )\n",
    "    \n",
    "    df.drop(columns=['Cumulative_Price_Volume', 'Cumulative_Volume'], inplace=True)\n",
    "    i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203e815-69f0-4891-a6c6-d0969863b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, (df, windows) in window_mapping.items():\n",
    "#     for window in windows:\n",
    "#         # Drop the VWAP and VWAP_{window} columns after the rolling averages are calculated\n",
    "#         df.drop(columns=['VWAP'], inplace=True, errors='ignore')  # Drop the VWAP column\n",
    "#         df.drop(columns=[f'VWAP_{window}' for window in windows], inplace=True, errors='ignore')  # Drop VWAP_{window} columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865196bf-9b2c-4235-bc9e-c4b8285048cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Bollinger Bands per symbol\n",
    "\n",
    "def calculate_bollinger_bands(df, windows):\n",
    "    # Ensure 'Close' is numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "\n",
    "    for window in windows:\n",
    "        df[f'bb_Middle_Band_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "        df[f'Std_Dev_{window}'] = df.groupby('Symbol')['Close'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "\n",
    "        df[f'Upper_Band_{window}'] = df[f'bb_Middle_Band_{window}'] + (df[f'Std_Dev_{window}'] * 2)\n",
    "        df[f'Lower_Band_{window}'] = df[f'bb_Middle_Band_{window}'] - (df[f'Std_Dev_{window}'] * 2)\n",
    "   \n",
    "    return df\n",
    "\n",
    "# Add Bollinger Bands to all dataframes\n",
    "i = 0\n",
    "while i < len(stock_dataframes):\n",
    "    \n",
    "    for name, (df, windows) in window_mapping.items():\n",
    "        calculate_bollinger_bands(df, windows)\n",
    "            \n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bf089-8654-49a4-8623-61c4b5709900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Average True Range (ATR)\n",
    "\n",
    "# Function to calculate True Range (TR)\n",
    "def calculate_true_range(df):\n",
    "    # Convert relevant columns to numeric (if not already numeric)\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "    df['Daily_High'] = pd.to_numeric(df['Daily_High'], errors='coerce')\n",
    "    df['Daily_Low'] = pd.to_numeric(df['Daily_Low'], errors='coerce')\n",
    "\n",
    "    # Ensure previous close is calculated per stock symbol to prevent cross-stock contamination\n",
    "    df['ATR_Prev_Close'] = df.groupby('Symbol')['Close'].shift(1)\n",
    "\n",
    "    df['ATR_High_Low'] = df['Daily_High'] - df['Daily_Low']  # High - Low\n",
    "    df['ATR_High_Close'] = (df['Daily_High'] - df['ATR_Prev_Close']).abs()  # High - Prev Close\n",
    "    df['ATR_Low_Close'] = (df['Daily_Low'] - df['ATR_Prev_Close']).abs()  # Low - Prev Close\n",
    "\n",
    "    # True Range is the max of the three\n",
    "    df['ATR'] = df[['ATR_High_Low', 'ATR_High_Close', 'ATR_Low_Close']].max(axis=1)\n",
    "\n",
    "    df.drop(columns=['ATR_Prev_Close', 'ATR_High_Low', 'ATR_High_Close', 'ATR_Low_Close'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add ATR calculation to all dataframes and add rolling windows\n",
    "for name, (df, windows) in window_mapping.items():\n",
    "    df = calculate_true_range(df)\n",
    "    for window in windows:\n",
    "\n",
    "        df[f'ATR_{window}'] = df.groupby('Symbol')['ATR'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a4085-12e8-4e7d-97f1-93e45e193eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_fibonacci_retracement(df, windows):\n",
    "    # Convert relevant columns to numeric\n",
    "    df['Daily_High'] = pd.to_numeric(df['Daily_High'], errors='coerce')\n",
    "    df['Daily_Low'] = pd.to_numeric(df['Daily_Low'], errors='coerce')\n",
    "\n",
    "    # Define Fibonacci levels\n",
    "    fib_levels = [0.236, 0.382, 0.500, 0.618, 0.786, 1.000, 1.618, 2.618, 4.236]\n",
    "\n",
    "    # Group by 'Symbol' and calculate Fibonacci levels for a given window\n",
    "    def fib_retracement(stock_df, window):\n",
    "        stock_df[f'Fib_{window}_High_Max'] = stock_df['Daily_High'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "        )\n",
    "        stock_df[f'Fib_{window}_Low_Min'] = stock_df['Daily_Low'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "        )\n",
    "\n",
    "        # Calculate Fibonacci retracement levels for each level\n",
    "        for level in fib_levels:\n",
    "            stock_df[f'{window}_day_Fib_{int(level*100)}'] = stock_df[f'Fib_{window}_High_Max'] - (\n",
    "                level * (stock_df[f'Fib_{window}_High_Max'] - stock_df[f'Fib_{window}_Low_Min']))\n",
    "\n",
    "        return stock_df\n",
    "\n",
    "    # Apply the function to each stock symbol and window\n",
    "    for window in windows:\n",
    "        df = df.groupby('Symbol', group_keys=False).apply(fib_retracement, window)\n",
    "\n",
    "    return df\n",
    "\n",
    "fib_windows = [5, 14, 30]\n",
    "\n",
    "# Apply Fibonacci Retracement calculation to the dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c997f-2749-4735-a311-2ffa48f30c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f959e-be42-4799-9273-e01afc55d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_all_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95450a29-d142-4d5b-b665-dc4add35c92b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OBV\n",
    "# Calculate OBV for each stock symbol separately\n",
    "df_all_cleaned['OBV'] = df_all_cleaned.groupby('Symbol')['Volume'].transform(\n",
    "    lambda x: (np.sign(x.diff()) * x).cumsum()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for window in one_week_window:\n",
    "\n",
    "    df_all_cleaned[f'OBV_{window}day_avg'] = df_all_cleaned.groupby('Symbol')['OBV'].transform(\n",
    "        lambda x: x.rolling(window=window).mean()\n",
    "    )\n",
    "\n",
    "    \n",
    "df_all_cleaned.head()\n",
    "\n",
    "# Calculate 3-day rolling average of OBV for each stock symbol\n",
    "# df_all_cleaned['OBV_3day_avg'] = df_all_cleaned.groupby('Symbol')['OBV'].rolling(window=3).mean()\n",
    "# df_all_cleaned['OBV_5day_avg'] = df_all_cleaned.groupby('Symbol')['OBV'].rolling(window=5).mean()\n",
    "# df_all_cleaned['OBV_7day_avg'] = df_all_cleaned.groupby('Symbol')['OBV'].rolling(window=7).mean()\n",
    "# df_all_cleaned['OBV_10day_avg'] = df_all_cleaned.groupby('Symbol')['OBV'].rolling(window=10).mean()\n",
    "# df_all_cleaned['OBV_12day_avg'] = df_all_cleaned.groupby('Symbol')['OBV'].rolling(window=12).mean()\n",
    "# df_all_cleaned['OBV_14day_avg'] = df_all_cleaned.groupby('Symbol')['OBV'].rolling(window=14).mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804d661-ebfd-47f2-97c3-84b53bdb17dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569b05f-ebbf-4eb8-a520-3da9dea52485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WMA\n",
    "\n",
    "# Define the function for weighted moving average\n",
    "def weighted_moving_average(series, weights):\n",
    "    return np.dot(series, weights) / weights.sum()\n",
    "\n",
    "\n",
    "# Apply the rolling WMA for each window\n",
    "for window in one_week_window:\n",
    "    df_all_cleaned[f'WMA_{window}'] = df_all_cleaned.groupby('Symbol')['Close'].rolling(\n",
    "        window=window, min_periods=1\n",
    "    ).apply(lambda x: weighted_moving_average(x, np.linspace(1, 0.1, len(x))), raw=False).droplevel(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c825c7e-a19d-4b91-93e3-3d3f0da8747d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5aa41-1b2d-4f7f-9400-17ff9b4bdfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum Features\n",
    "for window in one_week_window:\n",
    "\n",
    "    df_all_cleaned[f'Momentum_{window}'] = df_all_cleaned.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x - x.shift(window)\n",
    "    ).fillna(0)\n",
    "\n",
    "# df_all_cleaned['Momentum_3'] = df_all_cleaned.groupby('Symbol')['Close'].transform(lambda x: x.diff(3))\n",
    "# df_all_cleaned['Momentum_5'] = df_all_cleaned.groupby('Symbol')['Close'].transform(lambda x: x.diff(5))\n",
    "# df_all_cleaned['Momentum_10'] = df_all_cleaned.groupby('Symbol')['Close'].transform(lambda x: x.diff(10))\n",
    "# df_all_cleaned['Momentum_7'] = df_all_cleaned.groupby('Symbol')['Close'].transform(lambda x: x.diff(7))\n",
    "# df_all_cleaned['Momentum_12'] = df_all_cleaned.groupby('Symbol')['Close'].transform(lambda x: x.diff(12))\n",
    "# df_all_cleaned['Momentum_14'] = df_all_cleaned.groupby('Symbol')['Close'].transform(lambda x: x.diff(14))\n",
    "\n",
    "# Verify the results\n",
    "df_all_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e088481-5272-455e-bcf4-8892fb8a4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile-Based Features\n",
    "\n",
    "for window in one_week_window:\n",
    "\n",
    "    df_all_cleaned[f'Rolling_Median_{window}'] = df_all_cleaned.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.rolling(window=window).median()\n",
    "    ).fillna(0)\n",
    "\n",
    "    # Rolling 25th Quantile\n",
    "    df_all_cleaned[f'Rolling_Quantile_25_{window}'] = df_all_cleaned.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.rolling(window=window).quantile(0.25)\n",
    "    ).fillna(0)\n",
    "\n",
    "    # Rolling 75th Quantile\n",
    "    df_all_cleaned[f'Rolling_Quantile_75_{window}'] = df_all_cleaned.groupby('Symbol')['Close'].transform(\n",
    "        lambda x: x.rolling(window=window).quantile(0.75)\n",
    "    ).fillna(0)\n",
    "\n",
    "# df_all_cleaned['Rolling_Median_3'] = df_all_cleaned.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=3).median())\n",
    "\n",
    "# df_all_cleaned['Rolling_Quantile_25_3'] = df_all_cleaned.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=3).quantile(0.25))\n",
    "# df_all_cleaned['Rolling_Quantile_75_3'] = df_all_cleaned.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=3).quantile(0.75))\n",
    "\n",
    "# Verify the results\n",
    "df_all_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47a140-aed8-49e2-8e3b-dfdd6a53243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_all_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d80bf-cec7-46d6-b799-e600664bb121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of columns to create lags for (focusing on short-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_3', 'SMA_5', 'SMA_7', \n",
    "                  'EMA_3', 'EMA_5', 'EMA_7', 'Volume',\n",
    "                 'Daily_High', 'Daily_Low', 'RSI', 'RSI_3', 'RSI_5', 'RSI_7', 'Signal_Line', 'MACD', \n",
    "                  'VWAP', '%K', '%D_3','WMA_3', 'WMA_5', 'WMA_7', 'OBV', 'Momentum_3', \n",
    "                  'Momentum_5', 'Momentum_7','Std_Dev_3', 'Std_Dev_5', 'Std_Dev_7', \n",
    "                  'Rolling_Median_3', 'Rolling_Median_5', 'Rolling_Median_7', \n",
    "                  'Rolling_Quantile_25_3', 'Rolling_Quantile_25_5', 'Rolling_Quantile_25_7', \n",
    "                  'Rolling_Quantile_75_3', 'Rolling_Quantile_75_5', 'Rolling_Quantile_75_7', \n",
    "                 'Daily_High_3day_avg', 'Daily_High_5day_avg', 'Daily_High_7day_avg', \n",
    "                 'Daily_Low_3day_avg', 'Daily_Low_5day_avg', 'Daily_Low_7day_avg', \n",
    "                 'Volume_3day_avg', 'Volume_5day_avg', 'Volume_7day_avg']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 14, 20, 30, 50, 60, 90, 100 180, 200] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_all_cleaned[f'{col}_lag_{lag}'] = df_all_cleaned[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_all_cleaned.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef74e3-1761-4404-a55d-70af4dfccdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e9b57-176d-48e3-9d2b-8c69df3b69c2",
   "metadata": {},
   "source": [
    "RSI\n",
    "MACD and MACD Signal Line\n",
    "Stochastic Oscillator\n",
    "VWAP\n",
    "Bollinger Bands (Upper and Lower)\n",
    "Price Range (High - Low)\n",
    "Momentum and Standard Deviation\n",
    "Fib Levels\n",
    "OBV\n",
    "Quantile Features (Median, Upper/Lower Quantile)\n",
    "Weighted Moving Average (WMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54763e8-88ac-4442-93eb-be7c0f83a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we accidentally have the same values in any of these columns (rounded to 4 decimal places)\n",
    "# Get the first 1000 rows of the dataframe\n",
    "df_sample = df_all_cleaned.head(2500)\n",
    "\n",
    "# Exclude 'Symbol' and 'Date' columns\n",
    "columns = [col for col in df_sample.columns if col not in ['Symbol', 'Date']]\n",
    "\n",
    "# Lists to store matching column pairs\n",
    "col_i_list = []\n",
    "col_j_list = []\n",
    "\n",
    "# Loop through all pairs of remaining columns and compare their values rounded to 4 decimal places\n",
    "for i in range(len(columns)):\n",
    "    for j in range(i + 1, len(columns)):  # Avoid duplicate comparisons\n",
    "        if (df_sample[columns[i]].round(4) == df_sample[columns[j]].round(4)).all():\n",
    "            col_i_list.append(columns[i])\n",
    "            col_j_list.append(columns[j])\n",
    "\n",
    "# Print the two lists\n",
    "print(\"Matching Column Pairs:\")\n",
    "print(\"Column i:\", col_i_list)\n",
    "print(\"Column j:\", col_j_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e282b-ff6a-49ee-a6dd-5409e2945e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned.drop(columns=['Fib_3_Low_Min', 'Fib_5_Low_Min', 'Fib_7_Low_Min',\n",
    "                             'bb_Middle_Band_3', 'bb_Middle_Band_5', 'bb_Middle_Band_7'\n",
    "                             \n",
    "                            ], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e6ba7-891f-4f63-873d-25418d325ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we accidentally have the same values in any of these columns (rounded to 4 decimal places)\n",
    "# Get the first 1000 rows of the dataframe\n",
    "df_sample = df_all_cleaned.head(2500)\n",
    "\n",
    "# Exclude 'Symbol' and 'Date' columns\n",
    "columns = [col for col in df_sample.columns if col not in ['Symbol', 'Date']]\n",
    "\n",
    "# Lists to store matching column pairs\n",
    "col_i_list = []\n",
    "col_j_list = []\n",
    "\n",
    "# Loop through all pairs of remaining columns and compare their values rounded to 4 decimal places\n",
    "for i in range(len(columns)):\n",
    "    for j in range(i + 1, len(columns)):  # Avoid duplicate comparisons\n",
    "        if (df_sample[columns[i]].round(4) == df_sample[columns[j]].round(4)).all():\n",
    "            col_i_list.append(columns[i])\n",
    "            col_j_list.append(columns[j])\n",
    "\n",
    "# Print the two lists\n",
    "print(\"Matching Column Pairs:\")\n",
    "print(\"Column i:\", col_i_list)\n",
    "print(\"Column j:\", col_j_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba68b36-27c3-4050-8269-72b376f36ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as CSV file for easy access\n",
    "df_all_cleaned.to_csv('/Users/evancallaghan/flatiron_ds/phase_5/capstone_project/stock_10_yr_150_ta_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4260f-fdba-49c3-931a-0f1ac87620fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path to your CSV in Google Drive\n",
    "csv_file_path = '/Users/evancallaghan/flatiron_ds/phase_5/capstone_project/stock_ta_data.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df_stocks_price_ta = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "df_stocks_price_ta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562a153-aecd-4856-b687-27690a0fd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_data_1_week = df_all_cleaned.copy()\n",
    "df_stock_data_1_week.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dcb6ac-cf4f-46aa-a706-cf433b219ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_data_1_week = df_stocks_price_ta.copy()\n",
    "df_stocks_price_ta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a3f76-1e9c-4716-95f3-96351a0cfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_data_1_week.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e3735-0369-486e-8f4d-3394e79d96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i think this one would actually be the baseline, as i can separate the dates and test only\n",
    "# after feb 10 which is what i want to do\n",
    "# it also contains scaled data, which was better\n",
    "# baseline model\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_baseline.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_baseline['Close_Target'] = df_stock_data_train_1_week_baseline.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_baseline['Close_Target'] = df_stock_data_test_1_week_baseline.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_baseline = df_stock_data_train_1_week_baseline.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_baseline = df_stock_data_test_1_week_baseline.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_baseline.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_baseline.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_baseline[numeric_cols_train] = df_stock_data_train_1_week_baseline[numeric_cols_train].fillna(df_stock_data_train_1_week_baseline[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_baseline[numeric_cols_test] = df_stock_data_test_1_week_baseline[numeric_cols_test].fillna(df_stock_data_test_1_week_baseline[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_baseline.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_baseline.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_baseline = df_stock_data_train_1_week_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_baseline = df_stock_data_train_1_week_baseline['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_baseline = df_stock_data_test_1_week_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_baseline = df_stock_data_test_1_week_baseline['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_baseline shape: {X_train_1_week_baseline.shape}, y_train_1_week_baseline shape: {y_train_1_week_baseline.shape}\")\n",
    "print(f\"X_test_1_week_baseline shape: {X_test_1_week_baseline.shape}, y_test_1_week_baseline shape: {y_test_1_week_baseline.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_baseline.shape[0] == 0 or X_test_1_week_baseline.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_week = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_week.fit(X_train_1_week_baseline, y_train_1_week_baseline)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_baseline = model_baseline_1_week.predict(X_test_1_week_baseline)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_week_baseline = mean_squared_error(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test_1_week_baseline}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63f8dd-0c9a-4c3b-8b80-e54c451605d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_week_baseline` are your predictions for the test data and `y_test_1_week_baseline` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_baseline = mean_squared_error(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "mae_1_week_baseline = mean_absolute_error(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "rmse_1_week_baseline = np.sqrt(mse_1_week_baseline)  # Root Mean Squared Error\n",
    "r2_1_week_baseline = r2_score(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_baseline}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_baseline}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_baseline}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_baseline}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_week_baseline = median_absolute_error(y_test_1_week_baseline, y_pred_1_week_baseline)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_week_baseline}')\n",
    "\n",
    "dw_stat_1_week_baseline = durbin_watson(y_test_1_week_baseline - y_pred_1_week_baseline)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_baseline}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_baseline = np.mean(np.abs((y_test_1_week_baseline - y_pred_1_week_baseline) / y_test_1_week_baseline)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_baseline:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_baseline = dict(zip(X_train_1_week_baseline.columns, model_baseline_1_week.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_baseline = sorted(feature_importance_1_week_baseline.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_baseline:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799369d9-7383-4605-accf-17a8facc976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# we're going to use the scaled data, so the model above will be\n",
    "# our baseline\n",
    "# next we're going to use the same model and use a new dataframe\n",
    "# with features from the baseline model that contributed more than 1%\n",
    "# first we need to get a list of important feautres from our baseline\n",
    "# model and create a new dataframe containing only those features\n",
    "# Get feature importance as a dictionary\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_baseline = dict(zip(X_train_1_week_baseline.columns, model_baseline_1_week.feature_importances_))\n",
    "\n",
    "# Filter features with importance greater than 1%\n",
    "important_features_1_week_baseline = {feature: importance for feature, importance in feature_importance_1_week_baseline.items() if importance > 0.01}\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_important_features_1_week_baseline = sorted(important_features_1_week_baseline.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the features names (keys) into a list\n",
    "important_feature_names_1_week_baseline = [feature for feature, importance in sorted_important_features_1_week_baseline]\n",
    "\n",
    "# Print the sorted important features (optional)\n",
    "print(\"Features with more than 1% contribution:\")\n",
    "for feature in sorted_important_features_1_week_baseline:\n",
    "    print(f\"{feature[0]}: {feature[1] * 100:.2f}%\")\n",
    "\n",
    "# The list of important features that you can use to create a new dataframe\n",
    "print(\"List of important features:\")\n",
    "print(important_feature_names_1_week_baseline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea64e0-0f6c-4e1a-b89e-f8843b7a14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = ['Symbol', 'Date', 'Close', 'SMA_5', '10_day_Fib_100', 'Fib_5_High_Max', \n",
    "                      'EMA_3', 'Low', 'EMA_26_MACD', '5_day-Fib_23', 'Fib_30_High_Max', 'EMA_100',\n",
    "                      'High', '5_day-Fib_100', 'EMA_20', '30_day_Fib_100']\n",
    "df_important_feat_1_week = df_stock_data_1_week[important_features]\n",
    "df_important_feat_1_week.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872d7c2-824b-4f00-b994-282282e59b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to use the scaled data, so the model above will be\n",
    "# our baseline\n",
    "# next we're going to use the same model and use a new dataframe\n",
    "# with features from the baseline model that contributed more than 1%\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_important_feat_1_week = df_important_feat_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_if.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_if['Close_Target'] = df_stock_data_train_1_week_if.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_if['Close_Target'] = df_stock_data_test_1_week_if.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_if = df_stock_data_train_1_week_if.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_if = df_stock_data_test_1_week_if.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_if.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_if.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_if[numeric_cols_train] = df_stock_data_train_1_week_if[numeric_cols_train].fillna(df_stock_data_train_1_week_if[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_if[numeric_cols_test] = df_stock_data_test_1_week_if[numeric_cols_test].fillna(df_stock_data_test_1_week_if[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_if.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_if.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_if = df_stock_data_train_1_week_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_if = df_stock_data_train_1_week_if['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_if = df_stock_data_test_1_week_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_if = df_stock_data_test_1_week_if['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_if shape: {X_train_1_week_if.shape}, y_train_1_week_if shape: {y_train_1_week_if.shape}\")\n",
    "print(f\"X_test_1_week_if shape: {X_test_1_week_if.shape}, y_test_1_week_if shape: {y_test_1_week_if.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_if.shape[0] == 0 or X_test_1_week_if.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_update1_1_week = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_update1_1_week.fit(X_train_1_week_if, y_train_1_week_if)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_if = model_update1_1_week.predict(X_test_1_week_if)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_week_if = mean_squared_error(y_test_1_week_if, y_pred_1_week_if)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test_1_week_if}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0add6b-88df-4251-bc1b-e95e9785e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_week_if` are your predictions for the test data and `y_test_1_week_if` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_if = mean_squared_error(y_test_1_week_if, y_pred_1_week_if)\n",
    "mae_1_week_if = mean_absolute_error(y_test_1_week_if, y_pred_1_week_if)\n",
    "rmse_1_week_if = np.sqrt(mse_1_week_if)  # Root Mean Squared Error\n",
    "r2_1_week_if = r2_score(y_test_1_week_if, y_pred_1_week_if)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_if}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_if}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_if}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_if}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_week_if = median_absolute_error(y_test_1_week_if, y_pred_1_week_if)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_week_if}')\n",
    "\n",
    "dw_stat_1_week_if = durbin_watson(y_test_1_week_if - y_pred_1_week_if)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_if}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_if = np.mean(np.abs((y_test_1_week_if - y_pred_1_week_if) / y_test_1_week_if)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_if:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_if = dict(zip(X_train_1_week_if.columns, model_update1_1_week.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_if = sorted(feature_importance_1_week_if.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_if:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8deae-7fa7-4ac8-8ac8-600bbcaaa2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing important features led to the degredation of all metrics\n",
    "# we're going to use all metrics again and try adjusting a few of the hyper parameters\n",
    "#\n",
    "# i think this one would actually be the baseline, as i can separate the dates and test only\n",
    "# after feb 10 which is what i want to do\n",
    "# it also contains scaled data, which was better\n",
    "# learning_rate = 0.01\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_lr_01.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_lr_01['Close_Target'] = df_stock_data_train_1_week_lr_01.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_lr_01['Close_Target'] = df_stock_data_test_1_week_lr_01.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_lr_01 = df_stock_data_train_1_week_lr_01.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_lr_01 = df_stock_data_test_1_week_lr_01.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_lr_01.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_lr_01.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_lr_01[numeric_cols_train] = df_stock_data_train_1_week_lr_01[numeric_cols_train].fillna(df_stock_data_train_1_week_lr_01[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_lr_01[numeric_cols_test] = df_stock_data_test_1_week_lr_01[numeric_cols_test].fillna(df_stock_data_test_1_week_lr_01[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_lr_01.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_lr_01.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_lr_01 = df_stock_data_train_1_week_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_lr_01 = df_stock_data_train_1_week_lr_01['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_lr_01 = df_stock_data_test_1_week_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_lr_01 = df_stock_data_test_1_week_lr_01['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_lr_01 shape: {X_train_1_week_lr_01.shape}, y_train_1_week_lr_01 shape: {y_train_1_week_lr_01.shape}\")\n",
    "print(f\"X_test_1_week_lr_01 shape: {X_test_1_week_lr_01.shape}, y_test_1_week_lr_01 shape: {y_test_1_week_lr_01.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_lr_01.shape[0] == 0 or X_test_1_week_lr_01.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_week_LR_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_week_LR_01.fit(X_train_1_week_lr_01, y_train_1_week_lr_01)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_lr_01 = model_baseline_1_week_LR_01.predict(X_test_1_week_lr_01)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_week_lr_01 = mean_squared_error(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test_1_week_lr_01}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f7d07-9367-43ff-ab48-b4f44670b32a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_1_week_lr_01` are your predictions for the test data and `y_test_1_week_lr_01` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_lr_01 = mean_squared_error(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "mae_1_week_lr_01 = mean_absolute_error(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "rmse_1_week_lr_01 = np.sqrt(mse_1_week_lr_01)  # Root Mean Squared Error\n",
    "r2_1_week_lr_01 = r2_score(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_lr_01}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_lr_01}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_lr_01}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_lr_01}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_week_lr_01 = median_absolute_error(y_test_1_week_lr_01, y_pred_1_week_lr_01)\n",
    "print(f'Median Absolute Error on unseen data: {medae_week_lr_01}')\n",
    "\n",
    "dw_stat_1_week_lr_01 = durbin_watson(y_test_1_week_lr_01 - y_pred_1_week_lr_01)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_lr_01}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_lr_01 = np.mean(np.abs((y_test_1_week_lr_01 - y_pred_1_week_lr_01) / y_test_1_week_lr_01)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_lr_01:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_lr_01 = dict(zip(X_train_1_week_lr_01.columns, model_baseline_1_week_LR_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_lr_01 = sorted(feature_importance_1_week_lr_01.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_lr_01:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53d222-e712-4709-82eb-3999a328899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing important features led to the degredation of all metrics\n",
    "# we're going to use all metrics again and try adjusting a few of the hyper parameters\n",
    "#\n",
    "# i think this one would actually be the baseline, as i can separate the dates and test only\n",
    "# after feb 10 which is what i want to do\n",
    "# it also contains scaled data, which was better\n",
    "# learning_rate = 0.1\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_lr_1 = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_lr_1 = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_lr_1.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_lr_1['Close_Target'] = df_stock_data_train_1_week_lr_1.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_lr_1['Close_Target'] = df_stock_data_test_1_week_lr_1.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_lr_1 = df_stock_data_train_1_week_lr_1.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_lr_1 = df_stock_data_test_1_week_lr_1.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_lr_1.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_lr_1.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_lr_1[numeric_cols_train] = df_stock_data_train_1_week_lr_1[numeric_cols_train].fillna(df_stock_data_train_1_week_lr_1[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_lr_1[numeric_cols_test] = df_stock_data_test_1_week_lr_1[numeric_cols_test].fillna(df_stock_data_test_1_week_lr_1[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_lr_1.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_lr_1.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_lr_1 = df_stock_data_train_1_week_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_lr_1 = df_stock_data_train_1_week_lr_1['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_lr_1 = df_stock_data_test_1_week_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_lr_1 = df_stock_data_test_1_week_lr_1['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_lr_1 shape: {X_train_1_week_lr_1.shape}, y_train_1_week_lr_1 shape: {y_train_1_week_lr_1.shape}\")\n",
    "print(f\"X_test_1_week_lr_1 shape: {X_test_1_week_lr_1.shape}, y_test_1_week_lr_1 shape: {y_test_1_week_lr_1.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_lr_1.shape[0] == 0 or X_test_1_week_lr_1.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_week_LR_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_week_LR_1.fit(X_train_1_week_lr_1, y_train_1_week_lr_1)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_lr_1 = model_baseline_1_week_LR_1.predict(X_test_1_week_lr_1)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c41240-ce28-461a-8ca2-e17fa249276b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_week_lr_1` are your predictions for the test data and `y_test_1_week_lr_1` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_lr_1 = mean_squared_error(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "mae_1_week_lr_1 = mean_absolute_error(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "rmse_1_week_lr_1 = np.sqrt(mse_1_week_lr_1)  # Root Mean Squared Error\n",
    "r2_1_week_lr_1 = r2_score(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_lr_1}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_lr_1}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_lr_1}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_lr_1}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_week_lr_1 = median_absolute_error(y_test_1_week_lr_1, y_pred_1_week_lr_1)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_week_lr_1}')\n",
    "\n",
    "dw_stat_1_week_lr_1 = durbin_watson(y_test_1_week_lr_1 - y_pred_1_week_lr_1)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_lr_1}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_lr_1 = np.mean(np.abs((y_test_1_week_lr_1 - y_pred_1_week_lr_1) / y_test_1_week_lr_1)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_lr_1:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_lr_1 = dict(zip(X_train_1_week_lr_1.columns, model_baseline_1_week_LR_1.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_lr_1 = sorted(feature_importance_1_week_lr_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_lr_1:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5bf201-dd97-426a-a0e9-ae5951cf55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate outcome: learning_rate=0.01 showed the best improvement\n",
    "# and had better metrics than the baseline, so we'll keep it and now tweak max_depth\n",
    "\n",
    "# max_depth = 3\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_md_3 = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_baseline = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_md_3.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_md_3['Close_Target'] = df_stock_data_train_1_week_md_3.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_md_3['Close_Target'] = df_stock_data_test_1_week_md_3.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_md_3 = df_stock_data_train_1_week_md_3.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_md_3 = df_stock_data_test_1_week_md_3.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_md_3.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_md_3.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_md_3[numeric_cols_train] = df_stock_data_train_1_week_md_3[numeric_cols_train].fillna(df_stock_data_train_1_week_md_3[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_md_3[numeric_cols_test] = df_stock_data_test_1_week_md_3[numeric_cols_test].fillna(df_stock_data_test_1_week_md_3[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_md_3.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_md_3.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_md_3 = df_stock_data_train_1_week_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_md_3 = df_stock_data_train_1_week_md_3['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_md_3 = df_stock_data_test_1_week_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_md_3 = df_stock_data_test_1_week_md_3['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_md_3 shape: {X_train_1_week_md_3.shape}, y_train_1_week_md_3 shape: {y_train_1_week_md_3.shape}\")\n",
    "print(f\"X_test_1_week_md_3 shape: {X_test_1_week_md_3.shape}, y_test_1_week_md_3 shape: {y_test_1_week_md_3.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_md_3.shape[0] == 0 or X_test_1_week_md_3.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_week_MD_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_week_MD_3.fit(X_train_1_week_md_3, y_train_1_week_md_3)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_md_3 = model_1_week_MD_3.predict(X_test_1_week_md_3)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_week_md_3 = mean_squared_error(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test_1_week_md_3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f1ff1-97d5-4986-840e-51abb90c8117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_week_md_3` are your predictions for the test data and `y_test_1_week_md_3` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_md_3 = mean_squared_error(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "mae_1_week_md_3 = mean_absolute_error(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "rmse_1_week_md_3 = np.sqrt(mse_1_week_md_3)  # Root Mean Squared Error\n",
    "r2_1_week_md_3 = r2_score(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_md_3}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_md_3}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_md_3}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_md_3}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_week_md_3 = median_absolute_error(y_test_1_week_md_3, y_pred_1_week_md_3)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_week_md_3}')\n",
    "\n",
    "dw_stat_1_week_md_3 = durbin_watson(y_test_1_week_md_3 - y_pred_1_week_md_3)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_md_3}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_md_3 = np.mean(np.abs((y_test_1_week_md_3 - y_pred_1_week_md_3) / y_test_1_week_md_3)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_md_3:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_md_3 = dict(zip(X_train_1_week_md_3.columns, model_1_week_MD_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_md_3 = sorted(feature_importance_1_week_md_3.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_md_3:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e4ee4-0759-442d-b08c-f91e606d533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate outcome: learning_rate=0.01 showed the best improvement\n",
    "# and had better metrics than the baseline, so we'll keep it and now tweak max_depth\n",
    "\n",
    "# max_depth = 7\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Use data until the end of January 2022 for training (i.e., last 2-3 years for testing)\n",
    "df_stock_data_train_1_week_md_7 = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2023-01-24']\n",
    "\n",
    "# Use data from February 1, 2022, onwards for testing\n",
    "df_stock_data_test_1_week_md_7 = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2023-01-31']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_week_md_7.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train_1_week_md_7['Close_Target'] = df_stock_data_train_1_week_md_7.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test_1_week_md_7['Close_Target'] = df_stock_data_test_1_week_md_7.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_week_md_7 = df_stock_data_train_1_week_md_7.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_week_md_7 = df_stock_data_test_1_week_md_7.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_week_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_week_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_week_md_7.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_week_md_7.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_week_md_7[numeric_cols_train] = df_stock_data_train_1_week_md_7[numeric_cols_train].fillna(df_stock_data_train_1_week_md_7[numeric_cols_train].median())\n",
    "df_stock_data_test_1_week_md_7[numeric_cols_test] = df_stock_data_test_1_week_md_7[numeric_cols_test].fillna(df_stock_data_test_1_week_md_7[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_week_md_7.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_week_md_7.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_week_md_7 = df_stock_data_train_1_week_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_week_md_7 = df_stock_data_train_1_week_md_7['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_week_md_7 = df_stock_data_test_1_week_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_week_md_7 = df_stock_data_test_1_week_md_7['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_week_md_7 shape: {X_train_1_week_md_7.shape}, y_train_1_week_md_7 shape: {y_train_1_week_md_7.shape}\")\n",
    "print(f\"X_test_1_week_md_7 shape: {X_test_1_week_md_7.shape}, y_test_1_week_md_7 shape: {y_test_1_week_md_7.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_week_md_7.shape[0] == 0 or X_test_1_week_md_7.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_week_MD_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_week_MD_7.fit(X_train_1_week_md_7, y_train_1_week_md_7)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred_1_week_md_7 = model_1_week_MD_7.predict(X_test_1_week_md_7)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_week_md_7 = mean_squared_error(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test_1_week_md_7}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef582a0-083b-41ec-931f-8d6721af35c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_week_md_7` are your predictions for the test data and `y_test_1_week_md_7` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_week_md_7 = mean_squared_error(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "mae_1_week_md_7 = mean_absolute_error(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "rmse_1_week_md_7 = np.sqrt(mse_1_week_md_7)  # Root Mean Squared Error\n",
    "r2_1_week_md_7 = r2_score(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_week_md_7}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_week_md_7}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_week_md_7}')\n",
    "print(f'R-squared on unseen data: {r2_1_week_md_7}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_week_md_7 = median_absolute_error(y_test_1_week_md_7, y_pred_1_week_md_7)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_week_md_7}')\n",
    "\n",
    "dw_stat_1_week_md_7 = durbin_watson(y_test_1_week_md_7 - y_pred_1_week_md_7)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_week_md_7}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_week_md_7 = np.mean(np.abs((y_test_1_week_md_7 - y_pred_1_week_md_7) / y_test_1_week_md_7)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_week_md_7:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_week_md_7 = dict(zip(X_train_1_week_md_7.columns, model_1_week_MD_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_week_md_7 = sorted(feature_importance_1_week_md_7.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_week_md_7:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23429a-fd45-457f-a901-ebb2c578277f",
   "metadata": {},
   "source": [
    "best model: learning_rate = 0.01 and max_depth = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b727455-bcda-46ef-98ab-fcafa2f7d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features_1_week_md_3[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features_1_week_md_3[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 6)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=16, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=16, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 1 Week Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe46361-f5ba-45a1-9d2d-63f24006a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will modify our feature set to add bigger lagging indicators.\n",
    "# Create a new dataframe called 'df_stock_data_1_month' as a copy of 'df_stocks_price_ta'\n",
    "df_stock_data_1_month = df_stocks_price_ta.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e5901-1483-4177-b075-cebdf459bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to create lags for (focusing on short-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_5', 'EMA_5', 'Volume', 'EMA_12_MACD', 'SMA_20', 'EMA_20']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 12, 15, 20, 30, 60, 90, 180, 360] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7, 10, 12, 15, 20]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_stock_data_1_month[f'{col}_lag_{lag}'] = df_stock_data_1_month[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_stock_data_1_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da38ef-e130-4b2d-bc33-4e937217952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're going to move onto our next model: 1 month prediction\n",
    "# we'll start at our baseline model and then do the same as we just did\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_baseline = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_baseline = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_baseline.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_baseline['Close_Target'] = df_stock_data_train_1_month_baseline.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_baseline['Close_Target'] = df_stock_data_test_1_month_baseline.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_baseline = df_stock_data_train_1_month_baseline.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_baseline = df_stock_data_test_1_month_baseline.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_baseline[numeric_cols_train] = df_stock_data_train_1_month_baseline[numeric_cols_train].fillna(df_stock_data_train_1_month_baseline[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_baseline[numeric_cols_test] = df_stock_data_test_1_month_baseline[numeric_cols_test].fillna(df_stock_data_test_1_month_baseline[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_baseline.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_baseline.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_baseline = df_stock_data_train_1_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_baseline = df_stock_data_train_1_month_baseline['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_baseline = df_stock_data_test_1_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_baseline = df_stock_data_test_1_month_baseline['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_baseline shape: {X_train_1_month_baseline.shape}, y_train_1_month_baseline shape: {y_train_1_month_baseline.shape}\")\n",
    "print(f\"X_test_1_month_baseline shape: {X_test_1_month_baseline.shape}, y_test_1_month_baseline shape: {y_test_1_month_baseline.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_baseline.shape[0] == 0 or X_test_1_month_baseline.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_month.fit(X_train_1_month_baseline, y_train_1_month_baseline)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_baseline = model_baseline_1_month.predict(X_test_1_month_baseline)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_baseline = mean_squared_error(y_test_1_month_baseline, y_pred_1_month_baseline)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_baseline}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb8f43-31bf-4b04-9166-b2223d530da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_1_month_baseline` are your predictions for the test data and `y_test_1_month_baseline` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_month_baseline = mean_squared_error(y_test_1_month_baseline, y_pred_1_month_baseline)\n",
    "mae_1_month_baseline = mean_absolute_error(y_test_1_month_baseline, y_pred_1_month_baseline)\n",
    "rmse_1_month_baseline = np.sqrt(mse_1_month_baseline)  # Root Mean Squared Error\n",
    "r2_1_month_baseline = r2_score(y_test_1_month_baseline, y_pred_1_month_baseline)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_month_baseline}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_baseline}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_month_baseline}')\n",
    "print(f'R-squared on unseen data: {r2_1_month_baseline}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_baseline = median_absolute_error(y_test_1_month_baseline, y_pred_1_month_baseline)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_baseline}')\n",
    "\n",
    "dw_stat_1_month_baseline = durbin_watson(y_test_1_month_baseline - y_pred_1_month_baseline)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_baseline}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_baseline = np.mean(np.abs((y_test_1_month_baseline - y_pred_1_month_baseline) / y_test_1_month_baseline)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_baseline:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_month_baseline = dict(zip(X_train_1_month_baseline.columns, model_baseline_1_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_baseline = sorted(feature_importance_1_month_baseline.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_baseline:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf0fd0-3c2f-4376-b4a4-593709c8828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the baseline model (1-week prediction)\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_1_month.feature_importances_))\n",
    "\n",
    "# Filter features with importance greater than 1%\n",
    "important_features = {feature: importance for feature, importance in feature_importance.items() if importance > 0.01}\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_important_features = sorted(important_features.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the feature names (keys) into a list\n",
    "important_feature_names = [feature for feature, importance in sorted_important_features]\n",
    "\n",
    "# Print the sorted important features (optional)\n",
    "print(\"Features with more than 1% contribution:\")\n",
    "for feature in sorted_important_features:\n",
    "    print(f\"{feature[0]}: {feature[1] * 100:.2f}%\")\n",
    "\n",
    "# The list of important features that you can use to create a new dataframe\n",
    "print(\"List of important features:\")\n",
    "print(important_feature_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469fa610-eb03-442c-bbf2-bdbc978ded41",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = ['Symbol', 'Date', 'Close', 'Fib_30_High_Max', '30_day_Fib_23',\n",
    "                      'High', 'Low', 'Fib_30_Low_Min', 'Volume', 'EMA_5', '30_day_Fib_50',\n",
    "                      'Fib_5_Low_Min']\n",
    "df_important_feat_1_month = df_stock_data_1_month[important_features]\n",
    "df_important_feat_1_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ad4e4-9e00-4f0a-8232-ed5573072f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline 1 month prediction model with only features contributing over 1%\n",
    "# not as good as baseline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_important_feat_1_month = df_important_feat_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_if = df_important_feat_1_month[df_important_feat_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_if = df_important_feat_1_month[df_important_feat_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_if.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_if['Close_Target'] = df_stock_data_train_1_month_if.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_if['Close_Target'] = df_stock_data_test_1_month_if.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_if = df_stock_data_train_1_month_if.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_if = df_stock_data_test_1_month_if.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_if.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_if.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_if[numeric_cols_train] = df_stock_data_train_1_month_if[numeric_cols_train].fillna(df_stock_data_train_1_month_if[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_if[numeric_cols_test] = df_stock_data_test_1_month_if[numeric_cols_test].fillna(df_stock_data_test_1_month_if[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_if.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_if.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_if = df_stock_data_train_1_month_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_if = df_stock_data_train_1_month_if['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_if = df_stock_data_test_1_month_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_if = df_stock_data_test_1_month_if['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_if shape: {X_train_1_month_if.shape}, y_train_1_month_if shape: {y_train_1_month_if.shape}\")\n",
    "print(f\"X_test_1_month_if shape: {X_test_1_month_if.shape}, y_test_1_month_if shape: {y_test_1_month_if.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_if.shape[0] == 0 or X_test_1_month_if.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_if_1_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_if_1_month.fit(X_train_1_month_if, y_train_1_month_if)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_if = model_baseline_if_1_month.predict(X_test_1_month_if)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_if = mean_squared_error(y_test_1_month_if, y_pred_1_month_if)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_if}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20339820-f94b-4c03-83cd-3bf8ac18595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_month_if` are your predictions for the test data and `y_test_1_month_if` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_month_if = mean_squared_error(y_test_1_month_if, y_pred_1_month_if)\n",
    "mae_1_month_if = mean_absolute_error(y_test_1_month_if, y_pred_1_month_if)\n",
    "rmse_1_month_if = np.sqrt(mse_1_month_if)  # Root Mean Squared Error\n",
    "r2_1_month_if = r2_score(y_test_1_month_if, y_pred_1_month_if)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_month_if}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_if}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_month_if}')\n",
    "print(f'R-squared on unseen data: {r2_1_month_if}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_if = median_absolute_error(y_test_1_month_if, y_pred_1_month_if)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_if}')\n",
    "\n",
    "dw_stat_1_month_if = durbin_watson(y_test_1_month_if - y_pred_1_month_if)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_if}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_if = np.mean(np.abs((y_test_1_month_if - y_pred_1_month_if) / y_test_1_month_if)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_if:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_month_if = dict(zip(X_train_1_month_if.columns, model_baseline_if_1_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_if = sorted(feature_importance_1_month_if.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_if:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1861cb51-cd63-4973-944a-35e4679656d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 month baseline model with learning_rate=0.1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_lr_1 = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_lr_1 = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_lr_1.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_lr_1['Close_Target'] = df_stock_data_train_1_month_lr_1.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_lr_1['Close_Target'] = df_stock_data_test_1_month_lr_1.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_lr_1 = df_stock_data_train_1_month_lr_1.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_lr_1 = df_stock_data_test_1_month_lr_1.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_lr_1[numeric_cols_train] = df_stock_data_train_1_month_lr_1[numeric_cols_train].fillna(df_stock_data_train_1_month_lr_1[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_lr_1[numeric_cols_test] = df_stock_data_test_1_month_lr_1[numeric_cols_test].fillna(df_stock_data_test_1_month_lr_1[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_lr_1.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_lr_1.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_lr_1 = df_stock_data_train_1_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_lr_1 = df_stock_data_train_1_month_lr_1['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_lr_1 = df_stock_data_test_1_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_lr_1 = df_stock_data_test_1_month_lr_1['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_lr_1 shape: {X_train_1_month_lr_1.shape}, y_train_1_month_lr_1 shape: {y_train_1_month_lr_1.shape}\")\n",
    "print(f\"X_test_1_month_lr_1 shape: {X_test_1_month_lr_1.shape}, y_test_1_month_lr_1 shape: {y_test_1_month_lr_1.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_lr_1.shape[0] == 0 or X_test_1_month_lr_1.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_tr_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_tr_01.fit(X_train_1_month_lr_1, y_train_1_month_lr_1)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_lr_1 = model_1_month_tr_01.predict(X_test_1_month_lr_1)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_lr_1 = mean_squared_error(y_test_1_month_lr_1, y_pred_1_month_lr_1)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_lr_1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a25ea7-1c21-4032-9fcd-8e1963941b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_month_lr_1` are your predictions for the test data and `y_test_1_month_lr_1` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_month_lr_1 = mean_squared_error(y_test_1_month_lr_1, y_pred_1_month_lr_1)\n",
    "mae_1_month_lr_1 = mean_absolute_error(y_test_1_month_lr_1, y_pred_1_month_lr_1)\n",
    "rmse_1_month_lr_1 = np.sqrt(mse_1_month_lr_1)  # Root Mean Squared Error\n",
    "r2_1_month_lr_1 = r2_score(y_test_1_month_lr_1, y_pred_1_month_lr_1)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_month_lr_1}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_lr_1}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_month_lr_1}')\n",
    "print(f'R-squared on unseen data: {r2_1_month_lr_1}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_lr_1 = median_absolute_error(y_test_1_month_lr_1, y_pred_1_month_lr_1)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_lr_1}')\n",
    "\n",
    "dw_stat_1_month_lr_1 = durbin_watson(y_test_1_month_lr_1 - y_pred_1_month_lr_1)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_lr_1}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_lr_1 = np.mean(np.abs((y_test_1_month_lr_1 - y_pred_1_month_lr_1) / y_test_1_month_lr_1)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_lr_1:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_month_lr_1 = dict(zip(X_train_1_month_lr_1.columns, model_1_month_tr_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_lr_1 = sorted(feature_importance_1_month_lr_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_lr_1:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241485c-b2a1-41a6-ae31-03bf3500db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 month baseline model with learning_rate=0.01\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_lr_01 = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_lr_01 = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_lr_01.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_lr_01['Close_Target'] = df_stock_data_train_1_month_lr_01.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_lr_01['Close_Target'] = df_stock_data_test_1_month_lr_01.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_lr_01 = df_stock_data_train_1_month_lr_01.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_lr_01 = df_stock_data_test_1_month_lr_01.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_lr_01[numeric_cols_train] = df_stock_data_train_1_month_lr_01[numeric_cols_train].fillna(df_stock_data_train_1_month_lr_01[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_lr_01[numeric_cols_test] = df_stock_data_test_1_month_lr_01[numeric_cols_test].fillna(df_stock_data_test_1_month_lr_01[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_lr_01.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_lr_01.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_lr_01 = df_stock_data_train_1_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_lr_01 = df_stock_data_train_1_month_lr_01['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_lr_01 = df_stock_data_test_1_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_lr_01 = df_stock_data_test_1_month_lr_01['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_lr_01 shape: {X_train_1_month_lr_01.shape}, y_train_1_month_lr_01 shape: {y_train_1_month_lr_01.shape}\")\n",
    "print(f\"X_test_1_month_lr_01 shape: {X_test_1_month_lr_01.shape}, y_test_1_month_lr_01 shape: {y_test_1_month_lr_01.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_lr_01.shape[0] == 0 or X_test_1_month_lr_01.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_tr_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_tr_1.fit(X_train_1_month_lr_01, y_train_1_month_lr_01)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_lr_01 = model_1_month_tr_1.predict(X_test_1_month_lr_01)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_lr_01 = mean_squared_error(y_test_1_month_lr_01, y_pred_1_month_lr_01)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_lr_01}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2f654-7efe-4059-8f53-bbcfdfedf986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_1_month_lr_01` are your predictions for the test data and `y_test_1_month_lr_01` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_month_lr_01 = mean_squared_error(y_test_1_month_lr_01, y_pred_1_month_lr_01)\n",
    "mae_1_month_lr_01 = mean_absolute_error(y_test_1_month_lr_01, y_pred_1_month_lr_01)\n",
    "rmse_month_lr_01 = np.sqrt(mse_month_lr_01)  # Root Mean Squared Error\n",
    "r2_month_lr_01 = r2_score(y_test_1_month_lr_01, y_pred_1_month_lr_01)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_month_lr_01}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_lr_01}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_month_lr_01}')\n",
    "print(f'R-squared on unseen data: {r2_month_lr_01}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_lr_01 = median_absolute_error(y_test_1_month_lr_01, y_pred_1_month_lr_01)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_lr_01}')\n",
    "\n",
    "dw_stat_1_month_lr_01 = durbin_watson(y_test_1_month_lr_01 - y_pred_1_month_lr_01)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_lr_01}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_lr_01 = np.mean(np.abs((y_test_1_month_lr_01 - y_pred_1_month_lr_01) / y_test_1_month_lr_01)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_lr_01:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_month_lr_01 = dict(zip(X_train_1_month_lr_01.columns, model_1_month_tr_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_lr_01 = sorted(feature_importance_month_lr_01.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_lr_01:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3e270-fd95-4daa-86ef-fce7ca7ead0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with learning_rate = 0.01 is best again, so we keep that parameter\n",
    "# now we'll do max depth\n",
    "# max depth = 3\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_md_3 = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_md_3 = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_md_3.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_md_3['Close_Target'] = df_stock_data_train_1_month_md_3.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_md_3['Close_Target'] = df_stock_data_test_1_month_md_3.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_md_3 = df_stock_data_train_1_month_md_3.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_md_3 = df_stock_data_test_1_month_md_3.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_md_3[numeric_cols_train] = df_stock_data_train_1_month_md_3[numeric_cols_train].fillna(df_stock_data_train_1_month_md_3[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_md_3[numeric_cols_test] = df_stock_data_test_1_month_md_3[numeric_cols_test].fillna(df_stock_data_test_1_month_md_3[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_md_3.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_md_3.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_md_3 = df_stock_data_train_1_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_md_3 = df_stock_data_train_1_month_md_3['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_md_3 = df_stock_data_test_1_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_md_3 = df_stock_data_test_1_month_md_3['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_md_3 shape: {X_train_1_month_md_3.shape}, y_train_1_month_md_3 shape: {y_train_1_month_md_3.shape}\")\n",
    "print(f\"X_test_1_month_md_3 shape: {X_test_1_month_md_3.shape}, y_test_1_month_md_3 shape: {y_test_1_month_md_3.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_md_3.shape[0] == 0 or X_test_1_month_md_3.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_md_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_md_3.fit(X_train_1_month_md_3, y_train_1_month_md_3)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_md_3 = model_1_month_md_3.predict(X_test_1_month_md_3)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_md_3 = mean_squared_error(y_test_1_month_md_3, y_pred_1_month_md_3)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_md_3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb1a2c-a665-4731-b977-0d3e517b69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_1_month_md_3` are your predictions for the test data and `y_test_1_month_md_3` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_month_md_3 = mean_squared_error(y_test_1_month_md_3, y_pred_1_month_md_3)\n",
    "mae_1_month_md_3 = mean_absolute_error(y_test_1_month_md_3, y_pred_1_month_md_3)\n",
    "rmse_1_month_md_3 = np.sqrt(mse_1_month_md_3)  # Root Mean Squared Error\n",
    "r2_1_month_md_3 = r2_score(y_test_1_month_md_3, y_pred_1_month_md_3)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_month_md_3}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_md_3}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_month_md_3}')\n",
    "print(f'R-squared on unseen data: {r2_1_month_md_3}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_md_3 = median_absolute_error(y_test_1_month_md_3, y_pred_1_month_md_3)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_md_3}')\n",
    "\n",
    "dw_stat_1_month_md_3 = durbin_watson(y_test_1_month_md_3 - y_pred_1_month_md_3)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_md_3}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_md_3 = np.mean(np.abs((y_test_1_month_md_3 - y_pred_1_month_md_3) / y_test_1_month_md_3)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_md_3:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_month_md_3 = dict(zip(X_train_1_month_md_3.columns, model_1_month_md_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_md_3 = sorted(feature_importance_1_month_md_3.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_md_3:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c640a6d-da1c-4cb3-9f2b-20c872bf533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with learning_rate = 0.01 is best again, so we keep that parameter\n",
    "# now we'll do max depth\n",
    "# max depth = 7\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train_1_month_md_7 = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_1_month_md_7 = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_1_month_md_7.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_1_month_md_7['Close_Target'] = df_stock_data_train_1_month_md_7.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test_1_month_md_7['Close_Target'] = df_stock_data_test_1_month_md_7.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_1_month_md_7 = df_stock_data_train_1_month_md_7.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_1_month_md_7 = df_stock_data_test_1_month_md_7.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_1_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_1_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_1_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_1_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_1_month_md_7[numeric_cols_train] = df_stock_data_train_1_month_md_7[numeric_cols_train].fillna(df_stock_data_train_1_month_md_7[numeric_cols_train].median())\n",
    "df_stock_data_test_1_month_md_7[numeric_cols_test] = df_stock_data_test_1_month_md_7[numeric_cols_test].fillna(df_stock_data_test_1_month_md_7[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_1_month_md_7.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_1_month_md_7.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_1_month_md_7 = df_stock_data_train_1_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_1_month_md_7 = df_stock_data_train_1_month_md_7['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_1_month_md_7 = df_stock_data_test_1_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_1_month_md_7 = df_stock_data_test_1_month_md_7['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_1_month_md_7 shape: {X_train_1_month_md_7.shape}, y_train_1_month_md_7 shape: {y_train_1_month_md_7.shape}\")\n",
    "print(f\"X_test_1_month_md_7 shape: {X_test_1_month_md_7.shape}, y_test_1_month_md_7 shape: {y_test_1_month_md_7.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_1_month_md_7.shape[0] == 0 or X_test_1_month_md_7.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_md_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_md_7.fit(X_train_1_month_md_7, y_train_1_month_md_7)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_1_month_md_7 = model_1_month_md_7.predict(X_test_1_month_md_7)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_1_month_md_7 = mean_squared_error(y_test_1_month_md_7, y_pred_1_month_md_7)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_1_month_md_7}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f51e8b-000d-4892-9bd6-24ddb35a35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_1_month_md_7` are your predictions for the test data and `y_test_1_month_md_7` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_1_month_md_7 = mean_squared_error(y_test_1_month_md_7, y_pred_1_month_md_7)\n",
    "mae_1_month_md_7 = mean_absolute_error(y_test_1_month_md_7, y_pred_1_month_md_7)\n",
    "rmse_1_month_md_7 = np.sqrt(mse_1_month_md_7)  # Root Mean Squared Error\n",
    "r2_1_month_md_7 = r2_score(y_test_1_month_md_7, y_pred_1_month_md_7)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_1_month_md_7}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_1_month_md_7}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_1_month_md_7}')\n",
    "print(f'R-squared on unseen data: {r2_1_month_md_7}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_1_month_md_7 = median_absolute_error(y_test_1_month_md_7, y_pred_1_month_md_7)\n",
    "print(f'Median Absolute Error on unseen data: {medae_1_month_md_7}')\n",
    "\n",
    "dw_stat_1_month_md_7 = durbin_watson(y_test_1_month_md_7 - y_pred_1_month_md_7)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_1_month_md_7}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_1_month_md_7 = np.mean(np.abs((y_test_1_month_md_7 - y_pred_1_month_md_7) / y_test_1_month_md_7)) * 100\n",
    "print(f'MAPE on unseen data: {mape_1_month_md_7:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_1_month_md_7 = dict(zip(X_train_1_month_md_7.columns, model_1_month_md_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_1_month_md_7 = sorted(feature_importance_1_month_md_7.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_1_month_md_7:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df734c-0863-4b15-91e7-7f6c3b4ca6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features_1_month_md_7[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features_1_month_md_7[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 6)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=16, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=16, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 1 Month Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2163b5-dd6b-4c0f-94d3-8df2186bca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will modify our feature set to add bigger lagging indicators.\n",
    "# Create a new dataframe called 'df_stock_data_3_month' as a copy of 'df_stocks_price_ta'\n",
    "df_stock_data_3_month = df_stocks_price_ta.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc29a6-e7e2-42b4-a34e-828d6db188ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to create lags for (focusing on mid-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_5', 'EMA_5', 'Volume', 'SMA_20',\n",
    "       'SMA_50', 'EMA_5', 'EMA_20', 'EMA_50',  'EMA_12_MACD',\n",
    "       'EMA_26_MACD']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 12, 15, 20, 30, 60, 90, 180, 360] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7, 10, 15, 20, 25, 30, 40, 50, 60, 75, 90]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_stock_data_3_month.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01894bf1-679f-4c3d-a95e-3fcb8be05a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're going to move onto our next model: 3 month prediction\n",
    "# we'll start at our baseline model and then do the same as we just did\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_baseline = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_baseline = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_baseline.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_baseline['Close_Target'] = df_stock_data_train_3_month_baseline.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_baseline['Close_Target'] = df_stock_data_test_3_month_baseline.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_baseline = df_stock_data_train_3_month_baseline.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_baseline = df_stock_data_test_3_month_baseline.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_baseline[numeric_cols_train] = df_stock_data_train_3_month_baseline[numeric_cols_train].fillna(df_stock_data_train_3_month_baseline[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_baseline[numeric_cols_test] = df_stock_data_test_3_month_baseline[numeric_cols_test].fillna(df_stock_data_test_3_month_baseline[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_baseline.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_baseline.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_baseline = df_stock_data_train_3_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_baseline = df_stock_data_train_3_month_baseline['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_baseline = df_stock_data_test_3_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_baseline = df_stock_data_test_3_month_baseline['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_baseline shape: {X_train_3_month_baseline.shape}, y_train_3_month_baseline shape: {y_train_3_month_baseline.shape}\")\n",
    "print(f\"X_test_3_month_baseline shape: {X_test_3_month_baseline.shape}, y_test_3_month_baseline shape: {y_test_3_month_baseline.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_baseline.shape[0] == 0 or X_test_3_month_baseline.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_3_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_3_month.fit(X_train_3_month_baseline, y_train_3_month_baseline)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_baseline = model_baseline_3_month.predict(X_test_3_month_baseline)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_baseline = mean_squared_error(y_test_3_month_baseline, y_pred_3_month_baseline)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_baseline}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfae4e6-6f28-4d01-9880-4878ce427875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_3_month_baseline` are your predictions for the test data and `y_test_3_month_baseline` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_baseline = mean_squared_error(y_test_3_month_baseline, y_pred_3_month_baseline)\n",
    "mae_3_month_baseline = mean_absolute_error(y_test_3_month_baseline, y_pred_3_month_baseline)\n",
    "rmse_3_month_baseline = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2_3_month_baseline = r2_score(y_test_3_month_baseline, y_pred_3_month_baseline)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_baseline}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_baseline}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_baseline}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_baseline}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_baseline = median_absolute_error(y_test_3_month_baseline, y_pred_3_month_baseline)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_baseline}')\n",
    "\n",
    "dw_stat_3_month_baseline = durbin_watson(y_test_3_month_baseline - y_pred_3_month_baseline)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_baseline}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_baseline = np.mean(np.abs((y_test_3_month_baseline - y_pred_3_month_baseline) / y_test_3_month_baseline)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_baseline:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_baseline = dict(zip(X_train_3_month_baseline.columns, model_baseline_3_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_baseline = sorted(feature_importance_3_month_baseline.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_baseline:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ebb9a-2e8c-4b94-a240-f74295bb8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the baseline model (1-week prediction)\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_3_month.feature_importances_))\n",
    "\n",
    "# Filter features with importance greater than 1%\n",
    "important_features = {feature: importance for feature, importance in feature_importance.items() if importance > 0.01}\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_important_features = sorted(important_features.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the feature names (keys) into a list\n",
    "important_feature_names = [feature for feature, importance in sorted_important_features]\n",
    "\n",
    "# Print the sorted important features (optional)\n",
    "print(\"Features with more than 1% contribution:\")\n",
    "for feature in sorted_important_features:\n",
    "    print(f\"{feature[0]}: {feature[1] * 100:.2f}%\")\n",
    "\n",
    "# The list of important features that you can use to create a new dataframe\n",
    "print(\"List of important features:\")\n",
    "print(important_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03ff90-2e8c-489c-8482-f189fd3cbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = ['Symbol', 'Date', 'Close', 'Fib_30_Low_Min', '30_day_Fib_38',\n",
    "                      '5_day-Fib_61', '5_day-Fib_23', 'EMA_5', 'Volume',\n",
    "                      'EMA_12_MACD', 'High', 'Low', '30_day_Fib_61',\n",
    "                      'ATR_Prev_Close', 'Fib_5_Low_Min']\n",
    "df_important_feat_3_month = df_stock_data_3_month[important_features]\n",
    "df_important_feat_3_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907f0d3-cb76-4323-a038-1468b2b071ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 month prediction with only important featuers\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_important_feat_3_month = df_important_feat_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_if = df_important_feat_3_month[df_important_feat_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_if = df_important_feat_3_month[df_important_feat_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_if.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_if['Close_Target'] = df_stock_data_train_3_month_if.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_if['Close_Target'] = df_stock_data_test_3_month_if.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_if = df_stock_data_train_3_month_if.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_if = df_stock_data_test_3_month_if.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_if.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_if.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_if.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_if[numeric_cols_train] = df_stock_data_train_3_month_if[numeric_cols_train].fillna(df_stock_data_train_3_month_if[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_if[numeric_cols_test] = df_stock_data_test_3_month_if[numeric_cols_test].fillna(df_stock_data_test_3_month_if[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_if.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_if.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_if = df_stock_data_train_3_month_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_if = df_stock_data_train_3_month_if['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_if = df_stock_data_test_3_month_if.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_if = df_stock_data_test_3_month_if['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_if shape: {X_train_3_month_if.shape}, y_train_3_month_if shape: {y_train_3_month_if.shape}\")\n",
    "print(f\"X_test_3_month_if shape: {X_test_3_month_if.shape}, y_test_3_month_if shape: {y_test_3_month_if.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_if.shape[0] == 0 or X_test_3_month_if.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_if_3_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_if_3_month.fit(X_train_3_month_if, y_train_3_month_if)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_if = model_baseline_if_3_month.predict(X_test_3_month_if)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_if = mean_squared_error(y_test_3_month_if, y_pred_3_month_if)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_if}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52695914-d9ee-493e-8795-ad01a148d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_3_month_if` are your predictions for the test data and `y_test_3_month_if` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_if = mean_squared_error(y_test_3_month_if, y_pred_3_month_if)\n",
    "mae_3_month_if = mean_absolute_error(y_test_3_month_if, y_pred_3_month_if)\n",
    "rmse_3_month_if = np.sqrt(mse_3_month_if)  # Root Mean Squared Error\n",
    "r2_3_month_if = r2_score(y_test_3_month_if, y_pred_3_month_if)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_if}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_if}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_if}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_if}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_if = median_absolute_error(y_test_3_month_if, y_pred_3_month_if)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_if}')\n",
    "\n",
    "dw_stat_3_month_if = durbin_watson(y_test_3_month_if - y_pred_3_month_if)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_if}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_if = np.mean(np.abs((y_test_3_month_if - y_pred_3_month_if) / y_test_3_month_if)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_if:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_if = dict(zip(X_train_3_month_if.columns, model_baseline_if_3_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_if = sorted(feature_importance_3_month_if.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_if:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8314d71a-686c-4902-8101-1bc25fa85b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = 0.1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_lr_1 = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_lr_1 = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_lr_1.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_lr_1['Close_Target'] = df_stock_data_train_3_month_lr_1.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_lr_1['Close_Target'] = df_stock_data_test_3_month_lr_1.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_lr_1 = df_stock_data_train_3_month_lr_1.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_lr_1 = df_stock_data_test_3_month_lr_1.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_lr_1[numeric_cols_train] = df_stock_data_train_3_month_lr_1[numeric_cols_train].fillna(df_stock_data_train_3_month_lr_1[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_lr_1[numeric_cols_test] = df_stock_data_test_3_month_lr_1[numeric_cols_test].fillna(df_stock_data_test_3_month_lr_1[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_lr_1.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_lr_1.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_lr_1 = df_stock_data_train_3_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_lr_1 = df_stock_data_train_3_month_lr_1['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_lr_1 = df_stock_data_test_3_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_lr_1 = df_stock_data_test_3_month_lr_1['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_lr_1 shape: {X_train_3_month_lr_1.shape}, y_train_3_month_lr_1 shape: {y_train_3_month_lr_1.shape}\")\n",
    "print(f\"X_test_3_month_lr_1 shape: {X_test_3_month_lr_1.shape}, y_test_3_month_lr_1 shape: {y_test_3_month_lr_1.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_lr_1.shape[0] == 0 or X_test_3_month_lr_1.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_tf_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_tf_1.fit(X_train_3_month_lr_1, y_train_3_month_lr_1)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_lr_1 = model_3_month_tf_1.predict(X_test_3_month_lr_1)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_lr_1 = mean_squared_error(y_test_3_month_lr_1, y_pred_3_month_lr_1)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_lr_1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da607ba9-e76b-48e3-94b4-700d86cc9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_3_month_lr_1` are your predictions for the test data and `y_test_3_month_lr_1` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_lr_1 = mean_squared_error(y_test_3_month_lr_1, y_pred_3_month_lr_1)\n",
    "mae_3_month_lr_1 = mean_absolute_error(y_test_3_month_lr_1, y_pred_3_month_lr_1)\n",
    "rmse_3_month_lr_1 = np.sqrt(mse_3_month_lr_1)  # Root Mean Squared Error\n",
    "r2_3_month_lr_1 = r2_score(y_test_3_month_lr_1, y_pred_3_month_lr_1)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_lr_1}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_lr_1}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_lr_1}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_lr_1}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_lr_1 = median_absolute_error(y_test_3_month_lr_1, y_pred_3_month_lr_1)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_lr_1}')\n",
    "\n",
    "dw_stat_3_month_lr_1 = durbin_watson(y_test_3_month_lr_1 - y_pred_3_month_lr_1)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_lr_1}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_lr_1 = np.mean(np.abs((y_test_3_month_lr_1 - y_pred_3_month_lr_1) / y_test_3_month_lr_1)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_lr_1:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_lr_1 = dict(zip(X_train_3_month_lr_1.columns, model_3_month_tf_1.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_lr_1 = sorted(feature_importance_3_month_lr_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_lr_1:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c086d1f-6b98-4cc0-94b4-d7a2a347ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = 0.01\n",
    "# this is the best one again\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_lr_01 = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_lr_01 = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_lr_01.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_lr_01['Close_Target'] = df_stock_data_train_3_month_lr_01.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_lr_01['Close_Target'] = df_stock_data_test_3_month_lr_01.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_lr_01 = df_stock_data_train_3_month_lr_01.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_lr_01 = df_stock_data_test_3_month_lr_01.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_lr_01[numeric_cols_train] = df_stock_data_train_3_month_lr_01[numeric_cols_train].fillna(df_stock_data_train_3_month_lr_01[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_lr_01[numeric_cols_test] = df_stock_data_test_3_month_lr_01[numeric_cols_test].fillna(df_stock_data_test_3_month_lr_01[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_lr_01.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_lr_01.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_lr_01 = df_stock_data_train_3_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_lr_01 = df_stock_data_train_3_month_lr_01['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_lr_01 = df_stock_data_test_3_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_lr_01 = df_stock_data_test_3_month_lr_01['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_lr_01 shape: {X_train_3_month_lr_01.shape}, y_train_3_month_lr_01 shape: {y_train_3_month_lr_01.shape}\")\n",
    "print(f\"X_test_3_month_lr_01 shape: {X_test_3_month_lr_01.shape}, y_test_3_month_lr_01 shape: {y_test_3_month_lr_01.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_lr_01.shape[0] == 0 or X_test_3_month_lr_01.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_tf_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_tf_01.fit(X_train_3_month_lr_01, y_train_3_month_lr_01)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_lr_01 = model_3_month_tf_01.predict(X_test_3_month_lr_01)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_lr_01 = mean_squared_error(y_test_3_month_lr_01, y_pred_3_month_lr_01)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_lr_01}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e4f7c-a236-4bdf-b030-16da772eb9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_3_month_lr_01` are your predictions for the test data and `y_test_3_month_lr_01` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_lr_01 = mean_squared_error(y_test_3_month_lr_01, y_pred_3_month_lr_01)\n",
    "mae_3_month_lr_01 = mean_absolute_error(y_test_3_month_lr_01, y_pred_3_month_lr_01)\n",
    "rmse_3_month_lr_01 = np.sqrt(mse_3_month_lr_01)  # Root Mean Squared Error\n",
    "r2_3_month_lr_01 = r2_score(y_test_3_month_lr_01, y_pred_3_month_lr_01)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_lr_01}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_lr_01}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_lr_01}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_lr_01}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_lr_01 = median_absolute_error(y_test_3_month_lr_01, y_pred_3_month_lr_01)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_lr_01}')\n",
    "\n",
    "dw_stat_3_month_lr_01 = durbin_watson(y_test_3_month_lr_01 - y_pred_3_month_lr_01)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_lr_01}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_lr_01 = np.mean(np.abs((y_test_3_month_lr_01 - y_pred_3_month_lr_01) / y_test_3_month_lr_01)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_lr_01:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_lr_01 = dict(zip(X_train_3_month_lr_01.columns, model_3_month_tf_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_lr_01 = sorted(feature_importance_3_month_lr_01.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_lr_01:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348432af-38f0-446c-a678-3764e9a2fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = 0.01\n",
    "# max depth 3\n",
    "# this is actually the best one for 3 months now\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_md_3 = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_md_3 = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_md_3.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_md_3['Close_Target'] = df_stock_data_train_3_month_md_3.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_md_3['Close_Target'] = df_stock_data_test_3_month_md_3.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_md_3 = df_stock_data_train_3_month_md_3.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_md_3 = df_stock_data_test_3_month_md_3.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_md_3[numeric_cols_train] = df_stock_data_train_3_month_md_3[numeric_cols_train].fillna(df_stock_data_train_3_month_md_3[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_md_3[numeric_cols_test] = df_stock_data_test_3_month_md_3[numeric_cols_test].fillna(df_stock_data_test_3_month_md_3[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_md_3.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_md_3.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_md_3 = df_stock_data_train_3_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_md_3 = df_stock_data_train_3_month_md_3['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_md_3 = df_stock_data_test_3_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_md_3 = df_stock_data_test_3_month_md_3['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_md_3 shape: {X_train_3_month_md_3.shape}, y_train_3_month_md_3 shape: {y_train_3_month_md_3.shape}\")\n",
    "print(f\"X_test_3_month_md_3 shape: {X_test_3_month_md_3.shape}, y_test_3_month_md_3 shape: {y_test_3_month_md_3.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_md_3.shape[0] == 0 or X_test_3_month_md_3.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_md_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_md_3.fit(X_train_3_month_md_3, y_train_3_month_md_3)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_md_3 = model_3_month_md_3.predict(X_test_3_month_md_3)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_md_3 = mean_squared_error(y_test_3_month_md_3, y_pred_3_month_md_3)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_md_3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89181266-05d0-4c3d-8f06-93fa693c9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_3_month_md_3` are your predictions for the test data and `y_test_3_month_md_3` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_md_3 = mean_squared_error(y_test_3_month_md_3, y_pred_3_month_md_3)\n",
    "mae_3_month_md_3 = mean_absolute_error(y_test_3_month_md_3, y_pred_3_month_md_3)\n",
    "rmse_3_month_md_3 = np.sqrt(mse_3_month_md_3)  # Root Mean Squared Error\n",
    "r2_3_month_md_3 = r2_score(y_test_3_month_md_3, y_pred_3_month_md_3)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_md_3}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_md_3}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_md_3}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_md_3}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_md_3 = median_absolute_error(y_test_3_month_md_3, y_pred_3_month_md_3)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_md_3}')\n",
    "\n",
    "dw_stat_3_month_md_3 = durbin_watson(y_test_3_month_md_3 - y_pred_3_month_md_3)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_md_3}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_md_3 = np.mean(np.abs((y_test_3_month_md_3 - y_pred_3_month_md_3) / y_test_3_month_md_3)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_md_3:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_md_3 = dict(zip(X_train_3_month_md_3.columns, model_3_month_md_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_md_3 = sorted(feature_importance_3_month_md_3.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_md_3:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86699263-cf02-41ab-8ce1-a26c9531fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = 0.01\n",
    "# max depth 7\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_3_month_md_7 = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_3_month_md_7 = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_3_month_md_7.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_3_month_md_7['Close_Target'] = df_stock_data_train_3_month_md_7.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test_3_month_md_7['Close_Target'] = df_stock_data_test_3_month_md_7.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_3_month_md_7 = df_stock_data_train_3_month_md_7.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_3_month_md_7 = df_stock_data_test_3_month_md_7.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_3_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_3_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_3_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_3_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_3_month_md_7[numeric_cols_train] = df_stock_data_train_3_month_md_7[numeric_cols_train].fillna(df_stock_data_train_3_month_md_7[numeric_cols_train].median())\n",
    "df_stock_data_test_3_month_md_7[numeric_cols_test] = df_stock_data_test_3_month_md_7[numeric_cols_test].fillna(df_stock_data_test_3_month_md_7[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_3_month_md_7.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_3_month_md_7.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_3_month_md_7 = df_stock_data_train_3_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_3_month_md_7 = df_stock_data_train_3_month_md_7['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_3_month_md_7 = df_stock_data_test_3_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_3_month_md_7 = df_stock_data_test_3_month_md_7['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_3_month_md_7 shape: {X_train_3_month_md_7.shape}, y_train_3_month_md_7 shape: {y_train_3_month_md_7.shape}\")\n",
    "print(f\"X_test_3_month_md_7 shape: {X_test_3_month_md_7.shape}, y_test_3_month_md_7 shape: {y_test_3_month_md_7.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_3_month_md_7.shape[0] == 0 or X_test_3_month_md_7.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_md_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_md_7.fit(X_train_3_month_md_7, y_train_3_month_md_7)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_3_month_md_7 = model_3_month_md_7.predict(X_test_3_month_md_7)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_3_month_md_7 = mean_squared_error(y_test_3_month_md_7, y_pred_3_month_md_7)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_3_month_md_7}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd94b99-7fca-49a4-bcf0-716ad08f459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_3_month_md_7` are your predictions for the test data and `y_test_3_month_md_7` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_3_month_md_7 = mean_squared_error(y_test_3_month_md_7, y_pred_3_month_md_7)\n",
    "mae_3_month_md_7 = mean_absolute_error(y_test_3_month_md_7, y_pred_3_month_md_7)\n",
    "rmse_3_month_md_7 = np.sqrt(mse_3_month_md_7)  # Root Mean Squared Error\n",
    "r2_3_month_md_7 = r2_score(y_test_3_month_md_7, y_pred_3_month_md_7)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_3_month_md_7}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_3_month_md_7}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_3_month_md_7}')\n",
    "print(f'R-squared on unseen data: {r2_3_month_md_7}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_3_month_md_7 = median_absolute_error(y_test_3_month_md_7, y_pred_3_month_md_7)\n",
    "print(f'Median Absolute Error on unseen data: {medae_3_month_md_7}')\n",
    "\n",
    "dw_stat_3_month_md_7 = durbin_watson(y_test_3_month_md_7 - y_pred_3_month_md_7)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_3_month_md_7}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_3_month_md_7 = np.mean(np.abs((y_test_3_month_md_7 - y_pred_3_month_md_7) / y_test_3_month_md_7)) * 100\n",
    "print(f'MAPE on unseen data: {mape_3_month_md_7:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_3_month_md_7 = dict(zip(X_train_3_month_md_7.columns, model_3_month_md_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_3_month_md_7 = sorted(feature_importance_3_month_md_7.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_3_month_md_7:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2757dd-30da-423a-80a6-7b9bc4eeec73",
   "metadata": {},
   "source": [
    "best model: learning_rate = 0.01 and max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a7d87-e136-4beb-985c-b1989d1d6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features_3_month_md_3[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features_3_month_md_3[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 3)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=18, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=18, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 3 Month Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c48d5c5-d7c5-49d8-b601-312cbc13048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will modify our feature set to add bigger lagging indicators.\n",
    "# Create a new dataframe called 'df_stock_data_6_month' as a copy of 'df_stocks_price_ta'\n",
    "df_stock_data_6_month = df_stocks_price_ta.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e314a56-c23b-42b9-a9f0-bbf269d95b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to create lags for (focusing on mid-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_5', 'EMA_5', 'Volume', 'SMA_20',\n",
    "       'SMA_50', 'EMA_5', 'EMA_20', 'EMA_50',  'EMA_12_MACD',\n",
    "       'EMA_26_MACD']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 12, 15, 20, 30, 60, 90, 180, 360] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7, 10, 15, 20, 25, 30, 40, 50, 60, 75, 90, 180]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_stock_data_6_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f34d1-1235-42aa-8716-fe89165ce334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're going to move onto our next model: 6 month prediction\n",
    "# we'll start at our baseline model and then do the same as we just did\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_6_month_baseline = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_6_month_baseline = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_6_month_baseline.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_6_month_baseline['Close_Target'] = df_stock_data_train_6_month_baseline.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test_6_month_baseline['Close_Target'] = df_stock_data_test_6_month_baseline.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_6_month_baseline = df_stock_data_train_6_month_baseline.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_6_month_baseline = df_stock_data_test_6_month_baseline.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_6_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_6_month_baseline.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_6_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_6_month_baseline.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_6_month_baseline[numeric_cols_train] = df_stock_data_train_6_month_baseline[numeric_cols_train].fillna(df_stock_data_train_6_month_baseline[numeric_cols_train].median())\n",
    "df_stock_data_test_6_month_baseline[numeric_cols_test] = df_stock_data_test_6_month_baseline[numeric_cols_test].fillna(df_stock_data_test_6_month_baseline[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_6_month_baseline.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_6_month_baseline.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_6_month_baseline = df_stock_data_train_6_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_6_month_baseline = df_stock_data_train_6_month_baseline['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_6_month_baseline = df_stock_data_test_6_month_baseline.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_6_month_baseline = df_stock_data_test_6_month_baseline['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_6_month_baseline shape: {X_train_6_month_baseline.shape}, y_train_6_month_baseline shape: {y_train_6_month_baseline.shape}\")\n",
    "print(f\"X_test_6_month_baseline shape: {X_test_6_month_baseline.shape}, y_test_6_month_baseline shape: {y_test_6_month_baseline.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_6_month_baseline.shape[0] == 0 or X_test_6_month_baseline.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month.fit(X_train_6_month_baseline, y_train_6_month_baseline)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_6_month_baseline = model_baseline_6_month.predict(X_test_6_month_baseline)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_6_month_baseline = mean_squared_error(y_test_6_month_baseline, y_pred_6_month_baseline)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_6_month_baseline}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09aee03-9fcd-4fdb-88d5-0c9e0e73b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred_6_month_baseline` are your predictions for the test data and `y_test_6_month_baseline` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_6_month_baseline = mean_squared_error(y_test_6_month_baseline, y_pred_6_month_baseline)\n",
    "mae_6_month_baseline = mean_absolute_error(y_test_6_month_baseline, y_pred_6_month_baseline)\n",
    "rmse_6_month_baseline = np.sqrt(mse_6_month_baseline)  # Root Mean Squared Error\n",
    "r2_6_month_baseline = r2_score(y_test_6_month_baseline, y_pred_6_month_baseline)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_6_month_baseline}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_6_month_baseline}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_6_month_baseline}')\n",
    "print(f'R-squared on unseen data: {r2_6_month_baseline}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_6_month_baseline = median_absolute_error(y_test_6_month_baseline, y_pred_6_month_baseline)\n",
    "print(f'Median Absolute Error on unseen data: {medae_6_month_baseline}')\n",
    "\n",
    "dw_stat_6_month_baseline = durbin_watson(y_test_6_month_baseline - y_pred_6_month_baseline)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_6_month_baseline}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_6_month_baseline = np.mean(np.abs((y_test_6_month_baseline - y_pred_6_month_baseline) / y_test_6_month_baseline)) * 100\n",
    "print(f'MAPE on unseen data: {mape_6_month_baseline:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_6_month_baseline = dict(zip(X_train_6_month_baseline.columns, model_baseline_6_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_6_month_baseline = sorted(feature_importance_6_month_baseline.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_6_month_baseline:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28bc7d-0e99-4862-9908-d092c1b0408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_6_month_lr_1 = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_6_month_lr_1 = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_6_month_lr_1.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_6_month_lr_1['Close_Target'] = df_stock_data_train_6_month_lr_1.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test_6_month_lr_1['Close_Target'] = df_stock_data_test_6_month_lr_1.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_6_month_lr_1 = df_stock_data_train_6_month_lr_1.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_6_month_lr_1 = df_stock_data_test_6_month_lr_1.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_6_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_6_month_lr_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_6_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_6_month_lr_1.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_6_month_lr_1[numeric_cols_train] = df_stock_data_train_6_month_lr_1[numeric_cols_train].fillna(df_stock_data_train_6_month_lr_1[numeric_cols_train].median())\n",
    "df_stock_data_test_6_month_lr_1[numeric_cols_test] = df_stock_data_test_6_month_lr_1[numeric_cols_test].fillna(df_stock_data_test_6_month_lr_1[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_6_month_lr_1.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_6_month_lr_1.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_6_month_lr_1 = df_stock_data_train_6_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_6_month_lr_1 = df_stock_data_train_6_month_lr_1['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_6_month_lr_1 = df_stock_data_test_6_month_lr_1.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_6_month_lr_1 = df_stock_data_test_6_month_lr_1['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_6_month_lr_1 shape: {X_train_6_month_lr_1.shape}, y_train_6_month_lr_1 shape: {y_train_6_month_lr_1.shape}\")\n",
    "print(f\"X_test_6_month_lr_1 shape: {X_test_6_month_lr_1.shape}, y_test_6_month_lr_1 shape: {y_test_6_month_lr_1.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_6_month_lr_1.shape[0] == 0 or X_test_6_month_lr_1.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_lr_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_lr_1.fit(X_train_6_month_lr_1, y_train_6_month_lr_1)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_6_month_lr_1 = model_baseline_6_month_lr_1.predict(X_test_6_month_lr_1)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test_6_month_lr_1, y_pred_6_month_lr_1)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0079e37-5c7f-40ee-ba59-50bde2f72d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_6_month_lr_1` are your predictions for the test data and `y_test_6_month_lr_1` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_6_month_lr_1 = mean_squared_error(y_test_6_month_lr_1, y_pred_6_month_lr_1)\n",
    "mae_6_month_lr_1 = mean_absolute_error(y_test_6_month_lr_1, y_pred_6_month_lr_1)\n",
    "rmse_6_month_lr_1 = np.sqrt(mse_6_month_lr_1)  # Root Mean Squared Error\n",
    "r2_6_month_lr_1 = r2_score(y_test_6_month_lr_1, y_pred_6_month_lr_1)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_6_month_lr_1}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_6_month_lr_1}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_6_month_lr_1}')\n",
    "print(f'R-squared on unseen data: {r2_6_month_lr_1}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_6_month_lr_1 = median_absolute_error(y_test_6_month_lr_1, y_pred_6_month_lr_1)\n",
    "print(f'Median Absolute Error on unseen data: {medae_6_month_lr_1}')\n",
    "\n",
    "dw_stat_6_month_lr_1 = durbin_watson(y_test_6_month_lr_1 - y_pred_6_month_lr_1)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_6_month_lr_1}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_6_month_lr_1 = np.mean(np.abs((y_test_6_month_lr_1 - y_pred_6_month_lr_1) / y_test_6_month_lr_1)) * 100\n",
    "print(f'MAPE on unseen data: {mape_6_month_lr_1:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_6_month_lr_1 = dict(zip(X_train_6_month_lr_1.columns, model_baseline_6_month_lr_1.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_6_month_lr_1 = sorted(feature_importance_6_month_lr_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_6_month_lr_1:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368cfb0-0392-4be4-a586-29b4ce0bc69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.01\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_6_month_lr_01 = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_6_month_lr_01 = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_6_month_lr_01.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_6_month_lr_01['Close_Target'] = df_stock_data_train_6_month_lr_01.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test_6_month_lr_01['Close_Target'] = df_stock_data_test_6_month_lr_01.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_6_month_lr_01 = df_stock_data_train_6_month_lr_01.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_6_month_lr_01 = df_stock_data_test_6_month_lr_01.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_6_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_6_month_lr_01.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_6_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_6_month_lr_01.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_6_month_lr_01[numeric_cols_train] = df_stock_data_train_6_month_lr_01[numeric_cols_train].fillna(df_stock_data_train_6_month_lr_01[numeric_cols_train].median())\n",
    "df_stock_data_test_6_month_lr_01[numeric_cols_test] = df_stock_data_test_6_month_lr_01[numeric_cols_test].fillna(df_stock_data_test_6_month_lr_01[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_6_month_lr_01.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_6_month_lr_01.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_6_month_lr_01 = df_stock_data_train_6_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_6_month_lr_01 = df_stock_data_train_6_month_lr_01['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_6_month_lr_01 = df_stock_data_test_6_month_lr_01.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_6_month_lr_01 = df_stock_data_test_6_month_lr_01['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_6_month_lr_01 shape: {X_train_6_month_lr_01.shape}, y_train_6_month_lr_01 shape: {y_train_6_month_lr_01.shape}\")\n",
    "print(f\"X_test_6_month_lr_01 shape: {X_test_6_month_lr_01.shape}, y_test_6_month_lr_01 shape: {y_test_6_month_lr_01.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_6_month_lr_01.shape[0] == 0 or X_test_6_month_lr_01.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_lr_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_lr_01.fit(X_train_6_month_lr_01, y_train_6_month_lr_01)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_6_month_lr_01 = model_baseline_6_month_lr_01.predict(X_test_6_month_lr_01)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_6_month_lr_01 = mean_squared_error(y_test_6_month_lr_01, y_pred_6_month_lr_01)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_6_month_lr_01}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac22b48-ec53-40ad-bbab-4b9a39f7abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_6_month_lr_01` are your predictions for the test data and `y_test_6_month_lr_01` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_6_month_lr_01 = mean_squared_error(y_test_6_month_lr_01, y_pred_6_month_lr_01)\n",
    "mae_6_month_lr_01 = mean_absolute_error(y_test_6_month_lr_01, y_pred_6_month_lr_01)\n",
    "rmse_6_month_lr_01 = np.sqrt(mse_6_month_lr_01)  # Root Mean Squared Error\n",
    "r2_6_month_lr_01 = r2_score(y_test_6_month_lr_01, y_pred_6_month_lr_01)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_6_month_lr_01}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_6_month_lr_01}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_6_month_lr_01}')\n",
    "print(f'R-squared on unseen data: {r2_6_month_lr_01}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_6_month_lr_01 = median_absolute_error(y_test_6_month_lr_01, y_pred_6_month_lr_01)\n",
    "print(f'Median Absolute Error on unseen data: {medae_6_month_lr_01}')\n",
    "\n",
    "dw_stat_6_month_lr_01 = durbin_watson(y_test_6_month_lr_01 - y_pred_6_month_lr_01)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_6_month_lr_01}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_6_month_lr_01 = np.mean(np.abs((y_test_6_month_lr_01 - y_pred_6_month_lr_01) / y_test_6_month_lr_01)) * 100\n",
    "print(f'MAPE on unseen data: {mape_6_month_lr_01:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_6_month_lr_01 = dict(zip(X_train_6_month_lr_01.columns, model_baseline_6_month_lr_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_6_month_lr_01 = sorted(feature_importance_6_month_lr_01.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_6_month_lr_01:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be07e445-4b9a-4531-bf37-ff3d8725a3d3",
   "metadata": {},
   "source": [
    "model with learning_rate 0.01 performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447c125-546d-49b9-bf7f-43ba6018f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.01\n",
    "# max_depth = 3\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_6_month_md_3 = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_6_month_md_3 = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_6_month_md_3.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_6_month_md_3['Close_Target'] = df_stock_data_train_6_month_md_3.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test_6_month_md_3['Close_Target'] = df_stock_data_test_6_month_md_3.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_6_month_md_3 = df_stock_data_train_6_month_md_3.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_6_month_md_3 = df_stock_data_test_6_month_md_3.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_6_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_6_month_md_3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_6_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_6_month_md_3.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_6_month_md_3[numeric_cols_train] = df_stock_data_train_6_month_md_3[numeric_cols_train].fillna(df_stock_data_train_6_month_md_3[numeric_cols_train].median())\n",
    "df_stock_data_test_6_month_md_3[numeric_cols_test] = df_stock_data_test_6_month_md_3[numeric_cols_test].fillna(df_stock_data_test_6_month_md_3[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_6_month_md_3.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_6_month_md_3.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_6_month_md_3 = df_stock_data_train_6_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_6_month_md_3 = df_stock_data_train_6_month_md_3['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_6_month_md_3 = df_stock_data_test_6_month_md_3.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_6_month_md_3 = df_stock_data_test_6_month_md_3['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_6_month_md_3 shape: {X_train_6_month_md_3.shape}, y_train_6_month_md_3 shape: {y_train_6_month_md_3.shape}\")\n",
    "print(f\"X_test_6_month_md_3 shape: {X_test_6_month_md_3.shape}, y_test_6_month_md_3 shape: {y_test_6_month_md_3.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_6_month_md_3.shape[0] == 0 or X_test_6_month_md_3.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_md_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_md_3.fit(X_train_6_month_md_3, y_train_6_month_md_3)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_6_month_md_3 = model_baseline_6_month_md_3.predict(X_test_6_month_md_3)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_6_month_md_3 = mean_squared_error(y_test_6_month_md_3, y_pred_6_month_md_3)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_6_month_md_3}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ac369-2efa-460f-b7d2-733d5302933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_6_month_md_3` are your predictions for the test data and `y_test_6_month_md_3` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_6_month_md_3 = mean_squared_error(y_test_6_month_md_3, y_pred_6_month_md_3)\n",
    "mae_6_month_md_3 = mean_absolute_error(y_test_6_month_md_3, y_pred_6_month_md_3)\n",
    "rmse_6_month_md_3 = np.sqrt(mse_6_month_md_3)  # Root Mean Squared Error\n",
    "r2_6_month_md_3 = r2_score(y_test_6_month_md_3, y_pred_6_month_md_3)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_6_month_md_3}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_6_month_md_3}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_6_month_md_3}')\n",
    "print(f'R-squared on unseen data: {r2_6_month_md_3}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_6_month_md_3 = median_absolute_error(y_test_6_month_md_3, y_pred_6_month_md_3)\n",
    "print(f'Median Absolute Error on unseen data: {medae_6_month_md_3}')\n",
    "\n",
    "dw_stat_6_month_md_3 = durbin_watson(y_test_6_month_md_3 - y_pred_6_month_md_3)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_6_month_md_3}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_6_month_md_3 = np.mean(np.abs((y_test_6_month_md_3 - y_pred_6_month_md_3) / y_test_6_month_md_3)) * 100\n",
    "print(f'MAPE on unseen data: {mape_6_month_md_3:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_6_month_md_3 = dict(zip(X_train_6_month_md_3.columns, model_baseline_6_month_md_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_6_month_md_3 = sorted(feature_importance_6_month_md_3.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_6_month_md_3:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba1716-99b9-4fad-a2bc-74cbb5e5f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.01\n",
    "# max_depth = 7\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train_6_month_md_7 = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test_6_month_md_7 = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test_6_month_md_7.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train_6_month_md_7['Close_Target'] = df_stock_data_train_6_month_md_7.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test_6_month_md_7['Close_Target'] = df_stock_data_test_6_month_md_7.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train_6_month_md_7 = df_stock_data_train_6_month_md_7.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test_6_month_md_7 = df_stock_data_test_6_month_md_7.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train_6_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test_6_month_md_7.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train_6_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test_6_month_md_7.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train_6_month_md_7[numeric_cols_train] = df_stock_data_train_6_month_md_7[numeric_cols_train].fillna(df_stock_data_train_6_month_md_7[numeric_cols_train].median())\n",
    "df_stock_data_test_6_month_md_7[numeric_cols_test] = df_stock_data_test_6_month_md_7[numeric_cols_test].fillna(df_stock_data_test_6_month_md_7[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train_6_month_md_7.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test_6_month_md_7.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train_6_month_md_7 = df_stock_data_train_6_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train_6_month_md_7 = df_stock_data_train_6_month_md_7['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test_6_month_md_7 = df_stock_data_test_6_month_md_7.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test_6_month_md_7 = df_stock_data_test_6_month_md_7['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train_6_month_md_7 shape: {X_train_6_month_md_7.shape}, y_train_6_month_md_7 shape: {y_train_6_month_md_7.shape}\")\n",
    "print(f\"X_test_6_month_md_7 shape: {X_test_6_month_md_7.shape}, y_test_6_month_md_7 shape: {y_test_6_month_md_7.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train_6_month_md_7.shape[0] == 0 or X_test_6_month_md_7.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_md_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_md_7.fit(X_train_6_month_md_7, y_train_6_month_md_7)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred_6_month_md_7 = model_baseline_6_month_md_7.predict(X_test_6_month_md_7)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test_6_month_md_7 = mean_squared_error(y_test_6_month_md_7, y_pred_6_month_md_7)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test_6_month_md_7}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b5029-bb9b-4f8d-8e6b-111edd9a4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming `y_pred_6_month_md_7` are your predictions for the test data and `y_test_6_month_md_7` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse_6_month_md_7 = mean_squared_error(y_test_6_month_md_7, y_pred_6_month_md_7)\n",
    "mae_6_month_md_7 = mean_absolute_error(y_test_6_month_md_7, y_pred_6_month_md_7)\n",
    "rmse_6_month_md_7 = np.sqrt(mse_6_month_md_7)  # Root Mean Squared Error\n",
    "r2_6_month_md_7 = r2_score(y_test_6_month_md_7, y_pred_6_month_md_7)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse_6_month_md_7}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae_6_month_md_7}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse_6_month_md_7}')\n",
    "print(f'R-squared on unseen data: {r2_6_month_md_7}')\n",
    "\n",
    "# Additional metrics\n",
    "medae_6_month_md_7 = median_absolute_error(y_test_6_month_md_7, y_pred_6_month_md_7)\n",
    "print(f'Median Absolute Error on unseen data: {medae_6_month_md_7}')\n",
    "\n",
    "dw_stat_6_month_md_7 = durbin_watson(y_test_6_month_md_7 - y_pred_6_month_md_7)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat_6_month_md_7}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape_6_month_md_7 = np.mean(np.abs((y_test_6_month_md_7 - y_pred_6_month_md_7) / y_test_6_month_md_7)) * 100\n",
    "print(f'MAPE on unseen data: {mape_6_month_md_7:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance_6_month_md_7 = dict(zip(X_train_6_month_md_7.columns, model_baseline_6_month_md_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features_6_month_md_7 = sorted(feature_importance_6_month_md_7.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features_6_month_md_7:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed1f03-4d19-4618-812d-a4795292834d",
   "metadata": {},
   "source": [
    "best model: learning_rate = 0.01 and max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521210ee-88e2-428b-baf3-8a851df6cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features_6_month_md_3[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features_6_month_md_3[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 3)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=18, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=18, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 6 Month Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c7256-6210-45ca-bdf0-2f820a860f73",
   "metadata": {},
   "source": [
    "Best model metrics\n",
    "\n",
    "most important metrics to visualize:\n",
    "\n",
    "1. Root Mean Squared Error\n",
    "2. R-squared\n",
    "3. MAPE\n",
    "4. Median Absolute Error\n",
    "\n",
    "1 week:\n",
    "\n",
    "Mean Squared Error on unseen data: 7604.147083529955\n",
    "Mean Absolute Error on unseen data: 10.044193260395863\n",
    "Root Mean Squared Error on unseen data: 87.20176078227982\n",
    "R-squared on unseen data: 0.9656152295864199\n",
    "Median Absolute Error on unseen data: 1.588897705078125\n",
    "Durbin-Watson Statistic on unseen data: 0.1674710246723756\n",
    "MAPE on unseen data: 3.96%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Fib_10_Low_Min: 63.23%\n",
    "Fib_30_High_Max: 14.79%\n",
    "Low: 4.90%\n",
    "5_day-Fib_38: 4.25%\n",
    "High: 3.05%\n",
    "5_day-Fib_61: 1.76%\n",
    "10_day_Fib_23: 1.33%\n",
    "Fib_30_Low_Min: 1.28%\n",
    "Volume: 1.07%\n",
    "Fib_5_Low_Min: 0.89%\n",
    "5_day-Fib_50: 0.66%\n",
    "EMA_5: 0.49%\n",
    "SMA_5: 0.44%\n",
    "30_day_Fib_38: 0.42%\n",
    "5_day-Fib_23: 0.24%\n",
    "Std_Dev: 0.20%\n",
    "Cumulative_Price_Volume: 0.20%\n",
    "EMA_12_MACD: 0.08%\n",
    "SMA_20: 0.08%\n",
    "30_day_Fib_50: 0.06%\n",
    "VWAP: 0.05%\n",
    "30_day_Fib_61: 0.05%\n",
    "SMA_5_lag_7: 0.05%\n",
    "ATR_True_Range: 0.05%\n",
    "SMA_50: 0.05%\n",
    "ATR_Prev_Close: 0.05%\n",
    "Cumulative_Volume: 0.05%\n",
    "Fib_5_High_Max: 0.04%\n",
    "EMA_20: 0.04%\n",
    "ATR: 0.03%\n",
    "Upper_Band: 0.03%\n",
    "Fib_10_High_Max: 0.01%\n",
    "Lower_Band: 0.01%\n",
    "Volume_lag_1: 0.01%\n",
    "EMA_26_MACD: 0.01%\n",
    "EMA_50: 0.01%\n",
    "10_day_Fib_61: 0.01%\n",
    "\n",
    "\n",
    "1 month:\n",
    "\n",
    "Mean Squared Error on unseen data: 10844.724459170979\n",
    "Mean Absolute Error on unseen data: 15.639166248218757\n",
    "Root Mean Squared Error on unseen data: 104.13800679469037\n",
    "R-squared on unseen data: 0.9513849924746814\n",
    "Median Absolute Error on unseen data: 3.326946258544922\n",
    "Durbin-Watson Statistic on unseen data: 0.0750817215794377\n",
    "MAPE on unseen data: 7.82%\n",
    "\n",
    "30_day_Fib_23: 52.41%\n",
    "Fib_30_High_Max: 32.44%\n",
    "Fib_5_Low_Min: 2.25%\n",
    "Low: 2.08%\n",
    "10_day_Fib_23: 1.96%\n",
    "High: 1.77%\n",
    "Volume: 1.43%\n",
    "5_day-Fib_50: 1.11%\n",
    "30_day_Fib_50: 0.68%\n",
    "10_day_Fib_38: 0.62%\n",
    "5_day-Fib_23: 0.48%\n",
    "Fib_10_High_Max: 0.28%\n",
    "Fib_10_Low_Min: 0.25%\n",
    "Std_Dev: 0.21%\n",
    "VWAP: 0.21%\n",
    "ATR_Prev_Close: 0.20%\n",
    "EMA_5: 0.19%\n",
    "Fib_5_High_Max: 0.19%\n",
    "Cumulative_Price_Volume: 0.17%\n",
    "Lower_Band: 0.16%\n",
    "EMA_26_MACD: 0.14%\n",
    "30_day_Fib_61: 0.11%\n",
    "SMA_5: 0.11%\n",
    "SMA_20_lag_1: 0.06%\n",
    "Upper_Band: 0.05%\n",
    "5_day-Fib_61: 0.04%\n",
    "ATR_True_Range: 0.04%\n",
    "Cumulative_Volume: 0.04%\n",
    "30_day_Fib_38: 0.03%\n",
    "10_day_Fib_61: 0.03%\n",
    "EMA_50: 0.02%\n",
    "Fib_30_Low_Min: 0.02%\n",
    "ATR: 0.02%\n",
    "Volume_lag_1: 0.02%\n",
    "Close_lag_1: 0.02%\n",
    "EMA_20_lag_20: 0.02%\n",
    "SMA_50: 0.01%\n",
    "EMA_12_MACD_lag_15: 0.01%\n",
    "EMA_12_MACD_lag_12: 0.01%\n",
    "\n",
    "\n",
    "3 months:\n",
    "\n",
    "Mean Squared Error on unseen data: 25855.13009374476\n",
    "Mean Absolute Error on unseen data: 25.29591435606357\n",
    "Root Mean Squared Error on unseen data: 160.79530494931984\n",
    "R-squared on unseen data: 0.8883720519645201\n",
    "Median Absolute Error on unseen data: 6.283714294433594\n",
    "Durbin-Watson Statistic on unseen data: 0.017109218495924613\n",
    "MAPE on unseen data: 15.52%\n",
    "\n",
    "Fib_30_Low_Min: 27.17%\n",
    "30_day_Fib_38: 14.80%\n",
    "5_day-Fib_61: 11.17%\n",
    "Upper_Band: 8.25%\n",
    "Fib_10_Low_Min: 3.10%\n",
    "10_day_Fib_50: 2.76%\n",
    "Close_lag_1: 2.34%\n",
    "Volume: 1.87%\n",
    "EMA_5: 1.82%\n",
    "Fib_5_Low_Min: 1.69%\n",
    "Fib_5_High_Max: 1.51%\n",
    "30_day_Fib_23: 1.50%\n",
    "High: 1.48%\n",
    "EMA_50_lag_25: 1.37%\n",
    "30_day_Fib_50: 1.29%\n",
    "ATR_High_Low: 1.20%\n",
    "EMA_26_MACD: 1.11%\n",
    "EMA_50: 1.08%\n",
    "SMA_50: 1.02%\n",
    "ATR: 1.00%\n",
    "10_day_Fib_61: 0.87%\n",
    "Low: 0.86%\n",
    "EMA_12_MACD: 0.82%\n",
    "Close_lag_90: 0.76%\n",
    "5_day-Fib_23: 0.76%\n",
    "EMA_20: 0.73%\n",
    "SMA_20_lag_90: 0.60%\n",
    "EMA_12_MACD_lag_90: 0.58%\n",
    "10_day_Fib_38: 0.57%\n",
    "VWAP: 0.55%\n",
    "30_day_Fib_61: 0.54%\n",
    "Volume_lag_50: 0.53%\n",
    "ATR_True_Range: 0.39%\n",
    "ATR_Prev_Close: 0.32%\n",
    "SMA_5_lag_90: 0.31%\n",
    "EMA_50_lag_30: 0.30%\n",
    "Lower_Band: 0.28%\n",
    "Fib_30_High_Max: 0.24%\n",
    "Std_Dev: 0.23%\n",
    "Volume_lag_3: 0.16%\n",
    "Volume_lag_25: 0.15%\n",
    "SMA_50_lag_60: 0.13%\n",
    "EMA_26_MACD_lag_40: 0.13%\n",
    "Cumulative_Price_Volume: 0.13%\n",
    "EMA_50_lag_20: 0.12%\n",
    "Volume_lag_75: 0.11%\n",
    "EMA_12_MACD_lag_20: 0.10%\n",
    "SMA_50_lag_10: 0.07%\n",
    "Volume_lag_60: 0.07%\n",
    "EMA_20_lag_15: 0.06%\n",
    "EMA_50_lag_50: 0.05%\n",
    "SMA_50_lag_3: 0.05%\n",
    "EMA_5_lag_40: 0.04%\n",
    "SMA_50_lag_5: 0.04%\n",
    "EMA_50_lag_75: 0.04%\n",
    "Cumulative_Volume: 0.04%\n",
    "SMA_5_lag_60: 0.04%\n",
    "Fib_10_High_Max: 0.04%\n",
    "EMA_50_lag_60: 0.03%\n",
    "SMA_50_lag_30: 0.03%\n",
    "SMA_5_lag_25: 0.03%\n",
    "Volume_lag_90: 0.03%\n",
    "SMA_20_lag_25: 0.03%\n",
    "SMA_50_lag_7: 0.03%\n",
    "ATR_High_Close: 0.02%\n",
    "EMA_50_lag_90: 0.02%\n",
    "SMA_20_lag_40: 0.02%\n",
    "Signal_Line: 0.02%\n",
    "Volume_lag_1: 0.02%\n",
    "SMA_50_lag_20: 0.02%\n",
    "SMA_20_lag_30: 0.02%\n",
    "EMA_26_MACD_lag_50: 0.02%\n",
    "SMA_5_lag_50: 0.02%\n",
    "SMA_50_lag_15: 0.02%\n",
    "Volume_lag_40: 0.02%\n",
    "SMA_50_lag_25: 0.01%\n",
    "EMA_50_lag_15: 0.01%\n",
    "EMA_12_MACD_lag_40: 0.01%\n",
    "MACD: 0.01%\n",
    "SMA_20_lag_20: 0.01%\n",
    "EMA_12_MACD_lag_15: 0.01%\n",
    "Volume_lag_5: 0.01%\n",
    "SMA_5: 0.01%\n",
    "SMA_5_lag_15: 0.01%\n",
    "EMA_26_MACD_lag_30: 0.01%\n",
    "Close_lag_20: 0.01%\n",
    "EMA_20_lag_50: 0.01%\n",
    "Close_lag_25: 0.01%\n",
    "Volume_lag_20: 0.01%\n",
    "Close_lag_40: 0.01%\n",
    "EMA_5_lag_50: 0.01%\n",
    "SMA_5_lag_40: 0.01%\n",
    "SMA_50_lag_90: 0.01%\n",
    "5_day-Fib_50: 0.01%\n",
    "SMA_50_lag_40: 0.01%\n",
    "EMA_5_lag_75: 0.01%\n",
    "SMA_20_lag_50: 0.01%\n",
    "Close_lag_10: 0.01%\n",
    "EMA_12_MACD_lag_5: 0.01%\n",
    "\n",
    "\n",
    "6 months:\n",
    "\n",
    "Mean Squared Error on unseen data: 31411.174267588787\n",
    "Mean Absolute Error on unseen data: 34.84146322021573\n",
    "Root Mean Squared Error on unseen data: 177.23197868214638\n",
    "R-squared on unseen data: 0.871485858716382\n",
    "Median Absolute Error on unseen data: 8.994926452636719\n",
    "Durbin-Watson Statistic on unseen data: 0.020845241129731375\n",
    "MAPE on unseen data: 21.51%\n",
    "\n",
    "Fib_10_High_Max: 17.93%\n",
    "Fib_30_High_Max: 13.03%\n",
    "Fib_5_Low_Min: 12.74%\n",
    "ATR_High_Low: 5.86%\n",
    "10_day_Fib_50: 4.75%\n",
    "EMA_50_lag_25: 4.42%\n",
    "EMA_5: 2.79%\n",
    "Middle_Band: 2.77%\n",
    "Volume: 2.74%\n",
    "EMA_12_MACD: 2.23%\n",
    "EMA_50: 2.11%\n",
    "Fib_5_High_Max: 2.10%\n",
    "5_day-Fib_61: 1.93%\n",
    "Low: 1.78%\n",
    "High: 1.60%\n",
    "30_day_Fib_61: 1.47%\n",
    "VWAP: 1.45%\n",
    "Fib_30_Low_Min: 1.18%\n",
    "EMA_26_MACD_lag_40: 1.09%\n",
    "SMA_50: 1.05%\n",
    "Fib_10_Low_Min: 1.02%\n",
    "SMA_50_lag_90: 1.01%\n",
    "SMA_5: 0.98%\n",
    "EMA_50_lag_30: 0.97%\n",
    "ATR: 0.92%\n",
    "SMA_50_lag_20: 0.76%\n",
    "EMA_20: 0.69%\n",
    "Close_lag_90: 0.65%\n",
    "5_day-Fib_23: 0.58%\n",
    "SMA_50_lag_25: 0.55%\n",
    "ATR_Prev_Close: 0.54%\n",
    "Lower_Band: 0.53%\n",
    "Std_Dev: 0.43%\n",
    "ATR_True_Range: 0.38%\n",
    "Volume_lag_75: 0.35%\n",
    "SMA_5_lag_60: 0.31%\n",
    "30_day_Fib_23: 0.29%\n",
    "EMA_12_MACD_lag_3: 0.21%\n",
    "10_day_Fib_38: 0.21%\n",
    "Volume_lag_60: 0.20%\n",
    "SMA_50_lag_75: 0.20%\n",
    "Volume_lag_1: 0.18%\n",
    "EMA_5_lag_75: 0.17%\n",
    "SMA_50_lag_3: 0.17%\n",
    "Close_lag_180: 0.17%\n",
    "Cumulative_Price_Volume: 0.16%\n",
    "Volume_lag_50: 0.15%\n",
    "5_day-Fib_50: 0.15%\n",
    "EMA_50_lag_1: 0.14%\n",
    "EMA_50_lag_90: 0.13%\n",
    "Volume_lag_30: 0.13%\n",
    "10_day_Fib_61: 0.12%\n",
    "EMA_50_lag_40: 0.09%\n",
    "EMA_20_lag_180: 0.08%\n",
    "SMA_50_lag_40: 0.08%\n",
    "Volume_lag_25: 0.08%\n",
    "EMA_50_lag_75: 0.06%\n",
    "Volume_lag_3: 0.05%\n",
    "Cumulative_Volume: 0.04%\n",
    "Volume_lag_5: 0.04%\n",
    "EMA_20_lag_90: 0.04%\n",
    "Signal_Line: 0.04%\n",
    "EMA_50_lag_180: 0.04%\n",
    "Volume_lag_40: 0.04%\n",
    "EMA_26_MACD: 0.03%\n",
    "EMA_26_MACD_lag_180: 0.03%\n",
    "Upper_Band: 0.03%\n",
    "EMA_12_MACD_lag_180: 0.03%\n",
    "EMA_20_lag_3: 0.03%\n",
    "EMA_5_lag_40: 0.03%\n",
    "EMA_12_MACD_lag_40: 0.03%\n",
    "SMA_50_lag_180: 0.02%\n",
    "SMA_20_lag_15: 0.02%\n",
    "Volume_lag_20: 0.02%\n",
    "SMA_20_lag_30: 0.02%\n",
    "EMA_12_MACD_lag_25: 0.02%\n",
    "EMA_5_lag_60: 0.02%\n",
    "SMA_5_lag_7: 0.02%\n",
    "Volume_lag_180: 0.02%\n",
    "EMA_5_lag_50: 0.02%\n",
    "EMA_50_lag_15: 0.02%\n",
    "SMA_50_lag_50: 0.02%\n",
    "SMA_20_lag_60: 0.02%\n",
    "SMA_20_lag_40: 0.02%\n",
    "EMA_26_MACD_lag_10: 0.01%\n",
    "Close_lag_20: 0.01%\n",
    "5_day-Fib_38: 0.01%\n",
    "Close_lag_50: 0.01%\n",
    "MACD: 0.01%\n",
    "EMA_20_lag_40: 0.01%\n",
    "Close_lag_10: 0.01%\n",
    "EMA_20_lag_50: 0.01%\n",
    "SMA_5_lag_30: 0.01%\n",
    "SMA_50_lag_10: 0.01%\n",
    "Volume_lag_10: 0.01%\n",
    "MACD_Histogram: 0.01%\n",
    "EMA_50_lag_20: 0.01%\n",
    "RSI: 0.01%\n",
    "10_day_Fib_23: 0.01%\n",
    "%D: 0.01%\n",
    "Close_lag_5: 0.01%\n",
    "Volume_lag_90: 0.01%\n",
    "Volume_lag_15: 0.01%\n",
    "SMA_20_lag_3: 0.01%\n",
    "EMA_20_lag_15: 0.01%\n",
    "SMA_5_lag_90: 0.01%\n",
    "EMA_12_MACD_lag_50: 0.01%\n",
    "SMA_20_lag_75: 0.01%\n",
    "EMA_20_lag_75: 0.01%\n",
    "EMA_50_lag_60: 0.01%\n",
    "SMA_50_lag_1: 0.01%\n",
    "SMA_50_lag_60: 0.01%\n",
    "EMA_26_MACD_lag_60: 0.01%\n",
    "Close_lag_1: 0.01%\n",
    "Volume_lag_7: 0.01%\n",
    "SMA_20_lag_90: 0.01%\n",
    "Close_lag_75: 0.01%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f93f01-f1de-4a9f-934c-268d7f07bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root mean squared error graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define time horizons and RMSE values\n",
    "time_horizons = [\"1 Week\", \"1 Month\", \"3 Months\", \"6 Months\"]\n",
    "rmse_values = [rmse_1_week_md_7, rmse_1_month_md_7, rmse_3_month_md_3, rmse_6_month_md_3]\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(time_horizons, rmse_values, marker='o', linestyle='-', color='b', linewidth=2, markersize=8)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Prediction Horizon\")\n",
    "plt.ylabel(\"Root Mean Squared Error (RMSE)\")\n",
    "plt.title(\"RMSE Across Different Prediction Timelines\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Display plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6095d-cb9d-43ac-9edf-515b8024d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r squared graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define time horizons and R-squared values\n",
    "time_horizons = [\"1 Week\", \"1 Month\", \"3 Months\", \"6 Months\"]\n",
    "r_squared_values = [r2_1_week_md_7, r2_1_month_md_7, r2_3_month_md_3, r2_6_month_md_3]\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(time_horizons, r_squared_values, marker='o', linestyle='-', color='g', linewidth=2, markersize=8)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Prediction Horizon\")\n",
    "plt.ylabel(\"R-squared\")\n",
    "plt.title(\"R-squared Across Different Prediction Timelines\")\n",
    "plt.ylim(0.85, 1.0)  # Setting limits for better visualization\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Display plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a011625-fa36-4063-8140-728f7ae6c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define time horizons and MAPE values\n",
    "time_horizons = [\"1 Week\", \"1 Month\", \"3 Months\", \"6 Months\"]\n",
    "mape_values = [mape_1_week_md_7, mape_1_month_md_7, mape_3_month_md_3, mape_6_month_md_3]\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(time_horizons, mape_values, marker='o', linestyle='-', color='r', linewidth=2, markersize=8)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Prediction Horizon\")\n",
    "plt.ylabel(\"MAPE (%)\")\n",
    "plt.title(\"Mean Absolute Percentage Error (MAPE) Over Different Prediction Horizons\")\n",
    "plt.ylim(0, 30)  # Adjusted for better visualization\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Display plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdb9b9-0380-4efd-8729-6179a4431923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median Absolute Error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define time horizons and Median Absolute Error values\n",
    "time_horizons = [\"1 Week\", \"1 Month\", \"3 Months\", \"6 Months\"]\n",
    "medae_values = [medae_1_week_md_7, medae_1_month_md_7, medae_3_month_md_3, medae_6_month_md_3]\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(time_horizons, medae_values, marker='o', linestyle='-', color='b', linewidth=2, markersize=8)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Prediction Horizon\")\n",
    "plt.ylabel(\"Median Absolute Error\")\n",
    "plt.title(\"Median Absolute Error Over Different Prediction Horizons\")\n",
    "plt.ylim(0, 15)  # Adjusted for better visualization\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Display plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892988f-54e9-4307-96a0-358e7c30bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Week Price Prediction: Available Testing Dates\n",
    "# Prints the first available date and the last available date \n",
    "# where 'Close' and 'Close_Target' values exist for a given stock ticker.\n",
    "\n",
    "def print_stock_date_range(df_test, symbol):\n",
    " \n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "    \n",
    "    # Filter DataFrame for the given symbol and ensure 'Close' and 'Close_Target' are not NaN\n",
    "    df_filtered = df_test[(df_test['Symbol'] == symbol) & \n",
    "                          (df_test['Close'].notna()) & \n",
    "                          (df_test['Close_Target'].notna())].reset_index(drop=True)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No available data for symbol: {symbol}\")\n",
    "        return\n",
    "\n",
    "    # Extract first and last available dates\n",
    "    first_date = df_filtered['Date'].min()\n",
    "    last_date = df_filtered['Date'].max()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Symbol: {symbol}\"),\n",
    "    print(f\"First Available Date: {first_date.strftime('%Y-%m-%d')}\"),\n",
    "    print(f\"Last Available Date: {last_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_date_range(df_stock_data_test_1_week_md_7, 'NVDA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0e8dc-a9d3-42fa-81bc-e9d71650621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Week Price Prediction\n",
    "# Prints the stock symbol, specified date, actual close price ('Close'), \n",
    "# and predicted price ('Close_Target'). If the date is not available, \n",
    "# it finds the next available future date. If the entered date is the last date, \n",
    "# it looks for the closest previous available date.\n",
    "import pandas as pd\n",
    "\n",
    "def print_stock_prediction_by_date(df_test, symbol, date):\n",
    "\n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "    \n",
    "    # Convert date input to datetime for accurate comparisons\n",
    "    date = pd.to_datetime(date)\n",
    "\n",
    "    # Filter DataFrame for the given stock symbol\n",
    "    df_filtered = df_test[df_test['Symbol'] == symbol].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data available for symbol: {symbol}\")\n",
    "        return\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_filtered['Date'] = pd.to_datetime(df_filtered['Date'])\n",
    "\n",
    "    # Ensure sorting by date for correct traversal\n",
    "    df_filtered = df_filtered.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    # Try to find the exact date\n",
    "    if date in df_filtered['Date'].values:\n",
    "        closest_date = date\n",
    "    else:\n",
    "        # Find the next available future date\n",
    "        future_dates = df_filtered[df_filtered['Date'] > date]\n",
    "        if not future_dates.empty:\n",
    "            closest_date = future_dates['Date'].iloc[0]  # Next available future date\n",
    "        else:\n",
    "            # If no future date exists, get the closest past date\n",
    "            past_dates = df_filtered[df_filtered['Date'] < date]\n",
    "            if not past_dates.empty:\n",
    "                closest_date = past_dates['Date'].iloc[-1]  # Last available past date\n",
    "            else:\n",
    "                print(f\"No available dates found for symbol: {symbol}\")\n",
    "                return\n",
    "\n",
    "    # Get row for the closest available date\n",
    "    row = df_filtered[df_filtered['Date'] == closest_date].iloc[0]\n",
    "    actual_close = row['Close']\n",
    "    predicted_price = row['Close_Target']\n",
    "\n",
    "    # Calculate the future date (120 trading days = 6 months)\n",
    "    future_date = closest_date + pd.DateOffset(days=7)\n",
    "    \n",
    "    # To ensure it represents 120 trading days, we might need to filter out weekends\n",
    "    # and filter out time\n",
    "    future_trading_date = future_date\n",
    "    trading_days = pd.date_range(closest_date, future_date, freq='B')  # 'B' for business days (weekdays)\n",
    "    future_trading_date = (trading_days[-1] if len(trading_days) > 0 else future_date).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    #Calculate percent error of actual price vs. predicted price\n",
    "    percent_error = ((abs(actual_close - predicted_price)) / abs(actual_close)) * 100\n",
    "    formatted_percent_error = f'{percent_error:.2f}%'\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Symbol: {symbol}\")\n",
    "    print(f\"Date: {closest_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Close Price: {actual_close:.2f}\")\n",
    "    print(f\"Predicted Price Date: {future_trading_date}\")\n",
    "    print(f\"Predicted Price: {predicted_price:.2f}\")\n",
    "    print(f\"Percent Error: {formatted_percent_error}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_prediction_by_date(df_stock_data_test_1_week_md_7, 'NVDA', '2024-03-21')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b863a-125b-44d7-853d-25aa5228770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_data_test_1_week_md_3[1990:1995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d06c7-63ec-4579-ab9c-2924e85c49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Month Price Prediction: Available Testing Dates\n",
    "# Prints the first available date and the last available date \n",
    "# where 'Close' and 'Close_Target' values exist for a given stock ticker.\n",
    "\n",
    "def print_stock_date_range(df_test, symbol):\n",
    " \n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "\n",
    "    # Ensure the 'Date' column is in datetime format\n",
    "    df_test['Date'] = pd.to_datetime(df_test['Date']) \n",
    "    \n",
    "    # Filter DataFrame for the given symbol and ensure 'Close' and 'Close_Target' are not NaN\n",
    "    df_filtered = df_test[(df_test['Symbol'] == symbol) & \n",
    "                          (df_test['Close'].notna()) & \n",
    "                          (df_test['Close_Target'].notna())].reset_index(drop=True)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No available data for symbol: {symbol}\")\n",
    "        return\n",
    "\n",
    "    # Extract first and last available dates\n",
    "    first_date = df_filtered['Date'].min()\n",
    "    last_date = df_filtered['Date'].max()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Symbol: {symbol}\"),\n",
    "    print(f\"First Available Date: {first_date.strftime('%Y-%m-%d')}\"),\n",
    "    print(f\"Last Available Date: {last_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_date_range(df_stock_data_test_1_month_md_7, 'AAPL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f155f-03b7-4034-8739-9edf87492b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Month Price Prediction\n",
    "# Prints the stock symbol, specified date, actual close price ('Close'), \n",
    "# and predicted price ('Close_Target'). If the date is not available, \n",
    "# it finds the next available future date. If the entered date is the last date, \n",
    "# it looks for the closest previous available date.\n",
    "import pandas as pd\n",
    "\n",
    "def print_stock_prediction_by_date(df_test, symbol, date):\n",
    "\n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "    \n",
    "    # Convert date input to datetime for accurate comparisons\n",
    "    date = pd.to_datetime(date)\n",
    "\n",
    "    # Filter DataFrame for the given stock symbol\n",
    "    df_filtered = df_test[df_test['Symbol'] == symbol].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data available for symbol: {symbol}\")\n",
    "        return\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_filtered['Date'] = pd.to_datetime(df_filtered['Date'])\n",
    "\n",
    "    # Ensure sorting by date for correct traversal\n",
    "    df_filtered = df_filtered.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    # Try to find the exact date\n",
    "    if date in df_filtered['Date'].values:\n",
    "        closest_date = date\n",
    "    else:\n",
    "        # Find the next available future date\n",
    "        future_dates = df_filtered[df_filtered['Date'] > date]\n",
    "        if not future_dates.empty:\n",
    "            closest_date = future_dates['Date'].iloc[0]  # Next available future date\n",
    "        else:\n",
    "            # If no future date exists, get the closest past date\n",
    "            past_dates = df_filtered[df_filtered['Date'] < date]\n",
    "            if not past_dates.empty:\n",
    "                closest_date = past_dates['Date'].iloc[-1]  # Last available past date\n",
    "            else:\n",
    "                print(f\"No available dates found for symbol: {symbol}\")\n",
    "                return\n",
    "\n",
    "    # Get row for the closest available date\n",
    "    row = df_filtered[df_filtered['Date'] == closest_date].iloc[0]\n",
    "    actual_close = row['Close']\n",
    "    predicted_price = row['Close_Target']\n",
    "\n",
    "    # Calculate the future date (120 trading days = 6 months)\n",
    "    future_date = closest_date + pd.DateOffset(days=120)\n",
    "    \n",
    "    # To ensure it represents 120 trading days, we might need to filter out weekends\n",
    "    # and filter out time\n",
    "    future_trading_date = future_date\n",
    "    trading_days = pd.date_range(closest_date, future_date, freq='B')  # 'B' for business days (weekdays)\n",
    "    future_trading_date = (trading_days[-1] if len(trading_days) > 0 else future_date).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    #Calculate percent error of actual price vs. predicted price\n",
    "    percent_error = ((abs(actual_close - predicted_price)) / abs(actual_close)) * 100\n",
    "    formatted_percent_error = f'{percent_error:.2f}%'\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Symbol: {symbol}\")\n",
    "    print(f\"Date: {closest_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Close Price: {actual_close:.2f}\")\n",
    "    print(f\"Predicted Price Date: {future_trading_date}\")\n",
    "    print(f\"Predicted Price: {predicted_price:.2f}\")\n",
    "    print(f\"Percent Error: {formatted_percent_error}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_prediction_by_date(df_stock_data_test_1_month_md_3, 'MSFT', '2024-02-09')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05a66d-1af3-49d4-9d0f-a6621195d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Month Price Prediction: Available Testing Dates\n",
    "# Prints the first available date and the last available date \n",
    "# where 'Close' and 'Close_Target' values exist for a given stock ticker.\n",
    "\n",
    "def print_stock_date_range(df_test, symbol):\n",
    " \n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Filter DataFrame for the given symbol and ensure 'Close' and 'Close_Target' are not NaN\n",
    "    df_filtered = df_test[(df_test['Symbol'] == symbol) & \n",
    "                          (df_test['Close'].notna()) & \n",
    "                          (df_test['Close_Target'].notna())].reset_index(drop=True)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No available data for symbol: {symbol}\")\n",
    "        return\n",
    "\n",
    "    # Extract first and last available dates\n",
    "    first_date = df_filtered['Date'].min()\n",
    "    last_date = df_filtered['Date'].max()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Symbol: {symbol}\"),\n",
    "    print(f\"First Available Date: {first_date.strftime('%Y-%m-%d')}\"),\n",
    "    print(f\"Last Available Date: {last_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_date_range(df_stock_data_test_3_month_md_3, 'AAPL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75b408-60d7-4d08-9f84-0a689e28a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Month Price Prediction\n",
    "# Prints the stock symbol, specified date, actual close price ('Close'), \n",
    "# and predicted price ('Close_Target'). If the date is not available, \n",
    "# it finds the next available future date. If the entered date is the last date, \n",
    "# it looks for the closest previous available date.\n",
    "import pandas as pd\n",
    "\n",
    "def print_stock_prediction_by_date(df_test, symbol, date):\n",
    "\n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "    \n",
    "    # Convert date input to datetime for accurate comparisons\n",
    "    date = pd.to_datetime(date)\n",
    "\n",
    "    # Filter DataFrame for the given stock symbol\n",
    "    df_filtered = df_test[df_test['Symbol'] == symbol].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data available for symbol: {symbol}\")\n",
    "        return\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_filtered['Date'] = pd.to_datetime(df_filtered['Date'])\n",
    "\n",
    "    # Ensure sorting by date for correct traversal\n",
    "    df_filtered = df_filtered.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    # Try to find the exact date\n",
    "    if date in df_filtered['Date'].values:\n",
    "        closest_date = date\n",
    "    else:\n",
    "        # Find the next available future date\n",
    "        future_dates = df_filtered[df_filtered['Date'] > date]\n",
    "        if not future_dates.empty:\n",
    "            closest_date = future_dates['Date'].iloc[0]  # Next available future date\n",
    "        else:\n",
    "            # If no future date exists, get the closest past date\n",
    "            past_dates = df_filtered[df_filtered['Date'] < date]\n",
    "            if not past_dates.empty:\n",
    "                closest_date = past_dates['Date'].iloc[-1]  # Last available past date\n",
    "            else:\n",
    "                print(f\"No available dates found for symbol: {symbol}\")\n",
    "                return\n",
    "\n",
    "    # Get row for the closest available date\n",
    "    row = df_filtered[df_filtered['Date'] == closest_date].iloc[0]\n",
    "    actual_close = row['Close']\n",
    "    predicted_price = row['Close_Target']\n",
    "\n",
    "    # Calculate the future date (120 trading days = 6 months)\n",
    "    future_date = closest_date + pd.DateOffset(days=120)\n",
    "    \n",
    "    # To ensure it represents 120 trading days, we might need to filter out weekends\n",
    "    # and filter out time\n",
    "    future_trading_date = future_date\n",
    "    trading_days = pd.date_range(closest_date, future_date, freq='B')  # 'B' for business days (weekdays)\n",
    "    future_trading_date = (trading_days[-1] if len(trading_days) > 0 else future_date).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    #Calculate percent error of actual price vs. predicted price\n",
    "    percent_error = ((abs(actual_close - predicted_price)) / abs(actual_close)) * 100\n",
    "    formatted_percent_error = f'{percent_error:.2f}%'\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Symbol: {symbol}\")\n",
    "    print(f\"Date: {closest_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Close Price: {actual_close:.2f}\")\n",
    "    print(f\"Predicted Price Date: {future_trading_date}\")\n",
    "    print(f\"Predicted Price: {predicted_price:.2f}\")\n",
    "    print(f\"Percent Error: {formatted_percent_error}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_prediction_by_date(df_stock_data_test_3_month_md_3, 'AAPL', '2024-02-09')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113decb-1636-4a34-822c-0753eea942f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Month Price Prediction: Available Testing Dates\n",
    "# Prints the first available date and the last available date \n",
    "# where 'Close' and 'Close_Target' values exist for a given stock ticker.\n",
    "\n",
    "def print_stock_date_range(df_test, symbol):\n",
    " \n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "\n",
    "    # Ensure the 'Date' column is in datetime format\n",
    "    df_test['Date'] = pd.to_datetime(df_test['Date']) \n",
    "    \n",
    "    # Filter DataFrame for the given symbol and ensure 'Close' and 'Close_Target' are not NaN\n",
    "    df_filtered = df_test[(df_test['Symbol'] == symbol) & \n",
    "                          (df_test['Close'].notna()) & \n",
    "                          (df_test['Close_Target'].notna())].reset_index(drop=True)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No available data for symbol: {symbol}\")\n",
    "        return\n",
    "\n",
    "    # Extract first and last available dates\n",
    "    first_date = df_filtered['Date'].min()\n",
    "    last_date = df_filtered['Date'].max()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Symbol: {symbol}\"),\n",
    "    print(f\"First Available Date: {first_date.strftime('%Y-%m-%d')}\"),\n",
    "    print(f\"Last Available Date: {last_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_date_range(df_stock_data_test_6_month_md_3, 'MSFT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81b6db-3b38-4d60-a6e4-185853e6732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Month Price Prediction\n",
    "# Prints the stock symbol, specified date, actual close price ('Close'), \n",
    "# and predicted price ('Close_Target'). If the date is not available, \n",
    "# it finds the next available future date. If the entered date is the last date, \n",
    "# it looks for the closest previous available date.\n",
    "import pandas as pd\n",
    "\n",
    "def print_stock_prediction_by_date(df_test, symbol, date):\n",
    "\n",
    "    required_columns = {'Symbol', 'Date', 'Close', 'Close_Target'}\n",
    "    \n",
    "    # Ensure required columns exist in df_test\n",
    "    if not required_columns.issubset(df_test.columns):\n",
    "        raise ValueError(f\"The test dataframe must contain the following columns: {required_columns}\")\n",
    "    \n",
    "    # Convert date input to datetime for accurate comparisons\n",
    "    date = pd.to_datetime(date)\n",
    "\n",
    "    # Filter DataFrame for the given stock symbol\n",
    "    df_filtered = df_test[df_test['Symbol'] == symbol].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data available for symbol: {symbol}\")\n",
    "        return\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_filtered['Date'] = pd.to_datetime(df_filtered['Date'])\n",
    "\n",
    "    # Ensure sorting by date for correct traversal\n",
    "    df_filtered = df_filtered.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    # Try to find the exact date\n",
    "    if date in df_filtered['Date'].values:\n",
    "        closest_date = date\n",
    "    else:\n",
    "        # Find the next available future date\n",
    "        future_dates = df_filtered[df_filtered['Date'] > date]\n",
    "        if not future_dates.empty:\n",
    "            closest_date = future_dates['Date'].iloc[0]  # Next available future date\n",
    "        else:\n",
    "            # If no future date exists, get the closest past date\n",
    "            past_dates = df_filtered[df_filtered['Date'] < date]\n",
    "            if not past_dates.empty:\n",
    "                closest_date = past_dates['Date'].iloc[-1]  # Last available past date\n",
    "            else:\n",
    "                print(f\"No available dates found for symbol: {symbol}\")\n",
    "                return\n",
    "\n",
    "    # Get row for the closest available date\n",
    "    row = df_filtered[df_filtered['Date'] == closest_date].iloc[0]\n",
    "    actual_close = row['Close']\n",
    "    predicted_price = row['Close_Target']\n",
    "\n",
    "    # Calculate the future date (120 trading days = 6 months)\n",
    "    future_date = closest_date + pd.DateOffset(days=120)\n",
    "    \n",
    "    # To ensure it represents 120 trading days, we might need to filter out weekends\n",
    "    # and filter out time\n",
    "    future_trading_date = future_date\n",
    "    trading_days = pd.date_range(closest_date, future_date, freq='B')  # 'B' for business days (weekdays)\n",
    "    future_trading_date = (trading_days[-1] if len(trading_days) > 0 else future_date).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    #Calculate percent error of actual price vs. predicted price\n",
    "    percent_error = ((abs(actual_close - predicted_price)) / abs(actual_close)) * 100\n",
    "    formatted_percent_error = f'{percent_error:.2f}%'\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Symbol: {symbol}\")\n",
    "    print(f\"Date: {closest_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Close Price: {actual_close:.2f}\")\n",
    "    print(f\"Predicted Price Date: {future_trading_date}\")\n",
    "    print(f\"Predicted Price: {predicted_price:.2f}\")\n",
    "    print(f\"Percent Error: {formatted_percent_error}\")\n",
    "\n",
    "# Example usage:\n",
    "print_stock_prediction_by_date(df_stock_data_test_6_month_md_3, 'AAPL', '2024-02-09')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40d771-c51b-437c-b9fe-772ebdd07c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned[1500:1505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f85c6-9f05-472e-b63d-98945d413315",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
