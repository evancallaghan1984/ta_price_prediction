{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce9d5af-0fda-41db-a2fc-22a5d7e0f586",
   "metadata": {},
   "source": [
    "# Technical Analysis Indicator Price Prediction\n",
    "The goal of this project is to analyze the predictive power of the top 10 most popular TA indicators and see how well they do to predict price over a 30 day period. I am going to find the value of the indicators on day 1 (30 trading days ago) and then find the daily closing price for 30 days later and measure how well the indicator predicted the price.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b635c3-9842-408e-9f61-0c982d9c49a2",
   "metadata": {},
   "source": [
    "first we'll find the top 500 stocks by market cap from nasdaq and pull them into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196e2975-7d47-4313-93a9-26bcc74988d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Symbol                                               Name Last Sale  \\\n",
      "0      A             Agilent Technologies Inc. Common Stock   $138.31   \n",
      "1     AA                    Alcoa Corporation Common Stock     $34.50   \n",
      "2   AACG   ATA Creativity Global American Depositary Shares   $0.5025   \n",
      "3   AACT  Ares Acquisition Corporation II Class A Ordina...    $10.80   \n",
      "4   AADI                  Aadi Bioscience Inc. Common Stock     $1.88   \n",
      "\n",
      "   Net Change % Change    Market Cap        Country  IPO Year    Volume  \\\n",
      "0      1.0000   0.728%  3.974029e+10  United States    1999.0    887040   \n",
      "1      1.9800   6.089%  8.912735e+09  United States    2016.0  10730428   \n",
      "2     -0.0275  -5.189%  1.608006e+07          China    2008.0     25043   \n",
      "3      0.0200   0.186%  0.000000e+00            NaN    2023.0     35074   \n",
      "4      0.0800   4.444%  4.627589e+07  United States       NaN     81942   \n",
      "\n",
      "        Sector                                          Industry  \n",
      "0  Industrials  Biotechnology: Laboratory Analytical Instruments  \n",
      "1  Industrials                                          Aluminum  \n",
      "2  Real Estate                           Other Consumer Services  \n",
      "3      Finance                                      Blank Checks  \n",
      "4  Health Care        Biotechnology: Pharmaceutical Preparations  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc. Common Stock</td>\n",
       "      <td>3.288959e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft Corporation Common Stock</td>\n",
       "      <td>3.206167e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>NVIDIA Corporation Common Stock</td>\n",
       "      <td>2.864613e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>Alphabet Inc. Class C Capital Stock</td>\n",
       "      <td>1.957167e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet Inc. Class A Common Stock</td>\n",
       "      <td>1.945719e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Symbol                                 Name    Market Cap\n",
       "15     AAPL              Apple Inc. Common Stock  3.288959e+12\n",
       "4208   MSFT   Microsoft Corporation Common Stock  3.206167e+12\n",
       "4559   NVDA      NVIDIA Corporation Common Stock  2.864613e+12\n",
       "2819   GOOG  Alphabet Inc. Class C Capital Stock  1.957167e+12\n",
       "2820  GOOGL   Alphabet Inc. Class A Common Stock  1.945719e+12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing pandas library for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = '/Users/evancallaghan/Downloads/nasdaq_screener_1726538993372.csv' \n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Inspect the DataFrame to understand its structure\n",
    "print(df.head())\n",
    "\n",
    "# Filter DataFrame to only show the columns 'Symbol', 'Name', and 'Market Cap'\n",
    "df = df[['Symbol', 'Name', 'Market Cap']]\n",
    "\n",
    "# Convert 'Market Cap' to numeric if it's not already\n",
    "# Remove commas, dollar signs, and replace these symbols with empty spaces\n",
    "df['Market Cap'] = df['Market Cap'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "\n",
    "# Sort the DataFrame by Market Cap in descending order\n",
    "df_sorted = df.sort_values(by='Market Cap', ascending=False).head(5000)                                                                        \n",
    "df_sorted.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434eced7-138d-4dbe-a4d3-c9c92f32fbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc. Common Stock</td>\n",
       "      <td>3.288959e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft Corporation Common Stock</td>\n",
       "      <td>3.206167e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>NVIDIA Corporation Common Stock</td>\n",
       "      <td>2.864613e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>Alphabet Inc. Class C Capital Stock</td>\n",
       "      <td>1.957167e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet Inc. Class A Common Stock</td>\n",
       "      <td>1.945719e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                                 Name    Market Cap\n",
       "1   AAPL              Apple Inc. Common Stock  3.288959e+12\n",
       "2   MSFT   Microsoft Corporation Common Stock  3.206167e+12\n",
       "3   NVDA      NVIDIA Corporation Common Stock  2.864613e+12\n",
       "4   GOOG  Alphabet Inc. Class C Capital Stock  1.957167e+12\n",
       "5  GOOGL   Alphabet Inc. Class A Common Stock  1.945719e+12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the index of the DataFrame and drop the old index\n",
    "df_sorted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update the index to start from 1 instead of 0\n",
    "df_sorted.index = df_sorted.index + 1\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674b213-6fd9-428b-aa96-56497343cb2f",
   "metadata": {},
   "source": [
    "remove all stocks except common stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867fe5b5-1215-423f-9acf-fbcbae90e8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc. Common Stock</td>\n",
       "      <td>3.288959e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft Corporation Common Stock</td>\n",
       "      <td>3.206167e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>NVIDIA Corporation Common Stock</td>\n",
       "      <td>2.864613e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet Inc. Class A Common Stock</td>\n",
       "      <td>1.945719e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc. Common Stock</td>\n",
       "      <td>1.940525e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                                Name    Market Cap\n",
       "1   AAPL             Apple Inc. Common Stock  3.288959e+12\n",
       "2   MSFT  Microsoft Corporation Common Stock  3.206167e+12\n",
       "3   NVDA     NVIDIA Corporation Common Stock  2.864613e+12\n",
       "5  GOOGL  Alphabet Inc. Class A Common Stock  1.945719e+12\n",
       "6   AMZN        Amazon.com Inc. Common Stock  1.940525e+12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure there are no leading or trailing whitespaces in the 'Name' column\n",
    "df_sorted['Name'] = df_sorted['Name'].str.strip()\n",
    "\n",
    "# List of terms to filter out\n",
    "terms_to_drop = [\"Capital Stock\", \"Depository Shares\", \"Global Notes\", \"ADS\", \n",
    "                 \"Registry Shares\", \"Depositary Shares\"\n",
    "]\n",
    "\n",
    "# Create a regex pattern to match any of the terms\n",
    "# //b ensures that the match occues only at the start or end of a word\n",
    "# pipe '|' ensures that if any of the terms in 'terms_to_drop' are seen, \n",
    "# there is a match\n",
    "pattern = '|'.join([f\"\\\\b{term}\\\\b\" for term in terms_to_drop])\n",
    "\n",
    "# Apply filtering based on the updated pattern\n",
    "df_filtered = df_sorted[~df_sorted['Name'].str.contains(pattern, case=False, \n",
    "                                                        na=False)\n",
    "]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee41bc92-091c-4d0d-9ad7-c143c72bd072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc. Common Stock</td>\n",
       "      <td>3.288959e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft Corporation Common Stock</td>\n",
       "      <td>3.206167e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>NVIDIA Corporation Common Stock</td>\n",
       "      <td>2.864613e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet Inc. Class A Common Stock</td>\n",
       "      <td>1.945719e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc. Common Stock</td>\n",
       "      <td>1.940525e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                                Name    Market Cap\n",
       "1   AAPL             Apple Inc. Common Stock  3.288959e+12\n",
       "2   MSFT  Microsoft Corporation Common Stock  3.206167e+12\n",
       "3   NVDA     NVIDIA Corporation Common Stock  2.864613e+12\n",
       "4  GOOGL  Alphabet Inc. Class A Common Stock  1.945719e+12\n",
       "5   AMZN        Amazon.com Inc. Common Stock  1.940525e+12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the index of the DataFrame and drop the old index\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update the index to start from 1 instead of 0\n",
    "df_filtered.index = df_filtered.index + 1\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f48fa-2e14-4f6e-ae73-f451fa6fd1be",
   "metadata": {},
   "source": [
    "below are the 10 technical indicators we are going to use for this project.\n",
    "1. Relative Strength Index (RSI)\n",
    "2. Moving Average Convergence Divergence (MACD)\n",
    "3. Stochastic Oscillator\n",
    "4. Simple Moving Average (SMA)\n",
    "5. Exponential Moving Average (EMA)\n",
    "6. Volume Weighted Average Price (VWAP)\n",
    "7. Bollinger Bands\n",
    "8. Average True Range (ATR)\n",
    "9. Fibonacci Retracement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49d16e-ecff-4092-bfe7-c4ec158a2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a CSV file I have provided so this code does not need to be run again\n",
    "# Computationally intensive\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to download stock data for a single stock\n",
    "def download_stock_data(ticker):\n",
    "    try:\n",
    "        data = yf.download(ticker, start=\"2022-02-10\", end=\"2025-02-10\", interval=\"1d\")[['Close', 'High', 'Low', 'Volume']]\n",
    "        if data.empty:\n",
    "            print(f\"Warning: No data found for {ticker} (possibly due to non-trading days like weekends or holidays)\")\n",
    "            return None  # Return None if the data is empty\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data for {ticker}: {e}\")\n",
    "        return None  # Return None if there is any error (e.g., stock not found)\n",
    "\n",
    "# List of tickers from your df_filtered dataframe\n",
    "tickers = df_filtered['Symbol'].tolist()\n",
    "\n",
    "# Batch size for processing tickers in chunks\n",
    "batch_size = 100\n",
    "\n",
    "# Create a function to download data for a batch of tickers in parallel\n",
    "def download_batch(batch_tickers):\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        results = list(executor.map(download_stock_data, batch_tickers))\n",
    "    # Remove None values from the results\n",
    "    return [result for result in results if result is not None]\n",
    "\n",
    "# Loop through the tickers in batches\n",
    "for i in range(0, len(tickers), batch_size):\n",
    "    batch_tickers = tickers[i:i + batch_size]\n",
    "    results = download_batch(batch_tickers)\n",
    "\n",
    "    if results:  # Check if results are not empty\n",
    "        # Combine all individual stock data into a single dataframe\n",
    "        df_batch = pd.concat(results, keys=batch_tickers)\n",
    "\n",
    "        # Save the data to CSV for the current batch\n",
    "        df_batch.to_csv(f'/content/drive/MyDrive/stock_data_yahoo_{i // batch_size}.csv')\n",
    "        print(f\"Downloaded batch {i // batch_size} and saved to CSV\")\n",
    "    else:\n",
    "        print(f\"Batch {i // batch_size} has no data. Skipping...\")\n",
    "        # Optionally, log the tickers that failed for this batch\n",
    "        print(f\"Failed tickers in batch {i // batch_size}: {batch_tickers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b1a6ef-b1f9-4587-ac6a-865665dc5d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/562544689.py:15: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Close</th>\n",
       "      <th>Close.1</th>\n",
       "      <th>High</th>\n",
       "      <th>High.1</th>\n",
       "      <th>Low</th>\n",
       "      <th>Low.1</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volume.1</th>\n",
       "      <th>...</th>\n",
       "      <th>Low.21</th>\n",
       "      <th>Volume.21</th>\n",
       "      <th>Close.22</th>\n",
       "      <th>High.22</th>\n",
       "      <th>Low.22</th>\n",
       "      <th>Volume.22</th>\n",
       "      <th>Close.23</th>\n",
       "      <th>High.23</th>\n",
       "      <th>Low.23</th>\n",
       "      <th>Volume.23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>POST</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>POST</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>POST</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>POST</td>\n",
       "      <td>...</td>\n",
       "      <td>WTS</td>\n",
       "      <td>WTS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.520000457763672</td>\n",
       "      <td>69.12957763671875</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>70.2225112915039</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>68.92015838623047</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>642524.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09000015258789</td>\n",
       "      <td>69.76439666748047</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>70.5235595703125</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>68.95942687988281</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>492169.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>71.02094268798828</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>71.27617645263672</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>69.64659881591797</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>672473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Symbol  Unnamed: 1               Close            Close.1  \\\n",
       "0  Ticker         NaN                CCCS               POST   \n",
       "1     NaN        Date                 NaN                NaN   \n",
       "2    AAPL  2022-02-10  10.520000457763672  69.12957763671875   \n",
       "3    AAPL  2022-02-11   10.09000015258789  69.76439666748047   \n",
       "4    AAPL  2022-02-14  10.220000267028809  71.02094268798828   \n",
       "\n",
       "                 High             High.1                 Low  \\\n",
       "0                CCCS               POST                CCCS   \n",
       "1                 NaN                NaN                 NaN   \n",
       "2  10.729999542236328   70.2225112915039  10.199999809265137   \n",
       "3  10.489999771118164   70.5235595703125  10.020000457763672   \n",
       "4  10.460000038146973  71.27617645263672   9.970000267028809   \n",
       "\n",
       "               Low.1     Volume  Volume.1  ... Low.21 Volume.21 Close.22  \\\n",
       "0               POST       CCCS      POST  ...    WTS       WTS      NaN   \n",
       "1                NaN        NaN       NaN  ...    NaN       NaN      NaN   \n",
       "2  68.92015838623047  1037700.0  642524.0  ...    NaN       NaN      NaN   \n",
       "3  68.95942687988281   480300.0  492169.0  ...    NaN       NaN      NaN   \n",
       "4  69.64659881591797   724400.0  672473.0  ...    NaN       NaN      NaN   \n",
       "\n",
       "  High.22 Low.22 Volume.22 Close.23 High.23 Low.23 Volume.23  \n",
       "0     NaN    NaN       NaN      NaN     NaN    NaN       NaN  \n",
       "1     NaN    NaN       NaN      NaN     NaN    NaN       NaN  \n",
       "2     NaN    NaN       NaN      NaN     NaN    NaN       NaN  \n",
       "3     NaN    NaN       NaN      NaN     NaN    NaN       NaN  \n",
       "4     NaN    NaN       NaN      NaN     NaN    NaN       NaN  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to hold DataFrames\n",
    "df_list = []\n",
    "\n",
    "# List of specific file indices\n",
    "file_indices = [0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 27, 28, 29, 30, 45, 44, 26, 18, 46]\n",
    "\n",
    "# Loop through the specific CSV file indices\n",
    "for i in file_indices:\n",
    "    # Construct the file path for each batch\n",
    "    csv_file_path = f'/Users/evancallaghan/flatiron_ds/phase_5/capstone_project/stock_data_yahoo_{i}.csv'\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Rename 'Price' column to 'Symbol'\n",
    "    df = df.rename(columns={'Price': 'Symbol'})\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list along the rows (axis=0)\n",
    "df_all = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "288bddb0-fd45-4de2-955c-9f94380c0d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct symbols: 1792\n",
      "List of distinct symbols (first 10): ['Ticker' nan 'AAPL' 'MSFT' 'NVDA' 'GOOGL' 'AMZN' 'META' 'BRK/A' 'BRK/B']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of distinct tickers in the 'Symbol' column\n",
    "distinct_symbols = df_all['Symbol'].unique()\n",
    "\n",
    "# Count the number of distinct symbols\n",
    "num_distinct_symbols = len(distinct_symbols)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of distinct symbols: {num_distinct_symbols}\")\n",
    "\n",
    "# Optionally, print the list of distinct symbols (first 10 for brevity)\n",
    "print(f\"List of distinct symbols (first 10): {distinct_symbols[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b194b0a-a652-4e2a-8777-013abaab820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# Get the list of unique symbols from df_all and df_filtered\n",
    "unique_symbols_all = df_all['Symbol'].unique()\n",
    "unique_symbols_filtered = df_filtered['Symbol'].unique()\n",
    "\n",
    "# Find the symbols that are in df_all but not in df_filtered, ensuring all items are strings\n",
    "symbols_not_in_filtered = [str(symbol) for symbol in unique_symbols_all if str(symbol) not in map(str, unique_symbols_filtered)]\n",
    "\n",
    "# Print the list of symbols that are not in df_filtered\n",
    "print('\\n'.join(symbols_not_in_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2554560c-188d-4c90-8ad4-18b9418994ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Close</th>\n",
       "      <th>Close.1</th>\n",
       "      <th>High</th>\n",
       "      <th>High.1</th>\n",
       "      <th>Low</th>\n",
       "      <th>Low.1</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volume.1</th>\n",
       "      <th>...</th>\n",
       "      <th>Low.21</th>\n",
       "      <th>Volume.21</th>\n",
       "      <th>Close.22</th>\n",
       "      <th>High.22</th>\n",
       "      <th>Low.22</th>\n",
       "      <th>Volume.22</th>\n",
       "      <th>Close.23</th>\n",
       "      <th>High.23</th>\n",
       "      <th>Low.23</th>\n",
       "      <th>Volume.23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>POST</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>POST</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>POST</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>POST</td>\n",
       "      <td>...</td>\n",
       "      <td>WTS</td>\n",
       "      <td>WTS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.520000457763672</td>\n",
       "      <td>69.12957763671875</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>70.2225112915039</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>68.92015838623047</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>642524.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09000015258789</td>\n",
       "      <td>69.76439666748047</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>70.5235595703125</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>68.95942687988281</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>492169.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>71.02094268798828</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>71.27617645263672</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>69.64659881591797</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>672473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.5600004196167</td>\n",
       "      <td>71.5575942993164</td>\n",
       "      <td>10.569999694824219</td>\n",
       "      <td>72.25785064697266</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>70.99476623535156</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>380319.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Symbol  Unnamed: 1               Close            Close.1  \\\n",
       "0  Ticker         NaN                CCCS               POST   \n",
       "2    AAPL  2022-02-10  10.520000457763672  69.12957763671875   \n",
       "3    AAPL  2022-02-11   10.09000015258789  69.76439666748047   \n",
       "4    AAPL  2022-02-14  10.220000267028809  71.02094268798828   \n",
       "5    AAPL  2022-02-15    10.5600004196167   71.5575942993164   \n",
       "\n",
       "                 High             High.1                 Low  \\\n",
       "0                CCCS               POST                CCCS   \n",
       "2  10.729999542236328   70.2225112915039  10.199999809265137   \n",
       "3  10.489999771118164   70.5235595703125  10.020000457763672   \n",
       "4  10.460000038146973  71.27617645263672   9.970000267028809   \n",
       "5  10.569999694824219  72.25785064697266  10.220000267028809   \n",
       "\n",
       "               Low.1     Volume  Volume.1  ... Low.21 Volume.21 Close.22  \\\n",
       "0               POST       CCCS      POST  ...    WTS       WTS      NaN   \n",
       "2  68.92015838623047  1037700.0  642524.0  ...    NaN       NaN      NaN   \n",
       "3  68.95942687988281   480300.0  492169.0  ...    NaN       NaN      NaN   \n",
       "4  69.64659881591797   724400.0  672473.0  ...    NaN       NaN      NaN   \n",
       "5  70.99476623535156   758700.0  380319.0  ...    NaN       NaN      NaN   \n",
       "\n",
       "  High.22 Low.22 Volume.22 Close.23 High.23 Low.23 Volume.23  \n",
       "0     NaN    NaN       NaN      NaN     NaN    NaN       NaN  \n",
       "2     NaN    NaN       NaN      NaN     NaN    NaN       NaN  \n",
       "3     NaN    NaN       NaN      NaN     NaN    NaN       NaN  \n",
       "4     NaN    NaN       NaN      NaN     NaN    NaN       NaN  \n",
       "5     NaN    NaN       NaN      NaN     NaN    NaN       NaN  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert 'nan' strings or any NaN-like values to actual np.nan\n",
    "df_all['Symbol'] = df_all['Symbol'].apply(lambda x: np.nan if (isinstance(x, str) and x.lower() == 'nan') or pd.isna(x) else x)\n",
    "\n",
    "# Drop rows where the 'Symbol' column contains NaN values\n",
    "df_all_cleaned = df_all.dropna(subset=['Symbol'])\n",
    "\n",
    "# Verify the result by checking the first few rows\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a3415e0-07d9-4af4-9f94-b379e0ba4a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Symbol  Unnamed: 1               Close            Close.1  \\\n",
      "0  Ticker         NaN                CCCS               POST   \n",
      "1     NaN        Date                 NaN                NaN   \n",
      "2    AAPL  2022-02-10  10.520000457763672  69.12957763671875   \n",
      "3    AAPL  2022-02-11   10.09000015258789  69.76439666748047   \n",
      "4    AAPL  2022-02-14  10.220000267028809  71.02094268798828   \n",
      "\n",
      "                 High             High.1                 Low  \\\n",
      "0                CCCS               POST                CCCS   \n",
      "1                 NaN                NaN                 NaN   \n",
      "2  10.729999542236328   70.2225112915039  10.199999809265137   \n",
      "3  10.489999771118164   70.5235595703125  10.020000457763672   \n",
      "4  10.460000038146973  71.27617645263672   9.970000267028809   \n",
      "\n",
      "               Low.1     Volume Close.2  ... Low.20 Close.21 High.21 Low.21  \\\n",
      "0               POST       CCCS    LOAR  ...    KBH      WTS     WTS    WTS   \n",
      "1                NaN        NaN     NaN  ...    NaN      NaN     NaN    NaN   \n",
      "2  68.92015838623047  1037700.0     NaN  ...    NaN      NaN     NaN    NaN   \n",
      "3  68.95942687988281   480300.0     NaN  ...    NaN      NaN     NaN    NaN   \n",
      "4  69.64659881591797   724400.0     NaN  ...    NaN      NaN     NaN    NaN   \n",
      "\n",
      "  Close.22 High.22 Low.22 Close.23 High.23 Low.23  \n",
      "0      NaN     NaN    NaN      NaN     NaN    NaN  \n",
      "1      NaN     NaN    NaN      NaN     NaN    NaN  \n",
      "2      NaN     NaN    NaN      NaN     NaN    NaN  \n",
      "3      NaN     NaN    NaN      NaN     NaN    NaN  \n",
      "4      NaN     NaN    NaN      NaN     NaN    NaN  \n",
      "\n",
      "[5 rows x 75 columns]\n"
     ]
    }
   ],
   "source": [
    "# This will ensure that if any main column (Close, High, Low, or Volume)\n",
    "# has missing values, they will be filled with the corresponding values\n",
    "# from the suffixed columns (e.g., Close.1, Close.2, etc.), and once done,\n",
    "# the suffixed columns will be removed from the DataFrame.\n",
    "\n",
    "import re\n",
    "\n",
    "# List of main columns\n",
    "main_columns = ['Close', 'High', 'Low', 'Volume']\n",
    "\n",
    "# Iterate over the main columns to check and replace NaN values with corresponding suffixed columns\n",
    "for col in main_columns:\n",
    "    # Look for columns with numeric suffixes like .1, .2, .3, ..., .23\n",
    "    suffix_columns = [col + '.' + str(i) for i in range(1, 24)]  # Create the list of possible suffixes\n",
    "\n",
    "    # For each suffix column, if it exists, fill NaN in the main column with its values\n",
    "    for suffix_col in suffix_columns:\n",
    "        if suffix_col in df_all.columns:\n",
    "            df_all[col] = df_all[col].fillna(df_all[suffix_col])\n",
    "\n",
    "    # After filling NaN values from the suffixed columns, drop the suffixed columns\n",
    "    df_all_cleaned = df_all.drop(columns=suffix_columns)\n",
    "\n",
    "# Verify the changes\n",
    "print(df_all_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a738eb73-9a1a-4b6f-b6f2-d2ced09509e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Symbol', 'Unnamed: 1', 'Close', 'Close.1', 'High', 'High.1', 'Low',\n",
       "       'Low.1', 'Volume', 'Close.2', 'High.2', 'Low.2', 'Close.3', 'High.3',\n",
       "       'Low.3', 'Close.4', 'High.4', 'Low.4', 'Close.5', 'High.5', 'Low.5',\n",
       "       'Close.6', 'High.6', 'Low.6', 'Close.7', 'High.7', 'Low.7', 'Close.8',\n",
       "       'High.8', 'Low.8', 'Close.9', 'High.9', 'Low.9', 'Close.10', 'High.10',\n",
       "       'Low.10', 'Close.11', 'High.11', 'Low.11', 'Close.12', 'High.12',\n",
       "       'Low.12', 'Close.13', 'High.13', 'Low.13', 'Close.14', 'High.14',\n",
       "       'Low.14', 'Close.15', 'High.15', 'Low.15', 'Close.16', 'High.16',\n",
       "       'Low.16', 'Close.17', 'High.17', 'Low.17', 'Close.18', 'High.18',\n",
       "       'Low.18', 'Close.19', 'High.19', 'Low.19', 'Close.20', 'High.20',\n",
       "       'Low.20', 'Close.21', 'High.21', 'Low.21', 'Close.22', 'High.22',\n",
       "       'Low.22', 'Close.23', 'High.23', 'Low.23'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e79cdc09-431e-4248-a2b8-09a0253a8114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>CCCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.520000457763672</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>1037700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09000015258789</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>480300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>724400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Symbol  Unnamed: 1               Close                High  \\\n",
       "0  Ticker         NaN                CCCS                CCCS   \n",
       "1     NaN        Date                 NaN                 NaN   \n",
       "2    AAPL  2022-02-10  10.520000457763672  10.729999542236328   \n",
       "3    AAPL  2022-02-11   10.09000015258789  10.489999771118164   \n",
       "4    AAPL  2022-02-14  10.220000267028809  10.460000038146973   \n",
       "\n",
       "                  Low     Volume  \n",
       "0                CCCS       CCCS  \n",
       "1                 NaN        NaN  \n",
       "2  10.199999809265137  1037700.0  \n",
       "3  10.020000457763672   480300.0  \n",
       "4   9.970000267028809   724400.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of columns you want to keep\n",
    "columns_to_keep = ['Symbol', 'Unnamed: 1', 'Close', 'High', 'Low', 'Volume']\n",
    "\n",
    "# Select only the columns you want to keep and drop the others\n",
    "df_all_cleaned = df_all_cleaned[columns_to_keep]\n",
    "\n",
    "# Verify the result by checking the first few rows\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "390ccbc0-c4f0-4146-a013-d8226d9ac189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>CCCS</td>\n",
       "      <td>CCCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.520000457763672</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>1037700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09000015258789</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>480300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>724400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Symbol        Date               Close                High  \\\n",
       "0  Ticker         NaN                CCCS                CCCS   \n",
       "1     NaN        Date                 NaN                 NaN   \n",
       "2    AAPL  2022-02-10  10.520000457763672  10.729999542236328   \n",
       "3    AAPL  2022-02-11   10.09000015258789  10.489999771118164   \n",
       "4    AAPL  2022-02-14  10.220000267028809  10.460000038146973   \n",
       "\n",
       "                  Low     Volume  \n",
       "0                CCCS       CCCS  \n",
       "1                 NaN        NaN  \n",
       "2  10.199999809265137  1037700.0  \n",
       "3  10.020000457763672   480300.0  \n",
       "4   9.970000267028809   724400.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the 'Unnamed: 1' column to 'Date'\n",
    "df_all_cleaned = df_all_cleaned.rename(columns={'Unnamed: 1': 'Date'})\n",
    "\n",
    "# Verify the change by checking the first few rows\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d95cfda-59af-413c-b71f-65d2b9a234dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.520000457763672</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>1037700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09000015258789</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>480300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>724400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.5600004196167</td>\n",
       "      <td>10.569999694824219</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>758700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.4399995803833</td>\n",
       "      <td>10.550000190734863</td>\n",
       "      <td>10.390000343322754</td>\n",
       "      <td>685200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date               Close                High  \\\n",
       "0   AAPL  2022-02-10  10.520000457763672  10.729999542236328   \n",
       "1   AAPL  2022-02-11   10.09000015258789  10.489999771118164   \n",
       "2   AAPL  2022-02-14  10.220000267028809  10.460000038146973   \n",
       "3   AAPL  2022-02-15    10.5600004196167  10.569999694824219   \n",
       "4   AAPL  2022-02-16    10.4399995803833  10.550000190734863   \n",
       "\n",
       "                  Low     Volume  \n",
       "0  10.199999809265137  1037700.0  \n",
       "1  10.020000457763672   480300.0  \n",
       "2   9.970000267028809   724400.0  \n",
       "3  10.220000267028809   758700.0  \n",
       "4  10.390000343322754   685200.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with index 0 and 1\n",
    "df_all_cleaned = df_all_cleaned.drop([0, 1])\n",
    "\n",
    "# Reset the index\n",
    "df_all_cleaned = df_all_cleaned.reset_index(drop=True)\n",
    "\n",
    "# Verify the changes by checking the first few rows\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bba23c49-644a-4f8d-988b-eb3f58001025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Symbol Close\n",
      "57472    NaN  Ticker  MASI\n",
      "57473   Date     NaN   NaN\n",
      "109293   NaN  Ticker  BILL\n",
      "109294  Date     NaN   NaN\n",
      "169643   NaN  Ticker   NVO\n"
     ]
    }
   ],
   "source": [
    "# Check for non-numeric values in the 'Close' column\n",
    "non_numeric_values = df_all_cleaned[~df_all_cleaned['Close'].apply(pd.to_numeric, errors='coerce').notna()]\n",
    "print(non_numeric_values[['Date', 'Symbol', 'Close']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ddf6bc5-13e8-46fa-94b9-3b3279d4fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned = df_all_cleaned[pd.to_numeric(df_all_cleaned['Close'], errors='coerce').notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2909fe0e-16fa-46c0-b6ca-39b48978bbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.520000457763672</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09000015258789</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.5600004196167</td>\n",
       "      <td>10.569999694824219</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.4399995803833</td>\n",
       "      <td>10.550000190734863</td>\n",
       "      <td>10.390000343322754</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-17</td>\n",
       "      <td>10.1899995803833</td>\n",
       "      <td>10.399999618530273</td>\n",
       "      <td>10.109999656677246</td>\n",
       "      <td>609500.0</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>10.336667</td>\n",
       "      <td>10.336667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-18</td>\n",
       "      <td>10.09000015258789</td>\n",
       "      <td>10.3100004196167</td>\n",
       "      <td>10.069999694824219</td>\n",
       "      <td>623100.0</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>10.301429</td>\n",
       "      <td>10.301429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-22</td>\n",
       "      <td>10.149999618530273</td>\n",
       "      <td>10.279999732971191</td>\n",
       "      <td>9.920000076293945</td>\n",
       "      <td>733000.0</td>\n",
       "      <td>10.286000</td>\n",
       "      <td>10.282500</td>\n",
       "      <td>10.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-23</td>\n",
       "      <td>9.930000305175781</td>\n",
       "      <td>10.329999923706055</td>\n",
       "      <td>9.930000305175781</td>\n",
       "      <td>625600.0</td>\n",
       "      <td>10.160000</td>\n",
       "      <td>10.243333</td>\n",
       "      <td>10.243333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-24</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>10.479999542236328</td>\n",
       "      <td>9.680000305175781</td>\n",
       "      <td>1043800.0</td>\n",
       "      <td>10.164000</td>\n",
       "      <td>10.265000</td>\n",
       "      <td>10.265000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date               Close                High  \\\n",
       "0   AAPL  2022-02-10  10.520000457763672  10.729999542236328   \n",
       "1   AAPL  2022-02-11   10.09000015258789  10.489999771118164   \n",
       "2   AAPL  2022-02-14  10.220000267028809  10.460000038146973   \n",
       "3   AAPL  2022-02-15    10.5600004196167  10.569999694824219   \n",
       "4   AAPL  2022-02-16    10.4399995803833  10.550000190734863   \n",
       "5   AAPL  2022-02-17    10.1899995803833  10.399999618530273   \n",
       "6   AAPL  2022-02-18   10.09000015258789    10.3100004196167   \n",
       "7   AAPL  2022-02-22  10.149999618530273  10.279999732971191   \n",
       "8   AAPL  2022-02-23   9.930000305175781  10.329999923706055   \n",
       "9   AAPL  2022-02-24  10.460000038146973  10.479999542236328   \n",
       "\n",
       "                  Low     Volume      SMA_5     SMA_20     SMA_50  \n",
       "0  10.199999809265137  1037700.0  10.520000  10.520000  10.520000  \n",
       "1  10.020000457763672   480300.0  10.305000  10.305000  10.305000  \n",
       "2   9.970000267028809   724400.0  10.276667  10.276667  10.276667  \n",
       "3  10.220000267028809   758700.0  10.347500  10.347500  10.347500  \n",
       "4  10.390000343322754   685200.0  10.366000  10.366000  10.366000  \n",
       "5  10.109999656677246   609500.0  10.300000  10.336667  10.336667  \n",
       "6  10.069999694824219   623100.0  10.300000  10.301429  10.301429  \n",
       "7   9.920000076293945   733000.0  10.286000  10.282500  10.282500  \n",
       "8   9.930000305175781   625600.0  10.160000  10.243333  10.243333  \n",
       "9   9.680000305175781  1043800.0  10.164000  10.265000  10.265000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Moving Average\n",
    "# 5 Day SMA, 20 Day SMA, and 50 Day SMA\n",
    "\n",
    "# Group by 'Symbol' and then apply rolling averages within each group\n",
    "df_all_cleaned['SMA_5'] = df_all_cleaned.groupby('Symbol')['Close'].rolling(window=5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "df_all_cleaned['SMA_20'] = df_all_cleaned.groupby('Symbol')['Close'].rolling(window=20, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "df_all_cleaned['SMA_50'] = df_all_cleaned.groupby('Symbol')['Close'].rolling(window=50, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Verify the results by checking the first few rows\n",
    "df_all_cleaned.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72e77e20-54e1-4ed0-9ff2-a9780e9b2d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>EMA_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.520000457763672</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09000015258789</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>10.479048</td>\n",
       "      <td>10.503138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>10.454377</td>\n",
       "      <td>10.492034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.5600004196167</td>\n",
       "      <td>10.569999694824219</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>10.464436</td>\n",
       "      <td>10.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.4399995803833</td>\n",
       "      <td>10.550000190734863</td>\n",
       "      <td>10.390000343322754</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>10.462109</td>\n",
       "      <td>10.492555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date               Close                High  \\\n",
       "0   AAPL  2022-02-10  10.520000457763672  10.729999542236328   \n",
       "1   AAPL  2022-02-11   10.09000015258789  10.489999771118164   \n",
       "2   AAPL  2022-02-14  10.220000267028809  10.460000038146973   \n",
       "3   AAPL  2022-02-15    10.5600004196167  10.569999694824219   \n",
       "4   AAPL  2022-02-16    10.4399995803833  10.550000190734863   \n",
       "\n",
       "                  Low     Volume      SMA_5     SMA_20     SMA_50      EMA_5  \\\n",
       "0  10.199999809265137  1037700.0  10.520000  10.520000  10.520000  10.520000   \n",
       "1  10.020000457763672   480300.0  10.305000  10.305000  10.305000  10.376667   \n",
       "2   9.970000267028809   724400.0  10.276667  10.276667  10.276667  10.324445   \n",
       "3  10.220000267028809   758700.0  10.347500  10.347500  10.347500  10.402963   \n",
       "4  10.390000343322754   685200.0  10.366000  10.366000  10.366000  10.415309   \n",
       "\n",
       "      EMA_20     EMA_50  \n",
       "0  10.520000  10.520000  \n",
       "1  10.479048  10.503138  \n",
       "2  10.454377  10.492034  \n",
       "3  10.464436  10.494700  \n",
       "4  10.462109  10.492555  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exponential Moving Average\n",
    "\n",
    "# Exponential Moving Average (EMA)\n",
    "# 5 Day EMA, 20 Day EMA, and 50 Day EMA\n",
    "\n",
    "# Group by 'Symbol' and then apply ewm (Exponential Moving Average) within each group\n",
    "df_all_cleaned['EMA_5'] = df_all_cleaned.groupby('Symbol')['Close'].ewm(span=5, adjust=False).mean().reset_index(level=0, drop=True)\n",
    "df_all_cleaned['EMA_20'] = df_all_cleaned.groupby('Symbol')['Close'].ewm(span=20, adjust=False).mean().reset_index(level=0, drop=True)\n",
    "df_all_cleaned['EMA_50'] = df_all_cleaned.groupby('Symbol')['Close'].ewm(span=50, adjust=False).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Verify the results by checking the first few rows\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c9e729b-10b2-428f-ba82-d118223ea52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned['Close'] = pd.to_numeric(df_all_cleaned['Close'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0301330-a6b3-4916-bdf1-4f7c376f34a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>EMA_50</th>\n",
       "      <th>RSI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>10.479048</td>\n",
       "      <td>10.503138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>10.454377</td>\n",
       "      <td>10.492034</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.569999694824219</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>10.464436</td>\n",
       "      <td>10.494700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.550000190734863</td>\n",
       "      <td>10.390000343322754</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>10.462109</td>\n",
       "      <td>10.492555</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close                High                 Low  \\\n",
       "0   AAPL  2022-02-10  10.52  10.729999542236328  10.199999809265137   \n",
       "1   AAPL  2022-02-11  10.09  10.489999771118164  10.020000457763672   \n",
       "2   AAPL  2022-02-14  10.22  10.460000038146973   9.970000267028809   \n",
       "3   AAPL  2022-02-15  10.56  10.569999694824219  10.220000267028809   \n",
       "4   AAPL  2022-02-16  10.44  10.550000190734863  10.390000343322754   \n",
       "\n",
       "      Volume      SMA_5     SMA_20     SMA_50      EMA_5     EMA_20  \\\n",
       "0  1037700.0  10.520000  10.520000  10.520000  10.520000  10.520000   \n",
       "1   480300.0  10.305000  10.305000  10.305000  10.376667  10.479048   \n",
       "2   724400.0  10.276667  10.276667  10.276667  10.324445  10.454377   \n",
       "3   758700.0  10.347500  10.347500  10.347500  10.402963  10.464436   \n",
       "4   685200.0  10.366000  10.366000  10.366000  10.415309  10.462109   \n",
       "\n",
       "      EMA_50  RSI  \n",
       "0  10.520000  NaN  \n",
       "1  10.503138  NaN  \n",
       "2  10.492034  NaN  \n",
       "3  10.494700  NaN  \n",
       "4  10.492555  NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RSI\n",
    "\n",
    "\n",
    "# Define a function to calculate RSI\n",
    "def calculate_rsi(df, window=14):\n",
    "    # Calculate price changes\n",
    "    delta = df['Close'].diff()\n",
    "\n",
    "    # Separate gains and losses\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "    # Calculate the rolling average of gains and losses\n",
    "    avg_gain = gain.rolling(window=window).mean()\n",
    "    avg_loss = loss.rolling(window=window).mean()\n",
    "\n",
    "    # Calculate Relative Strength (RS)\n",
    "    rs = avg_gain / avg_loss\n",
    "\n",
    "    # Calculate RSI\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi\n",
    "\n",
    "# Apply the function to the dataframe to calculate RSI\n",
    "df_all_cleaned['RSI'] = calculate_rsi(df_all_cleaned)\n",
    "\n",
    "# Verify the results by checking the first few rows\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44e9627f-e119-4714-93e1-a5c0c073f1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>EMA_50</th>\n",
       "      <th>RSI</th>\n",
       "      <th>EMA_12_MACD</th>\n",
       "      <th>EMA_26_MACD</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>10.479048</td>\n",
       "      <td>10.503138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.453847</td>\n",
       "      <td>10.488149</td>\n",
       "      <td>-0.034302</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>-0.027442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>10.454377</td>\n",
       "      <td>10.492034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.417870</td>\n",
       "      <td>10.468286</td>\n",
       "      <td>-0.050416</td>\n",
       "      <td>-0.015571</td>\n",
       "      <td>-0.034844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.569999694824219</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>10.464436</td>\n",
       "      <td>10.494700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.439736</td>\n",
       "      <td>10.475079</td>\n",
       "      <td>-0.035343</td>\n",
       "      <td>-0.019526</td>\n",
       "      <td>-0.015817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.550000190734863</td>\n",
       "      <td>10.390000343322754</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>10.462109</td>\n",
       "      <td>10.492555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.439777</td>\n",
       "      <td>10.472481</td>\n",
       "      <td>-0.032704</td>\n",
       "      <td>-0.022161</td>\n",
       "      <td>-0.010543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close                High                 Low  \\\n",
       "0   AAPL  2022-02-10  10.52  10.729999542236328  10.199999809265137   \n",
       "1   AAPL  2022-02-11  10.09  10.489999771118164  10.020000457763672   \n",
       "2   AAPL  2022-02-14  10.22  10.460000038146973   9.970000267028809   \n",
       "3   AAPL  2022-02-15  10.56  10.569999694824219  10.220000267028809   \n",
       "4   AAPL  2022-02-16  10.44  10.550000190734863  10.390000343322754   \n",
       "\n",
       "      Volume      SMA_5     SMA_20     SMA_50      EMA_5     EMA_20  \\\n",
       "0  1037700.0  10.520000  10.520000  10.520000  10.520000  10.520000   \n",
       "1   480300.0  10.305000  10.305000  10.305000  10.376667  10.479048   \n",
       "2   724400.0  10.276667  10.276667  10.276667  10.324445  10.454377   \n",
       "3   758700.0  10.347500  10.347500  10.347500  10.402963  10.464436   \n",
       "4   685200.0  10.366000  10.366000  10.366000  10.415309  10.462109   \n",
       "\n",
       "      EMA_50  RSI  EMA_12_MACD  EMA_26_MACD      MACD  Signal_Line  \\\n",
       "0  10.520000  NaN    10.520000    10.520000  0.000000     0.000000   \n",
       "1  10.503138  NaN    10.453847    10.488149 -0.034302    -0.006860   \n",
       "2  10.492034  NaN    10.417870    10.468286 -0.050416    -0.015571   \n",
       "3  10.494700  NaN    10.439736    10.475079 -0.035343    -0.019526   \n",
       "4  10.492555  NaN    10.439777    10.472481 -0.032704    -0.022161   \n",
       "\n",
       "   MACD_Histogram  \n",
       "0        0.000000  \n",
       "1       -0.027442  \n",
       "2       -0.034844  \n",
       "3       -0.015817  \n",
       "4       -0.010543  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MACD\n",
    "\n",
    "# Calculate the 12-day EMA (Fast EMA) and reset the index\n",
    "df_all_cleaned['EMA_12_MACD'] = df_all_cleaned.groupby('Symbol')['Close'].ewm(span=12, adjust=False).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate the 26-day EMA (Slow EMA) and reset the index\n",
    "df_all_cleaned['EMA_26_MACD'] = df_all_cleaned.groupby('Symbol')['Close'].ewm(span=26, adjust=False).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate the MACD (12-day EMA - 26-day EMA)\n",
    "df_all_cleaned['MACD'] = df_all_cleaned['EMA_12_MACD'] - df_all_cleaned['EMA_26_MACD']\n",
    "\n",
    "# Calculate the Signal Line (9-day EMA of the MACD) and reset the index\n",
    "df_all_cleaned['Signal_Line'] = df_all_cleaned.groupby('Symbol')['MACD'].ewm(span=9, adjust=False).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate the MACD Histogram (MACD - Signal Line)\n",
    "df_all_cleaned['MACD_Histogram'] = df_all_cleaned['MACD'] - df_all_cleaned['Signal_Line']\n",
    "\n",
    "# Verify the results by checking the first few rows\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bfe0cbc-984c-4585-b08d-ec460754adb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>EMA_50</th>\n",
       "      <th>RSI</th>\n",
       "      <th>EMA_12_MACD</th>\n",
       "      <th>EMA_26_MACD</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>10.479048</td>\n",
       "      <td>10.503138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.453847</td>\n",
       "      <td>10.488149</td>\n",
       "      <td>-0.034302</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>-0.027442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>10.454377</td>\n",
       "      <td>10.492034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.417870</td>\n",
       "      <td>10.468286</td>\n",
       "      <td>-0.050416</td>\n",
       "      <td>-0.015571</td>\n",
       "      <td>-0.034844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.569999694824219</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>10.464436</td>\n",
       "      <td>10.494700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.439736</td>\n",
       "      <td>10.475079</td>\n",
       "      <td>-0.035343</td>\n",
       "      <td>-0.019526</td>\n",
       "      <td>-0.015817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.550000190734863</td>\n",
       "      <td>10.390000343322754</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>10.462109</td>\n",
       "      <td>10.492555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.439777</td>\n",
       "      <td>10.472481</td>\n",
       "      <td>-0.032704</td>\n",
       "      <td>-0.022161</td>\n",
       "      <td>-0.010543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close                High                 Low  \\\n",
       "0   AAPL  2022-02-10  10.52  10.729999542236328  10.199999809265137   \n",
       "1   AAPL  2022-02-11  10.09  10.489999771118164  10.020000457763672   \n",
       "2   AAPL  2022-02-14  10.22  10.460000038146973   9.970000267028809   \n",
       "3   AAPL  2022-02-15  10.56  10.569999694824219  10.220000267028809   \n",
       "4   AAPL  2022-02-16  10.44  10.550000190734863  10.390000343322754   \n",
       "\n",
       "      Volume      SMA_5     SMA_20     SMA_50      EMA_5     EMA_20  \\\n",
       "0  1037700.0  10.520000  10.520000  10.520000  10.520000  10.520000   \n",
       "1   480300.0  10.305000  10.305000  10.305000  10.376667  10.479048   \n",
       "2   724400.0  10.276667  10.276667  10.276667  10.324445  10.454377   \n",
       "3   758700.0  10.347500  10.347500  10.347500  10.402963  10.464436   \n",
       "4   685200.0  10.366000  10.366000  10.366000  10.415309  10.462109   \n",
       "\n",
       "      EMA_50  RSI  EMA_12_MACD  EMA_26_MACD      MACD  Signal_Line  \\\n",
       "0  10.520000  NaN    10.520000    10.520000  0.000000     0.000000   \n",
       "1  10.503138  NaN    10.453847    10.488149 -0.034302    -0.006860   \n",
       "2  10.492034  NaN    10.417870    10.468286 -0.050416    -0.015571   \n",
       "3  10.494700  NaN    10.439736    10.475079 -0.035343    -0.019526   \n",
       "4  10.492555  NaN    10.439777    10.472481 -0.032704    -0.022161   \n",
       "\n",
       "   MACD_Histogram  %K  %D  \n",
       "0        0.000000 NaN NaN  \n",
       "1       -0.027442 NaN NaN  \n",
       "2       -0.034844 NaN NaN  \n",
       "3       -0.015817 NaN NaN  \n",
       "4       -0.010543 NaN NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stochastic oscillator\n",
    "# Calculate the Stochastic Oscillator (%K)\n",
    "df_all_cleaned['Stoch_Lowest_Low_14'] = df_all_cleaned.groupby('Symbol')['Low'].rolling(window=14).min().reset_index(level=0, drop=True)\n",
    "df_all_cleaned['Stoch_Highest_High_14'] = df_all_cleaned.groupby('Symbol')['High'].rolling(window=14).max().reset_index(level=0, drop=True)\n",
    "\n",
    "df_all_cleaned['%K'] = ((df_all_cleaned['Close'] - df_all_cleaned['Stoch_Lowest_Low_14']) / (df_all_cleaned['Stoch_Highest_High_14'] - df_all_cleaned['Stoch_Lowest_Low_14'])) * 100\n",
    "\n",
    "# Calculate the %D (3-day Simple Moving Average of %K)\n",
    "df_all_cleaned['%D'] = df_all_cleaned.groupby('Symbol')['%K'].rolling(window=3).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Drop intermediate columns if you don't need them\n",
    "df_all_cleaned.drop(columns=['Stoch_Lowest_Low_14', 'Stoch_Highest_High_14'], inplace=True)\n",
    "\n",
    "# Verify the results by checking the first few rows\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ef2efea-5d80-47cf-bca5-f389b95b7f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>EMA_12_MACD</th>\n",
       "      <th>EMA_26_MACD</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Signal_Line</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>Cumulative_Price_Volume</th>\n",
       "      <th>Cumulative_Volume</th>\n",
       "      <th>VWAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.091660e+07</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>...</td>\n",
       "      <td>10.453847</td>\n",
       "      <td>10.488149</td>\n",
       "      <td>-0.034302</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>-0.027442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.576283e+07</td>\n",
       "      <td>1518000.0</td>\n",
       "      <td>10.383947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>...</td>\n",
       "      <td>10.417870</td>\n",
       "      <td>10.468286</td>\n",
       "      <td>-0.050416</td>\n",
       "      <td>-0.015571</td>\n",
       "      <td>-0.034844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.316620e+07</td>\n",
       "      <td>2242400.0</td>\n",
       "      <td>10.330985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.569999694824219</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>...</td>\n",
       "      <td>10.439736</td>\n",
       "      <td>10.475079</td>\n",
       "      <td>-0.035343</td>\n",
       "      <td>-0.019526</td>\n",
       "      <td>-0.015817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.117807e+07</td>\n",
       "      <td>3001100.0</td>\n",
       "      <td>10.388881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.550000190734863</td>\n",
       "      <td>10.390000343322754</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>...</td>\n",
       "      <td>10.439777</td>\n",
       "      <td>10.472481</td>\n",
       "      <td>-0.032704</td>\n",
       "      <td>-0.022161</td>\n",
       "      <td>-0.010543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.833156e+07</td>\n",
       "      <td>3686300.0</td>\n",
       "      <td>10.398383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close                High                 Low  \\\n",
       "0   AAPL  2022-02-10  10.52  10.729999542236328  10.199999809265137   \n",
       "1   AAPL  2022-02-11  10.09  10.489999771118164  10.020000457763672   \n",
       "2   AAPL  2022-02-14  10.22  10.460000038146973   9.970000267028809   \n",
       "3   AAPL  2022-02-15  10.56  10.569999694824219  10.220000267028809   \n",
       "4   AAPL  2022-02-16  10.44  10.550000190734863  10.390000343322754   \n",
       "\n",
       "      Volume      SMA_5     SMA_20     SMA_50      EMA_5  ...  EMA_12_MACD  \\\n",
       "0  1037700.0  10.520000  10.520000  10.520000  10.520000  ...    10.520000   \n",
       "1   480300.0  10.305000  10.305000  10.305000  10.376667  ...    10.453847   \n",
       "2   724400.0  10.276667  10.276667  10.276667  10.324445  ...    10.417870   \n",
       "3   758700.0  10.347500  10.347500  10.347500  10.402963  ...    10.439736   \n",
       "4   685200.0  10.366000  10.366000  10.366000  10.415309  ...    10.439777   \n",
       "\n",
       "   EMA_26_MACD      MACD  Signal_Line  MACD_Histogram  %K  %D  \\\n",
       "0    10.520000  0.000000     0.000000        0.000000 NaN NaN   \n",
       "1    10.488149 -0.034302    -0.006860       -0.027442 NaN NaN   \n",
       "2    10.468286 -0.050416    -0.015571       -0.034844 NaN NaN   \n",
       "3    10.475079 -0.035343    -0.019526       -0.015817 NaN NaN   \n",
       "4    10.472481 -0.032704    -0.022161       -0.010543 NaN NaN   \n",
       "\n",
       "   Cumulative_Price_Volume  Cumulative_Volume       VWAP  \n",
       "0             1.091660e+07          1037700.0  10.520000  \n",
       "1             1.576283e+07          1518000.0  10.383947  \n",
       "2             2.316620e+07          2242400.0  10.330985  \n",
       "3             3.117807e+07          3001100.0  10.388881  \n",
       "4             3.833156e+07          3686300.0  10.398383  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VWAP\n",
    "\n",
    "# Calculate Volume Weighted Average Price (VWAP) per symbol\n",
    "def calculate_vwap(df):\n",
    "    # Ensure 'Close' and 'Volume' are numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "    df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')\n",
    "\n",
    "    # Group by 'Symbol' and calculate VWAP for each symbol\n",
    "    df['Cumulative_Price_Volume'] = df.groupby('Symbol').apply(\n",
    "        lambda x: (x['Close'] * x['Volume']).cumsum()).reset_index(level=0, drop=True)\n",
    "\n",
    "    df['Cumulative_Volume'] = df.groupby('Symbol').apply(\n",
    "        lambda x: x['Volume'].cumsum()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Calculate VWAP as the ratio of cumulative sums for each group (symbol)\n",
    "    df['VWAP'] = df['Cumulative_Price_Volume'] / df['Cumulative_Volume']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to calculate VWAP\n",
    "df_all_cleaned = calculate_vwap(df_all_cleaned)\n",
    "\n",
    "# Verify the results by checking the first few rows\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "865196bf-9b2c-4235-bc9e-c4b8285048cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>Cumulative_Price_Volume</th>\n",
       "      <th>Cumulative_Volume</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Std_Dev</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.729999542236328</td>\n",
       "      <td>10.199999809265137</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.091660e+07</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.489999771118164</td>\n",
       "      <td>10.020000457763672</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.576283e+07</td>\n",
       "      <td>1518000.0</td>\n",
       "      <td>10.383947</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>0.304056</td>\n",
       "      <td>10.913113</td>\n",
       "      <td>9.696888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.460000038146973</td>\n",
       "      <td>9.970000267028809</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.316620e+07</td>\n",
       "      <td>2242400.0</td>\n",
       "      <td>10.330985</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>0.220530</td>\n",
       "      <td>10.717727</td>\n",
       "      <td>9.835607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.569999694824219</td>\n",
       "      <td>10.220000267028809</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.117807e+07</td>\n",
       "      <td>3001100.0</td>\n",
       "      <td>10.388881</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>0.229111</td>\n",
       "      <td>10.805722</td>\n",
       "      <td>9.889279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.550000190734863</td>\n",
       "      <td>10.390000343322754</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.833156e+07</td>\n",
       "      <td>3686300.0</td>\n",
       "      <td>10.398383</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>0.202682</td>\n",
       "      <td>10.771364</td>\n",
       "      <td>9.960636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close                High                 Low  \\\n",
       "0   AAPL  2022-02-10  10.52  10.729999542236328  10.199999809265137   \n",
       "1   AAPL  2022-02-11  10.09  10.489999771118164  10.020000457763672   \n",
       "2   AAPL  2022-02-14  10.22  10.460000038146973   9.970000267028809   \n",
       "3   AAPL  2022-02-15  10.56  10.569999694824219  10.220000267028809   \n",
       "4   AAPL  2022-02-16  10.44  10.550000190734863  10.390000343322754   \n",
       "\n",
       "      Volume      SMA_5     SMA_20     SMA_50      EMA_5  ...  MACD_Histogram  \\\n",
       "0  1037700.0  10.520000  10.520000  10.520000  10.520000  ...        0.000000   \n",
       "1   480300.0  10.305000  10.305000  10.305000  10.376667  ...       -0.027442   \n",
       "2   724400.0  10.276667  10.276667  10.276667  10.324445  ...       -0.034844   \n",
       "3   758700.0  10.347500  10.347500  10.347500  10.402963  ...       -0.015817   \n",
       "4   685200.0  10.366000  10.366000  10.366000  10.415309  ...       -0.010543   \n",
       "\n",
       "   %K  %D  Cumulative_Price_Volume  Cumulative_Volume       VWAP  Middle_Band  \\\n",
       "0 NaN NaN             1.091660e+07          1037700.0  10.520000    10.520000   \n",
       "1 NaN NaN             1.576283e+07          1518000.0  10.383947    10.305000   \n",
       "2 NaN NaN             2.316620e+07          2242400.0  10.330985    10.276667   \n",
       "3 NaN NaN             3.117807e+07          3001100.0  10.388881    10.347500   \n",
       "4 NaN NaN             3.833156e+07          3686300.0  10.398383    10.366000   \n",
       "\n",
       "    Std_Dev  Upper_Band  Lower_Band  \n",
       "0       NaN         NaN         NaN  \n",
       "1  0.304056   10.913113    9.696888  \n",
       "2  0.220530   10.717727    9.835607  \n",
       "3  0.229111   10.805722    9.889279  \n",
       "4  0.202682   10.771364    9.960636  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Bollinger Bands per symbol\n",
    "def calculate_bollinger_bands(df, window=20):\n",
    "    # Ensure 'Close' is numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "\n",
    "    # Calculate the rolling mean (Middle Band) and rolling standard deviation\n",
    "    df['Middle_Band'] = df.groupby('Symbol')['Close'].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "    df['Std_Dev'] = df.groupby('Symbol')['Close'].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
    "\n",
    "    # Calculate the Upper and Lower Bands\n",
    "    df['Upper_Band'] = df['Middle_Band'] + (df['Std_Dev'] * 2)\n",
    "    df['Lower_Band'] = df['Middle_Band'] - (df['Std_Dev'] * 2)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to calculate Bollinger Bands\n",
    "df_all_cleaned = calculate_bollinger_bands(df_all_cleaned)\n",
    "\n",
    "# Verify the results by checking the first few rows\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "770bf089-8654-49a4-8623-61c4b5709900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>Middle_Band</th>\n",
       "      <th>Std_Dev</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>ATR_Prev_Close</th>\n",
       "      <th>ATR_High_Low</th>\n",
       "      <th>ATR_High_Close</th>\n",
       "      <th>ATR_Low_Close</th>\n",
       "      <th>ATR_True_Range</th>\n",
       "      <th>ATR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.02</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>...</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>0.304056</td>\n",
       "      <td>10.913113</td>\n",
       "      <td>9.696888</td>\n",
       "      <td>10.52</td>\n",
       "      <td>0.469999</td>\n",
       "      <td>0.030001</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.97</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>...</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>0.220530</td>\n",
       "      <td>10.717727</td>\n",
       "      <td>9.835607</td>\n",
       "      <td>10.09</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.506667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.57</td>\n",
       "      <td>10.22</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>...</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>0.229111</td>\n",
       "      <td>10.805722</td>\n",
       "      <td>9.889279</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.349999</td>\n",
       "      <td>0.349999</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.349999</td>\n",
       "      <td>0.467500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.39</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>...</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>0.202682</td>\n",
       "      <td>10.771364</td>\n",
       "      <td>9.960636</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.408000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close   High    Low     Volume      SMA_5     SMA_20  \\\n",
       "0   AAPL  2022-02-10  10.52  10.73  10.20  1037700.0  10.520000  10.520000   \n",
       "1   AAPL  2022-02-11  10.09  10.49  10.02   480300.0  10.305000  10.305000   \n",
       "2   AAPL  2022-02-14  10.22  10.46   9.97   724400.0  10.276667  10.276667   \n",
       "3   AAPL  2022-02-15  10.56  10.57  10.22   758700.0  10.347500  10.347500   \n",
       "4   AAPL  2022-02-16  10.44  10.55  10.39   685200.0  10.366000  10.366000   \n",
       "\n",
       "      SMA_50      EMA_5  ...  Middle_Band   Std_Dev  Upper_Band  Lower_Band  \\\n",
       "0  10.520000  10.520000  ...    10.520000       NaN         NaN         NaN   \n",
       "1  10.305000  10.376667  ...    10.305000  0.304056   10.913113    9.696888   \n",
       "2  10.276667  10.324445  ...    10.276667  0.220530   10.717727    9.835607   \n",
       "3  10.347500  10.402963  ...    10.347500  0.229111   10.805722    9.889279   \n",
       "4  10.366000  10.415309  ...    10.366000  0.202682   10.771364    9.960636   \n",
       "\n",
       "   ATR_Prev_Close  ATR_High_Low  ATR_High_Close  ATR_Low_Close  \\\n",
       "0             NaN      0.530000             NaN            NaN   \n",
       "1           10.52      0.469999        0.030001           0.50   \n",
       "2           10.09      0.490000        0.370000           0.12   \n",
       "3           10.22      0.349999        0.349999           0.00   \n",
       "4           10.56      0.160000        0.010000           0.17   \n",
       "\n",
       "   ATR_True_Range       ATR  \n",
       "0        0.530000  0.530000  \n",
       "1        0.500000  0.515000  \n",
       "2        0.490000  0.506667  \n",
       "3        0.349999  0.467500  \n",
       "4        0.170000  0.408000  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average True Range (ATR)\n",
    "\n",
    "# Function to calculate True Range (TR)\n",
    "def calculate_true_range(df):\n",
    "    # Convert relevant columns to numeric (if not already numeric)\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "    df['High'] = pd.to_numeric(df['High'], errors='coerce')\n",
    "    df['Low'] = pd.to_numeric(df['Low'], errors='coerce')\n",
    "\n",
    "    # Ensure previous close is calculated per stock symbol to prevent cross-stock contamination\n",
    "    df['ATR_Prev_Close'] = df.groupby('Symbol')['Close'].shift(1)\n",
    "\n",
    "    df['ATR_High_Low'] = df['High'] - df['Low']  # High - Low\n",
    "    df['ATR_High_Close'] = (df['High'] - df['ATR_Prev_Close']).abs()  # High - Prev Close\n",
    "    df['ATR_Low_Close'] = (df['Low'] - df['ATR_Prev_Close']).abs()  # Low - Prev Close\n",
    "\n",
    "    # True Range is the max of the three\n",
    "    df['ATR_True_Range'] = df[['ATR_High_Low', 'ATR_High_Close', 'ATR_Low_Close']].max(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to calculate Average True Range (ATR)\n",
    "def calculate_atr(df, window=14):\n",
    "    # Apply the True Range calculation\n",
    "    df = calculate_true_range(df)\n",
    "\n",
    "    # Compute ATR within each stock symbol\n",
    "    df['ATR'] = df.groupby('Symbol')['ATR_True_Range'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to calculate ATR\n",
    "df_all_cleaned = calculate_atr(df_all_cleaned)\n",
    "\n",
    "# Verify the results by checking the first few rows\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e15a4085-12e8-4e7d-97f1-93e45e193eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>10_day_Fib_50</th>\n",
       "      <th>10_day_Fib_61</th>\n",
       "      <th>10_day_Fib_100</th>\n",
       "      <th>Fib_5_High_Max</th>\n",
       "      <th>Fib_5_Low_Min</th>\n",
       "      <th>5_day-Fib_23</th>\n",
       "      <th>5_day-Fib_38</th>\n",
       "      <th>5_day-Fib_50</th>\n",
       "      <th>5_day-Fib_61</th>\n",
       "      <th>5_day-Fib_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.465</td>\n",
       "      <td>10.40246</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.60492</td>\n",
       "      <td>10.52754</td>\n",
       "      <td>10.465</td>\n",
       "      <td>10.40246</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.02</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>...</td>\n",
       "      <td>10.375</td>\n",
       "      <td>10.29122</td>\n",
       "      <td>10.02</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.02</td>\n",
       "      <td>10.56244</td>\n",
       "      <td>10.45878</td>\n",
       "      <td>10.375</td>\n",
       "      <td>10.29122</td>\n",
       "      <td>10.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.97</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>...</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.73</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.55064</td>\n",
       "      <td>10.43968</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.57</td>\n",
       "      <td>10.22</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>...</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.73</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.55064</td>\n",
       "      <td>10.43968</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.39</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>...</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.73</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.55064</td>\n",
       "      <td>10.43968</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close   High    Low     Volume      SMA_5     SMA_20  \\\n",
       "0   AAPL  2022-02-10  10.52  10.73  10.20  1037700.0  10.520000  10.520000   \n",
       "1   AAPL  2022-02-11  10.09  10.49  10.02   480300.0  10.305000  10.305000   \n",
       "2   AAPL  2022-02-14  10.22  10.46   9.97   724400.0  10.276667  10.276667   \n",
       "3   AAPL  2022-02-15  10.56  10.57  10.22   758700.0  10.347500  10.347500   \n",
       "4   AAPL  2022-02-16  10.44  10.55  10.39   685200.0  10.366000  10.366000   \n",
       "\n",
       "      SMA_50      EMA_5  ...  10_day_Fib_50  10_day_Fib_61  10_day_Fib_100  \\\n",
       "0  10.520000  10.520000  ...         10.465       10.40246           10.20   \n",
       "1  10.305000  10.376667  ...         10.375       10.29122           10.02   \n",
       "2  10.276667  10.324445  ...         10.350       10.26032            9.97   \n",
       "3  10.347500  10.402963  ...         10.350       10.26032            9.97   \n",
       "4  10.366000  10.415309  ...         10.350       10.26032            9.97   \n",
       "\n",
       "   Fib_5_High_Max  Fib_5_Low_Min  5_day-Fib_23  5_day-Fib_38  5_day-Fib_50  \\\n",
       "0           10.73          10.20      10.60492      10.52754        10.465   \n",
       "1           10.73          10.02      10.56244      10.45878        10.375   \n",
       "2           10.73           9.97      10.55064      10.43968        10.350   \n",
       "3           10.73           9.97      10.55064      10.43968        10.350   \n",
       "4           10.73           9.97      10.55064      10.43968        10.350   \n",
       "\n",
       "   5_day-Fib_61  5_day-Fib_100  \n",
       "0      10.40246          10.20  \n",
       "1      10.29122          10.02  \n",
       "2      10.26032           9.97  \n",
       "3      10.26032           9.97  \n",
       "4      10.26032           9.97  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fibonacci Retracement levels\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to calculate Fibonacci Retracement levels\n",
    "def calculate_fibonacci_retracement_30_day(df, window=30):\n",
    "    # Convert relevant columns to numeric\n",
    "    df['High'] = pd.to_numeric(df['High'], errors='coerce')\n",
    "    df['Low'] = pd.to_numeric(df['Low'], errors='coerce')\n",
    "\n",
    "    # Define Fibonacci levels\n",
    "    fib_levels = [0.236, 0.382, 0.500, 0.618, 1.000]\n",
    "\n",
    "    # Group by 'Symbol' and calculate Fibonacci levels for a given window\n",
    "    def fib_retracement(stock_df):\n",
    "        stock_df['Fib_30_High_Max'] = stock_df['High'].rolling(window=window, min_periods=1).max()\n",
    "        stock_df['Fib_30_Low_Min'] = stock_df['Low'].rolling(window=window, min_periods=1).min()\n",
    "\n",
    "        # Calculate Fibonacci retracement levels\n",
    "        for level in fib_levels:\n",
    "            stock_df[f'30_day_Fib_{int(level*100)}'] = stock_df['Fib_30_High_Max'] - (level * (stock_df['Fib_30_High_Max'] - stock_df['Fib_30_Low_Min']))\n",
    "\n",
    "        return stock_df\n",
    "\n",
    "    # Apply the function to each stock symbol\n",
    "    df = df.groupby('Symbol', group_keys=False).apply(fib_retracement)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to calculate Fibonacci Retracement levels\n",
    "def calculate_fibonacci_retracement_10_day(df, window=10):\n",
    "    # Convert relevant columns to numeric\n",
    "    df['High'] = pd.to_numeric(df['High'], errors='coerce')\n",
    "    df['Low'] = pd.to_numeric(df['Low'], errors='coerce')\n",
    "\n",
    "    # Define Fibonacci levels\n",
    "    fib_levels = [0.236, 0.382, 0.500, 0.618, 1.000]\n",
    "\n",
    "    # Group by 'Symbol' and calculate Fibonacci levels for a given window\n",
    "    def fib_retracement(stock_df):\n",
    "        stock_df['Fib_10_High_Max'] = stock_df['High'].rolling(window=window, min_periods=1).max()\n",
    "        stock_df['Fib_10_Low_Min'] = stock_df['Low'].rolling(window=window, min_periods=1).min()\n",
    "\n",
    "        # Calculate Fibonacci retracement levels\n",
    "        for level in fib_levels:\n",
    "            stock_df[f'10_day_Fib_{int(level*100)}'] = stock_df['Fib_10_High_Max'] - (level * (stock_df['Fib_10_High_Max'] - stock_df['Fib_10_Low_Min']))\n",
    "\n",
    "        return stock_df\n",
    "\n",
    "    # Apply the function to each stock symbol\n",
    "    df = df.groupby('Symbol', group_keys=False).apply(fib_retracement)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to calculate Fibonacci Retracement levels\n",
    "def calculate_fibonacci_retracement_5_day(df, window=5):\n",
    "    # Convert relevant columns to numeric\n",
    "    df['High'] = pd.to_numeric(df['High'], errors='coerce')\n",
    "    df['Low'] = pd.to_numeric(df['Low'], errors='coerce')\n",
    "\n",
    "    # Define Fibonacci levels\n",
    "    fib_levels = [0.236, 0.382, 0.500, 0.618, 1.000]\n",
    "\n",
    "    # Group by 'Symbol' and calculate Fibonacci levels for a given window\n",
    "    def fib_retracement(stock_df):\n",
    "        stock_df['Fib_5_High_Max'] = stock_df['High'].rolling(window=window, min_periods=1).max()\n",
    "        stock_df['Fib_5_Low_Min'] = stock_df['Low'].rolling(window=window, min_periods=1).min()\n",
    "\n",
    "        # Calculate Fibonacci retracement levels\n",
    "        for level in fib_levels:\n",
    "            stock_df[f'5_day-Fib_{int(level*100)}'] = stock_df['Fib_5_High_Max'] - (level * (stock_df['Fib_5_High_Max'] - stock_df['Fib_5_Low_Min']))\n",
    "\n",
    "        return stock_df\n",
    "\n",
    "    # Apply the function to each stock symbol\n",
    "    df = df.groupby('Symbol', group_keys=False).apply(fib_retracement)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply Fibonacci Retracement calculation to the dataframe\n",
    "df_all_cleaned = calculate_fibonacci_retracement_30_day(df_all_cleaned, window=30)\n",
    "df_all_cleaned = calculate_fibonacci_retracement_10_day(df_all_cleaned, window=10)\n",
    "df_all_cleaned = calculate_fibonacci_retracement_5_day(df_all_cleaned, window=5)\n",
    "\n",
    "# Display the first few rows to verify results\n",
    "df_all_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ba68b36-27c3-4050-8269-72b376f36ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as CSV file for easy access\n",
    "df_all_cleaned.to_csv('/Users/evancallaghan/flatiron_ds/phase_5/capstone_project/stock_ta_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1d4260f-fdba-49c3-931a-0f1ac87620fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>10_day_Fib_50</th>\n",
       "      <th>10_day_Fib_61</th>\n",
       "      <th>10_day_Fib_100</th>\n",
       "      <th>Fib_5_High_Max</th>\n",
       "      <th>Fib_5_Low_Min</th>\n",
       "      <th>5_day-Fib_23</th>\n",
       "      <th>5_day-Fib_38</th>\n",
       "      <th>5_day-Fib_50</th>\n",
       "      <th>5_day-Fib_61</th>\n",
       "      <th>5_day-Fib_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.465</td>\n",
       "      <td>10.40246</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.60492</td>\n",
       "      <td>10.52754</td>\n",
       "      <td>10.465</td>\n",
       "      <td>10.40246</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.02</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>...</td>\n",
       "      <td>10.375</td>\n",
       "      <td>10.29122</td>\n",
       "      <td>10.02</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.02</td>\n",
       "      <td>10.56244</td>\n",
       "      <td>10.45878</td>\n",
       "      <td>10.375</td>\n",
       "      <td>10.29122</td>\n",
       "      <td>10.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.97</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>...</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.73</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.55064</td>\n",
       "      <td>10.43968</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.57</td>\n",
       "      <td>10.22</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>...</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.73</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.55064</td>\n",
       "      <td>10.43968</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.39</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>...</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.73</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.55064</td>\n",
       "      <td>10.43968</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close   High    Low     Volume      SMA_5     SMA_20  \\\n",
       "0   AAPL  2022-02-10  10.52  10.73  10.20  1037700.0  10.520000  10.520000   \n",
       "1   AAPL  2022-02-11  10.09  10.49  10.02   480300.0  10.305000  10.305000   \n",
       "2   AAPL  2022-02-14  10.22  10.46   9.97   724400.0  10.276667  10.276667   \n",
       "3   AAPL  2022-02-15  10.56  10.57  10.22   758700.0  10.347500  10.347500   \n",
       "4   AAPL  2022-02-16  10.44  10.55  10.39   685200.0  10.366000  10.366000   \n",
       "\n",
       "      SMA_50      EMA_5  ...  10_day_Fib_50  10_day_Fib_61  10_day_Fib_100  \\\n",
       "0  10.520000  10.520000  ...         10.465       10.40246           10.20   \n",
       "1  10.305000  10.376667  ...         10.375       10.29122           10.02   \n",
       "2  10.276667  10.324445  ...         10.350       10.26032            9.97   \n",
       "3  10.347500  10.402963  ...         10.350       10.26032            9.97   \n",
       "4  10.366000  10.415309  ...         10.350       10.26032            9.97   \n",
       "\n",
       "   Fib_5_High_Max  Fib_5_Low_Min  5_day-Fib_23  5_day-Fib_38  5_day-Fib_50  \\\n",
       "0           10.73          10.20      10.60492      10.52754        10.465   \n",
       "1           10.73          10.02      10.56244      10.45878        10.375   \n",
       "2           10.73           9.97      10.55064      10.43968        10.350   \n",
       "3           10.73           9.97      10.55064      10.43968        10.350   \n",
       "4           10.73           9.97      10.55064      10.43968        10.350   \n",
       "\n",
       "   5_day-Fib_61  5_day-Fib_100  \n",
       "0      10.40246          10.20  \n",
       "1      10.29122          10.02  \n",
       "2      10.26032           9.97  \n",
       "3      10.26032           9.97  \n",
       "4      10.26032           9.97  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the file path to your CSV in Google Drive\n",
    "csv_file_path = '/Users/evancallaghan/flatiron_ds/phase_5/capstone_project/stock_ta_data.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df_all_cleaned = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "df_all_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7dcb6ac-cf4f-46aa-a706-cf433b219ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>10_day_Fib_50</th>\n",
       "      <th>10_day_Fib_61</th>\n",
       "      <th>10_day_Fib_100</th>\n",
       "      <th>Fib_5_High_Max</th>\n",
       "      <th>Fib_5_Low_Min</th>\n",
       "      <th>5_day-Fib_23</th>\n",
       "      <th>5_day-Fib_38</th>\n",
       "      <th>5_day-Fib_50</th>\n",
       "      <th>5_day-Fib_61</th>\n",
       "      <th>5_day-Fib_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.465</td>\n",
       "      <td>10.40246</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.60492</td>\n",
       "      <td>10.52754</td>\n",
       "      <td>10.465</td>\n",
       "      <td>10.40246</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.02</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>...</td>\n",
       "      <td>10.375</td>\n",
       "      <td>10.29122</td>\n",
       "      <td>10.02</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.02</td>\n",
       "      <td>10.56244</td>\n",
       "      <td>10.45878</td>\n",
       "      <td>10.375</td>\n",
       "      <td>10.29122</td>\n",
       "      <td>10.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.97</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>...</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.73</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.55064</td>\n",
       "      <td>10.43968</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.57</td>\n",
       "      <td>10.22</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>...</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.73</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.55064</td>\n",
       "      <td>10.43968</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.39</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>...</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.73</td>\n",
       "      <td>9.97</td>\n",
       "      <td>10.55064</td>\n",
       "      <td>10.43968</td>\n",
       "      <td>10.350</td>\n",
       "      <td>10.26032</td>\n",
       "      <td>9.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close   High    Low     Volume      SMA_5     SMA_20  \\\n",
       "0   AAPL  2022-02-10  10.52  10.73  10.20  1037700.0  10.520000  10.520000   \n",
       "1   AAPL  2022-02-11  10.09  10.49  10.02   480300.0  10.305000  10.305000   \n",
       "2   AAPL  2022-02-14  10.22  10.46   9.97   724400.0  10.276667  10.276667   \n",
       "3   AAPL  2022-02-15  10.56  10.57  10.22   758700.0  10.347500  10.347500   \n",
       "4   AAPL  2022-02-16  10.44  10.55  10.39   685200.0  10.366000  10.366000   \n",
       "\n",
       "      SMA_50      EMA_5  ...  10_day_Fib_50  10_day_Fib_61  10_day_Fib_100  \\\n",
       "0  10.520000  10.520000  ...         10.465       10.40246           10.20   \n",
       "1  10.305000  10.376667  ...         10.375       10.29122           10.02   \n",
       "2  10.276667  10.324445  ...         10.350       10.26032            9.97   \n",
       "3  10.347500  10.402963  ...         10.350       10.26032            9.97   \n",
       "4  10.366000  10.415309  ...         10.350       10.26032            9.97   \n",
       "\n",
       "   Fib_5_High_Max  Fib_5_Low_Min  5_day-Fib_23  5_day-Fib_38  5_day-Fib_50  \\\n",
       "0           10.73          10.20      10.60492      10.52754        10.465   \n",
       "1           10.73          10.02      10.56244      10.45878        10.375   \n",
       "2           10.73           9.97      10.55064      10.43968        10.350   \n",
       "3           10.73           9.97      10.55064      10.43968        10.350   \n",
       "4           10.73           9.97      10.55064      10.43968        10.350   \n",
       "\n",
       "   5_day-Fib_61  5_day-Fib_100  \n",
       "0      10.40246          10.20  \n",
       "1      10.29122          10.02  \n",
       "2      10.26032           9.97  \n",
       "3      10.26032           9.97  \n",
       "4      10.26032           9.97  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stocks_price_ta =df_all_cleaned.copy()\n",
    "df_stocks_price_ta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12c4d888-c1e4-4c95-bce9-d3c272787b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>SMA_5_lag_5</th>\n",
       "      <th>SMA_5_lag_7</th>\n",
       "      <th>EMA_5_lag_1</th>\n",
       "      <th>EMA_5_lag_3</th>\n",
       "      <th>EMA_5_lag_5</th>\n",
       "      <th>EMA_5_lag_7</th>\n",
       "      <th>Volume_lag_1</th>\n",
       "      <th>Volume_lag_3</th>\n",
       "      <th>Volume_lag_5</th>\n",
       "      <th>Volume_lag_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.02</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.97</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.57</td>\n",
       "      <td>10.22</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.39</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close   High    Low     Volume      SMA_5     SMA_20  \\\n",
       "0   AAPL  2022-02-10  10.52  10.73  10.20  1037700.0  10.520000  10.520000   \n",
       "1   AAPL  2022-02-11  10.09  10.49  10.02   480300.0  10.305000  10.305000   \n",
       "2   AAPL  2022-02-14  10.22  10.46   9.97   724400.0  10.276667  10.276667   \n",
       "3   AAPL  2022-02-15  10.56  10.57  10.22   758700.0  10.347500  10.347500   \n",
       "4   AAPL  2022-02-16  10.44  10.55  10.39   685200.0  10.366000  10.366000   \n",
       "\n",
       "      SMA_50      EMA_5  ...  SMA_5_lag_5  SMA_5_lag_7  EMA_5_lag_1  \\\n",
       "0  10.520000  10.520000  ...          NaN          NaN          NaN   \n",
       "1  10.305000  10.376667  ...          NaN          NaN    10.520000   \n",
       "2  10.276667  10.324445  ...          NaN          NaN    10.376667   \n",
       "3  10.347500  10.402963  ...          NaN          NaN    10.324445   \n",
       "4  10.366000  10.415309  ...          NaN          NaN    10.402963   \n",
       "\n",
       "   EMA_5_lag_3  EMA_5_lag_5  EMA_5_lag_7  Volume_lag_1  Volume_lag_3  \\\n",
       "0          NaN          NaN          NaN           NaN           NaN   \n",
       "1          NaN          NaN          NaN     1037700.0           NaN   \n",
       "2          NaN          NaN          NaN      480300.0           NaN   \n",
       "3    10.520000          NaN          NaN      724400.0     1037700.0   \n",
       "4    10.376667          NaN          NaN      758700.0      480300.0   \n",
       "\n",
       "   Volume_lag_5  Volume_lag_7  \n",
       "0           NaN           NaN  \n",
       "1           NaN           NaN  \n",
       "2           NaN           NaN  \n",
       "3           NaN           NaN  \n",
       "4           NaN           NaN  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe called 'df_stock_data_1_week' as a copy of 'df_stocks_price_ta'\n",
    "df_stock_data_1_week = df_stocks_price_ta.copy()\n",
    "\n",
    "# List of columns to create lags for (focusing on short-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_5', 'EMA_5', 'Volume']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 15, 20, 30, 60, 90, 180, 360] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_stock_data_1_week[f'{col}_lag_{lag}'] = df_stock_data_1_week[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_stock_data_1_week.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59f016a9-5162-41a7-8656-9a0021f2acd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "print(df_stock_data_1_week['Date'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0c6b3da-33c9-4c97-869a-a03dc2110ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "df_stock_data_1_week['Date'] = pd.to_datetime(df_stock_data_1_week['Date'], errors='coerce')\n",
    "print(df_stock_data_1_week['Date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "087e3735-0369-486e-8f4d-3394e79d96f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/4133690100.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/4133690100.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (867895, 71)\n",
      "Testing data shape: (430180, 71)\n",
      "X_train shape: (867895, 67), y_train shape: (867895,)\n",
      "X_test shape: (430180, 67), y_test shape: (430180,)\n",
      "Mean Squared Error on unseen data (post-February 17, 2024): 9222.592316604241\n"
     ]
    }
   ],
   "source": [
    "# i think this one would actually be the baseline, as i can separate the dates and test only\n",
    "# after feb 10 which is what i want to do\n",
    "# it also contains scaled data, which was better\n",
    "# baseline model\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to February 10, 2024 for training\n",
    "df_stock_data_train = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2024-02-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 17, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2024-02-17']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_week = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_week.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred = model_baseline_1_week.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a63f8dd-0c9a-4c3b-8b80-e54c451605d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 9222.592316604241\n",
      "Mean Absolute Error on unseen data: 11.174428157067275\n",
      "Root Mean Squared Error on unseen data: 96.03432884445145\n",
      "R-squared on unseen data: 0.9582968719645971\n",
      "Median Absolute Error on unseen data: 1.6202430725097656\n",
      "Durbin-Watson Statistic on unseen data: 0.12107588672303043\n",
      "MAPE on unseen data: 4.06%\n",
      "Fib_10_Low_Min: 57.64%\n",
      "Fib_30_High_Max: 16.90%\n",
      "Low: 5.14%\n",
      "High: 4.55%\n",
      "5_day-Fib_61: 3.14%\n",
      "Fib_30_Low_Min: 2.50%\n",
      "5_day-Fib_38: 1.71%\n",
      "EMA_5: 1.67%\n",
      "5_day-Fib_50: 1.28%\n",
      "Volume: 1.20%\n",
      "30_day_Fib_50: 0.78%\n",
      "SMA_5: 0.73%\n",
      "SMA_50: 0.49%\n",
      "EMA_50: 0.31%\n",
      "30_day_Fib_61: 0.28%\n",
      "Fib_5_Low_Min: 0.26%\n",
      "Cumulative_Price_Volume: 0.21%\n",
      "VWAP: 0.20%\n",
      "ATR_Prev_Close: 0.16%\n",
      "Std_Dev: 0.15%\n",
      "5_day-Fib_23: 0.10%\n",
      "10_day_Fib_23: 0.08%\n",
      "Fib_5_High_Max: 0.07%\n",
      "30_day_Fib_38: 0.06%\n",
      "ATR_True_Range: 0.06%\n",
      "Lower_Band: 0.06%\n",
      "EMA_26_MACD: 0.04%\n",
      "Cumulative_Volume: 0.04%\n",
      "ATR: 0.02%\n",
      "Fib_10_High_Max: 0.02%\n",
      "SMA_20: 0.02%\n",
      "30_day_Fib_23: 0.02%\n",
      "Upper_Band: 0.02%\n",
      "ATR_High_Low: 0.01%\n",
      "Volume_lag_1: 0.01%\n",
      "EMA_12_MACD: 0.01%\n",
      "Volume_lag_3: 0.01%\n",
      "10_day_Fib_61: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "Volume_lag_7: 0.01%\n",
      "MACD: 0.01%\n",
      "EMA_5_lag_1: 0.00%\n",
      "Signal_Line: 0.00%\n",
      "%K: 0.00%\n",
      "10_day_Fib_38: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "%D: 0.00%\n",
      "RSI: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "EMA_20: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_1_week.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "799369d9-7383-4605-accf-17a8facc976d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with more than 1% contribution:\n",
      "Fib_10_Low_Min: 57.64%\n",
      "Fib_30_High_Max: 16.90%\n",
      "Low: 5.14%\n",
      "High: 4.55%\n",
      "5_day-Fib_61: 3.14%\n",
      "Fib_30_Low_Min: 2.50%\n",
      "5_day-Fib_38: 1.71%\n",
      "EMA_5: 1.67%\n",
      "5_day-Fib_50: 1.28%\n",
      "Volume: 1.20%\n",
      "List of important features:\n",
      "['Fib_10_Low_Min', 'Fib_30_High_Max', 'Low', 'High', '5_day-Fib_61', 'Fib_30_Low_Min', '5_day-Fib_38', 'EMA_5', '5_day-Fib_50', 'Volume']\n"
     ]
    }
   ],
   "source": [
    "# we're going to use the scaled data, so the model above will be\n",
    "# our baseline\n",
    "# next we're going to use the same model and use a new dataframe\n",
    "# with features from the baseline model that contributed more than 1%\n",
    "# first we need to get a list of important feautres from our baseline\n",
    "# model and create a new dataframe containing only those features\n",
    "# Get feature importance as a dictionary\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_1_week.feature_importances_))\n",
    "\n",
    "# Filter features with importance greater than 1%\n",
    "important_features = {feature: importance for feature, importance in feature_importance.items() if importance > 0.01}\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_important_features = sorted(important_features.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the features names (keys) into a list\n",
    "important_feature_names = [feature for feature, importance in sorted_important_features]\n",
    "\n",
    "# Print the sorted important features (optional)\n",
    "print(\"Features with more than 1% contribution:\")\n",
    "for feature in sorted_important_features:\n",
    "    print(f\"{feature[0]}: {feature[1] * 100:.2f}%\")\n",
    "\n",
    "# The list of important features that you can use to create a new dataframe\n",
    "print(\"List of important features:\")\n",
    "print(important_feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "feea64e0-0f6c-4e1a-b89e-f8843b7a14ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Fib_10_Low_Min</th>\n",
       "      <th>Fib_30_High_Max</th>\n",
       "      <th>Low</th>\n",
       "      <th>High</th>\n",
       "      <th>5_day-Fib_61</th>\n",
       "      <th>Fib_30_Low_Min</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>5_day-Fib_38</th>\n",
       "      <th>Volume</th>\n",
       "      <th>5_day-Fib_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171141</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>50.058052</td>\n",
       "      <td>49.918547</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>49.918547</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>50.398164</td>\n",
       "      <td>49.918547</td>\n",
       "      <td>50.058052</td>\n",
       "      <td>50.694471</td>\n",
       "      <td>1889800.0</td>\n",
       "      <td>50.546317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171142</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>47.580647</td>\n",
       "      <td>47.234290</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>47.234290</td>\n",
       "      <td>49.028604</td>\n",
       "      <td>48.739293</td>\n",
       "      <td>47.234290</td>\n",
       "      <td>49.232250</td>\n",
       "      <td>49.669085</td>\n",
       "      <td>3747400.0</td>\n",
       "      <td>49.204189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171143</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>47.210232</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>47.460380</td>\n",
       "      <td>48.468756</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>48.558244</td>\n",
       "      <td>49.501860</td>\n",
       "      <td>3157800.0</td>\n",
       "      <td>48.985308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171144</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>48.759216</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>48.427291</td>\n",
       "      <td>49.153677</td>\n",
       "      <td>48.468756</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>48.625235</td>\n",
       "      <td>49.501860</td>\n",
       "      <td>1900000.0</td>\n",
       "      <td>48.985308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171145</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>49.629917</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>48.855427</td>\n",
       "      <td>49.716506</td>\n",
       "      <td>48.468756</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>48.960129</td>\n",
       "      <td>49.501860</td>\n",
       "      <td>2010600.0</td>\n",
       "      <td>48.985308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Symbol       Date      Close  Fib_10_Low_Min  Fib_30_High_Max  \\\n",
       "171141      A 2022-02-10  50.058052       49.918547        51.174088   \n",
       "171142      A 2022-02-11  47.580647       47.234290        51.174088   \n",
       "171143      A 2022-02-14  47.210232       46.796529        51.174088   \n",
       "171144      A 2022-02-15  48.759216       46.796529        51.174088   \n",
       "171145      A 2022-02-16  49.629917       46.796529        51.174088   \n",
       "\n",
       "              Low       High  5_day-Fib_61  Fib_30_Low_Min      EMA_5  \\\n",
       "171141  49.918547  51.174088     50.398164       49.918547  50.058052   \n",
       "171142  47.234290  49.028604     48.739293       47.234290  49.232250   \n",
       "171143  46.796529  47.460380     48.468756       46.796529  48.558244   \n",
       "171144  48.427291  49.153677     48.468756       46.796529  48.625235   \n",
       "171145  48.855427  49.716506     48.468756       46.796529  48.960129   \n",
       "\n",
       "        5_day-Fib_38     Volume  5_day-Fib_50  \n",
       "171141     50.694471  1889800.0     50.546317  \n",
       "171142     49.669085  3747400.0     49.204189  \n",
       "171143     49.501860  3157800.0     48.985308  \n",
       "171144     49.501860  1900000.0     48.985308  \n",
       "171145     49.501860  2010600.0     48.985308  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = ['Symbol', 'Date', 'Close', 'Fib_10_Low_Min', 'Fib_30_High_Max',\n",
    "                      'Low', 'High', '5_day-Fib_61', 'Fib_30_Low_Min', 'EMA_5',\n",
    "                      '5_day-Fib_38', 'Volume', '5_day-Fib_50']\n",
    "df_important_feat_1_week = df_stock_data_1_week[important_features]\n",
    "df_important_feat_1_week.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e872d7c2-824b-4f00-b994-282282e59b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/115773503.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/115773503.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (867895, 14)\n",
      "Testing data shape: (430180, 14)\n",
      "X_train shape: (867895, 10), y_train shape: (867895,)\n",
      "X_test shape: (430180, 10), y_test shape: (430180,)\n",
      "Mean Squared Error on unseen data (post-February 17, 2024): 29133.41088889282\n"
     ]
    }
   ],
   "source": [
    "# we're going to use the scaled data, so the model above will be\n",
    "# our baseline\n",
    "# next we're going to use the same model and use a new dataframe\n",
    "# with features from the baseline model that contributed more than 1%\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_important_feat_1_week = df_important_feat_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to February 10, 2024 for training\n",
    "df_stock_data_train = df_important_feat_1_week[df_important_feat_1_week['Date'] <= '2024-02-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 17, 2024 for testing\n",
    "df_stock_data_test = df_important_feat_1_week[df_important_feat_1_week['Date'] > '2024-02-17']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_update1_1_week = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_update1_1_week.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred = model_update1_1_week.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d0add6b-88df-4251-bc1b-e95e9785e145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 29133.41088889282\n",
      "Mean Absolute Error on unseen data: 16.858962051733705\n",
      "Root Mean Squared Error on unseen data: 170.68512204903163\n",
      "R-squared on unseen data: 0.8682632471761647\n",
      "Median Absolute Error on unseen data: 1.6140975952148438\n",
      "Durbin-Watson Statistic on unseen data: 0.04397998041253925\n",
      "MAPE on unseen data: 4.13%\n",
      "Fib_10_Low_Min: 43.09%\n",
      "Low: 22.04%\n",
      "High: 16.96%\n",
      "Fib_30_High_Max: 7.94%\n",
      "Fib_30_Low_Min: 3.05%\n",
      "Volume: 1.75%\n",
      "EMA_5: 1.63%\n",
      "5_day-Fib_61: 1.40%\n",
      "5_day-Fib_38: 1.39%\n",
      "5_day-Fib_50: 0.76%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_update1_1_week.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6a8deae-7fa7-4ac8-8ac8-600bbcaaa2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2778787306.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2778787306.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (867895, 71)\n",
      "Testing data shape: (430180, 71)\n",
      "X_train shape: (867895, 67), y_train shape: (867895,)\n",
      "X_test shape: (430180, 67), y_test shape: (430180,)\n",
      "Mean Squared Error on unseen data (post-February 17, 2024): 8827.132736004045\n"
     ]
    }
   ],
   "source": [
    "# removing important features led to the degredation of all metrics\n",
    "# we're going to use all metrics again and try adjusting a few of the hyper parameters\n",
    "#\n",
    "# i think this one would actually be the baseline, as i can separate the dates and test only\n",
    "# after feb 10 which is what i want to do\n",
    "# it also contains scaled data, which was better\n",
    "# learning_rate = 0.01\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to February 10, 2024 for training\n",
    "df_stock_data_train = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2024-02-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 17, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2024-02-17']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_week_LR_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_week_LR_01.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred = model_baseline_1_week_LR_01.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "318f7d07-9367-43ff-ab48-b4f44670b32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 8827.132736004045\n",
      "Mean Absolute Error on unseen data: 10.84996547004774\n",
      "Root Mean Squared Error on unseen data: 93.95282186291183\n",
      "R-squared on unseen data: 0.960085078680935\n",
      "Median Absolute Error on unseen data: 1.5982322692871094\n",
      "Durbin-Watson Statistic on unseen data: 0.12782060179913898\n",
      "MAPE on unseen data: 4.30%\n",
      "Fib_10_Low_Min: 57.45%\n",
      "Fib_30_High_Max: 18.53%\n",
      "Low: 3.41%\n",
      "High: 2.98%\n",
      "Fib_30_Low_Min: 2.81%\n",
      "5_day-Fib_61: 2.06%\n",
      "Volume: 1.87%\n",
      "5_day-Fib_38: 1.72%\n",
      "5_day-Fib_50: 1.54%\n",
      "30_day_Fib_50: 1.47%\n",
      "EMA_5: 0.95%\n",
      "SMA_5: 0.81%\n",
      "EMA_50: 0.66%\n",
      "SMA_50: 0.60%\n",
      "30_day_Fib_61: 0.43%\n",
      "Fib_5_Low_Min: 0.43%\n",
      "10_day_Fib_23: 0.30%\n",
      "VWAP: 0.30%\n",
      "ATR_Prev_Close: 0.22%\n",
      "Std_Dev: 0.20%\n",
      "Cumulative_Price_Volume: 0.16%\n",
      "EMA_26_MACD: 0.13%\n",
      "5_day-Fib_23: 0.11%\n",
      "Fib_5_High_Max: 0.11%\n",
      "30_day_Fib_38: 0.09%\n",
      "Lower_Band: 0.08%\n",
      "ATR_True_Range: 0.08%\n",
      "10_day_Fib_50: 0.07%\n",
      "10_day_Fib_61: 0.06%\n",
      "SMA_20: 0.06%\n",
      "Cumulative_Volume: 0.04%\n",
      "Fib_10_High_Max: 0.03%\n",
      "ATR: 0.03%\n",
      "ATR_High_Low: 0.02%\n",
      "30_day_Fib_23: 0.02%\n",
      "EMA_12_MACD: 0.02%\n",
      "Volume_lag_1: 0.02%\n",
      "Upper_Band: 0.02%\n",
      "Volume_lag_5: 0.02%\n",
      "Volume_lag_3: 0.01%\n",
      "Volume_lag_7: 0.01%\n",
      "MACD: 0.01%\n",
      "Signal_Line: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "%K: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "%D: 0.00%\n",
      "10_day_Fib_38: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_20: 0.00%\n",
      "RSI: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_1_week_LR_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df53d222-e712-4709-82eb-3999a328899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3635638608.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3635638608.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (867895, 71)\n",
      "Testing data shape: (430180, 71)\n",
      "X_train shape: (867895, 67), y_train shape: (867895,)\n",
      "X_test shape: (430180, 67), y_test shape: (430180,)\n",
      "Mean Squared Error on unseen data (post-February 17, 2024): 9120.560692357518\n"
     ]
    }
   ],
   "source": [
    "# removing important features led to the degredation of all metrics\n",
    "# we're going to use all metrics again and try adjusting a few of the hyper parameters\n",
    "#\n",
    "# i think this one would actually be the baseline, as i can separate the dates and test only\n",
    "# after feb 10 which is what i want to do\n",
    "# it also contains scaled data, which was better\n",
    "# learning_rate = 0.1\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to February 10, 2024 for training\n",
    "df_stock_data_train = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2024-02-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 17, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2024-02-17']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_week_LR_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_week_LR_1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred = model_baseline_1_week_LR_1.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5c41240-ce28-461a-8ca2-e17fa249276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 9120.560692357518\n",
      "Mean Absolute Error on unseen data: 11.246323507268523\n",
      "Root Mean Squared Error on unseen data: 95.50162664770437\n",
      "R-squared on unseen data: 0.9587582431001249\n",
      "Median Absolute Error on unseen data: 1.6834745407104492\n",
      "Durbin-Watson Statistic on unseen data: 0.1256779195880115\n",
      "MAPE on unseen data: 4.18%\n",
      "Fib_10_Low_Min: 58.56%\n",
      "Fib_30_High_Max: 17.30%\n",
      "Low: 7.19%\n",
      "High: 5.55%\n",
      "EMA_5: 2.33%\n",
      "Fib_30_Low_Min: 2.28%\n",
      "Volume: 1.49%\n",
      "SMA_5: 1.13%\n",
      "5_day-Fib_38: 0.55%\n",
      "30_day_Fib_50: 0.47%\n",
      "EMA_50: 0.45%\n",
      "30_day_Fib_61: 0.31%\n",
      "Cumulative_Price_Volume: 0.30%\n",
      "Fib_5_Low_Min: 0.25%\n",
      "5_day-Fib_61: 0.24%\n",
      "5_day-Fib_50: 0.20%\n",
      "Lower_Band: 0.20%\n",
      "VWAP: 0.18%\n",
      "Std_Dev: 0.16%\n",
      "5_day-Fib_23: 0.15%\n",
      "10_day_Fib_23: 0.12%\n",
      "SMA_50: 0.10%\n",
      "ATR_True_Range: 0.07%\n",
      "EMA_26_MACD: 0.06%\n",
      "ATR_Prev_Close: 0.06%\n",
      "Cumulative_Volume: 0.05%\n",
      "Fib_5_High_Max: 0.05%\n",
      "ATR: 0.02%\n",
      "30_day_Fib_23: 0.02%\n",
      "Volume_lag_1: 0.01%\n",
      "30_day_Fib_38: 0.01%\n",
      "SMA_20: 0.01%\n",
      "ATR_High_Low: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "Volume_lag_3: 0.01%\n",
      "Upper_Band: 0.01%\n",
      "EMA_5_lag_1: 0.01%\n",
      "10_day_Fib_61: 0.01%\n",
      "MACD: 0.01%\n",
      "Volume_lag_7: 0.01%\n",
      "Signal_Line: 0.00%\n",
      "%K: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "%D: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "RSI: 0.00%\n",
      "Fib_10_High_Max: 0.00%\n",
      "10_day_Fib_38: 0.00%\n",
      "EMA_12_MACD: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "EMA_20: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_1_week_LR_1.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b5bf201-dd97-426a-a0e9-ae5951cf55b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/370453240.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/370453240.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (867895, 71)\n",
      "Testing data shape: (430180, 71)\n",
      "X_train shape: (867895, 67), y_train shape: (867895,)\n",
      "X_test shape: (430180, 67), y_test shape: (430180,)\n",
      "Mean Squared Error on unseen data (post-February 17, 2024): 9833.324126183123\n"
     ]
    }
   ],
   "source": [
    "# learning_rate outcome: learning_rate=0.01 showed the best improvement\n",
    "# and had better metrics than the baseline, so we'll keep it and now tweak max_depth\n",
    "\n",
    "# max_depth = 3\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to February 10, 2024 for training\n",
    "df_stock_data_train = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2024-02-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 17, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2024-02-17']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_week_MD_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_week_MD_3.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred = model_1_week_MD_3.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e63f1ff1-97d5-4986-840e-51abb90c8117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 9833.324126183123\n",
      "Mean Absolute Error on unseen data: 10.908578656742478\n",
      "Root Mean Squared Error on unseen data: 99.16311878003395\n",
      "R-squared on unseen data: 0.9555352377108195\n",
      "Median Absolute Error on unseen data: 1.6677398681640625\n",
      "Durbin-Watson Statistic on unseen data: 0.06765623238259566\n",
      "MAPE on unseen data: 5.19%\n",
      "Fib_10_Low_Min: 38.36%\n",
      "Fib_30_High_Max: 30.34%\n",
      "Low: 3.86%\n",
      "Fib_30_Low_Min: 3.53%\n",
      "5_day-Fib_50: 3.31%\n",
      "VWAP: 2.95%\n",
      "High: 2.52%\n",
      "Fib_5_Low_Min: 2.36%\n",
      "Volume: 1.87%\n",
      "5_day-Fib_38: 1.63%\n",
      "EMA_26_MACD: 1.39%\n",
      "10_day_Fib_61: 1.15%\n",
      "10_day_Fib_23: 1.01%\n",
      "5_day-Fib_61: 0.86%\n",
      "EMA_5: 0.80%\n",
      "ATR_High_Low: 0.62%\n",
      "SMA_5: 0.39%\n",
      "Std_Dev: 0.39%\n",
      "Lower_Band: 0.32%\n",
      "ATR: 0.29%\n",
      "SMA_50: 0.24%\n",
      "Cumulative_Price_Volume: 0.24%\n",
      "ATR_Prev_Close: 0.21%\n",
      "ATR_True_Range: 0.17%\n",
      "SMA_20: 0.15%\n",
      "5_day-Fib_23: 0.13%\n",
      "Close_lag_7: 0.13%\n",
      "30_day_Fib_50: 0.11%\n",
      "Volume_lag_3: 0.08%\n",
      "EMA_12_MACD: 0.07%\n",
      "30_day_Fib_61: 0.07%\n",
      "Cumulative_Volume: 0.06%\n",
      "EMA_5_lag_3: 0.04%\n",
      "ATR_High_Close: 0.04%\n",
      "Signal_Line: 0.03%\n",
      "MACD: 0.03%\n",
      "Close_lag_3: 0.03%\n",
      "Volume_lag_1: 0.03%\n",
      "EMA_20: 0.02%\n",
      "EMA_50: 0.02%\n",
      "10_day_Fib_38: 0.02%\n",
      "30_day_Fib_38: 0.02%\n",
      "Volume_lag_5: 0.01%\n",
      "Fib_10_High_Max: 0.01%\n",
      "30_day_Fib_23: 0.01%\n",
      "Upper_Band: 0.01%\n",
      "Fib_5_High_Max: 0.01%\n",
      "EMA_5_lag_7: 0.01%\n",
      "EMA_5_lag_5: 0.01%\n",
      "SMA_5_lag_3: 0.01%\n",
      "Volume_lag_7: 0.01%\n",
      "Close_lag_5: 0.01%\n",
      "MACD_Histogram: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "%K: 0.00%\n",
      "%D: 0.00%\n",
      "RSI: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_1_week_MD_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "976e4ee4-0759-442d-b08c-f91e606d533c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1812720381.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1812720381.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (867895, 71)\n",
      "Testing data shape: (430180, 71)\n",
      "X_train shape: (867895, 67), y_train shape: (867895,)\n",
      "X_test shape: (430180, 67), y_test shape: (430180,)\n",
      "Mean Squared Error on unseen data (post-February 17, 2024): 7604.147083529955\n"
     ]
    }
   ],
   "source": [
    "# learning_rate outcome: learning_rate=0.01 showed the best improvement\n",
    "# and had better metrics than the baseline, so we'll keep it and now tweak max_depth\n",
    "\n",
    "# max_depth = 7\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_week = df_stock_data_1_week.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to February 10, 2024 for training\n",
    "df_stock_data_train = df_stock_data_1_week[df_stock_data_1_week['Date'] <= '2024-02-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 17, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_week[df_stock_data_1_week['Date'] > '2024-02-17']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 5 trading days ahead\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-5)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-5)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_week_MD_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_week_MD_7.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (February 17, 2024, onwards)\n",
    "y_pred = model_1_week_MD_7.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 17, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ef582a0-083b-41ec-931f-8d6721af35c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 7604.147083529955\n",
      "Mean Absolute Error on unseen data: 10.044193260395863\n",
      "Root Mean Squared Error on unseen data: 87.20176078227982\n",
      "R-squared on unseen data: 0.9656152295864199\n",
      "Median Absolute Error on unseen data: 1.588897705078125\n",
      "Durbin-Watson Statistic on unseen data: 0.1674710246723756\n",
      "MAPE on unseen data: 3.96%\n",
      "Fib_10_Low_Min: 63.23%\n",
      "Fib_30_High_Max: 14.79%\n",
      "Low: 4.90%\n",
      "5_day-Fib_38: 4.25%\n",
      "High: 3.05%\n",
      "5_day-Fib_61: 1.76%\n",
      "10_day_Fib_23: 1.33%\n",
      "Fib_30_Low_Min: 1.28%\n",
      "Volume: 1.07%\n",
      "Fib_5_Low_Min: 0.89%\n",
      "5_day-Fib_50: 0.66%\n",
      "EMA_5: 0.49%\n",
      "SMA_5: 0.44%\n",
      "30_day_Fib_38: 0.42%\n",
      "5_day-Fib_23: 0.24%\n",
      "Std_Dev: 0.20%\n",
      "Cumulative_Price_Volume: 0.20%\n",
      "EMA_12_MACD: 0.08%\n",
      "SMA_20: 0.08%\n",
      "30_day_Fib_50: 0.06%\n",
      "VWAP: 0.05%\n",
      "30_day_Fib_61: 0.05%\n",
      "SMA_5_lag_7: 0.05%\n",
      "ATR_True_Range: 0.05%\n",
      "SMA_50: 0.05%\n",
      "ATR_Prev_Close: 0.05%\n",
      "Cumulative_Volume: 0.05%\n",
      "Fib_5_High_Max: 0.04%\n",
      "EMA_20: 0.04%\n",
      "ATR: 0.03%\n",
      "Upper_Band: 0.03%\n",
      "Fib_10_High_Max: 0.01%\n",
      "Lower_Band: 0.01%\n",
      "Volume_lag_1: 0.01%\n",
      "EMA_26_MACD: 0.01%\n",
      "EMA_50: 0.01%\n",
      "10_day_Fib_61: 0.01%\n",
      "Volume_lag_7: 0.00%\n",
      "Signal_Line: 0.00%\n",
      "Volume_lag_3: 0.00%\n",
      "30_day_Fib_23: 0.00%\n",
      "10_day_Fib_38: 0.00%\n",
      "MACD: 0.00%\n",
      "Volume_lag_5: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "ATR_High_Low: 0.00%\n",
      "RSI: 0.00%\n",
      "%K: 0.00%\n",
      "%D: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_1_week_MD_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b727455-bcda-46ef-98ab-fcafa2f7d5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 6)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=16, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=16, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 1 Week Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfe46361-f5ba-45a1-9d2d-63f24006a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will modify our feature set to add bigger lagging indicators.\n",
    "# Create a new dataframe called 'df_stock_data_1_month' as a copy of 'df_stocks_price_ta'\n",
    "df_stock_data_1_month = df_stocks_price_ta.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a86e5901-1483-4177-b075-cebdf459bd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>SMA_20_lag_15</th>\n",
       "      <th>SMA_20_lag_20</th>\n",
       "      <th>EMA_20_lag_1</th>\n",
       "      <th>EMA_20_lag_3</th>\n",
       "      <th>EMA_20_lag_5</th>\n",
       "      <th>EMA_20_lag_7</th>\n",
       "      <th>EMA_20_lag_10</th>\n",
       "      <th>EMA_20_lag_12</th>\n",
       "      <th>EMA_20_lag_15</th>\n",
       "      <th>EMA_20_lag_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.02</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.97</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.479048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.57</td>\n",
       "      <td>10.22</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.454377</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.39</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.464436</td>\n",
       "      <td>10.479048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close   High    Low     Volume      SMA_5     SMA_20  \\\n",
       "0   AAPL  2022-02-10  10.52  10.73  10.20  1037700.0  10.520000  10.520000   \n",
       "1   AAPL  2022-02-11  10.09  10.49  10.02   480300.0  10.305000  10.305000   \n",
       "2   AAPL  2022-02-14  10.22  10.46   9.97   724400.0  10.276667  10.276667   \n",
       "3   AAPL  2022-02-15  10.56  10.57  10.22   758700.0  10.347500  10.347500   \n",
       "4   AAPL  2022-02-16  10.44  10.55  10.39   685200.0  10.366000  10.366000   \n",
       "\n",
       "      SMA_50      EMA_5  ...  SMA_20_lag_15  SMA_20_lag_20  EMA_20_lag_1  \\\n",
       "0  10.520000  10.520000  ...            NaN            NaN           NaN   \n",
       "1  10.305000  10.376667  ...            NaN            NaN     10.520000   \n",
       "2  10.276667  10.324445  ...            NaN            NaN     10.479048   \n",
       "3  10.347500  10.402963  ...            NaN            NaN     10.454377   \n",
       "4  10.366000  10.415309  ...            NaN            NaN     10.464436   \n",
       "\n",
       "   EMA_20_lag_3  EMA_20_lag_5  EMA_20_lag_7  EMA_20_lag_10  EMA_20_lag_12  \\\n",
       "0           NaN           NaN           NaN            NaN            NaN   \n",
       "1           NaN           NaN           NaN            NaN            NaN   \n",
       "2           NaN           NaN           NaN            NaN            NaN   \n",
       "3     10.520000           NaN           NaN            NaN            NaN   \n",
       "4     10.479048           NaN           NaN            NaN            NaN   \n",
       "\n",
       "   EMA_20_lag_15  EMA_20_lag_20  \n",
       "0            NaN            NaN  \n",
       "1            NaN            NaN  \n",
       "2            NaN            NaN  \n",
       "3            NaN            NaN  \n",
       "4            NaN            NaN  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of columns to create lags for (focusing on short-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_5', 'EMA_5', 'Volume', 'EMA_12_MACD', 'SMA_20', 'EMA_20']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 12, 15, 20, 30, 60, 90, 180, 360] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7, 10, 12, 15, 20]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_stock_data_1_month[f'{col}_lag_{lag}'] = df_stock_data_1_month[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_stock_data_1_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "95da38ef-e130-4b2d-bc33-4e937217952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/528320042.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/528320042.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (804275, 111)\n",
      "Testing data shape: (412225, 111)\n",
      "X_train shape: (804275, 107), y_train shape: (804275,)\n",
      "X_test shape: (412225, 107), y_test shape: (412225,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 11580.595262448778\n"
     ]
    }
   ],
   "source": [
    "# now we're going to move onto our next model: 1 month prediction\n",
    "# we'll start at our baseline model and then do the same as we just did\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_month.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_1_month.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5eeb8f43-31bf-4b04-9166-b2223d530da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 11580.595262448778\n",
      "Mean Absolute Error on unseen data: 16.106877200112017\n",
      "Root Mean Squared Error on unseen data: 107.61317420487502\n",
      "R-squared on unseen data: 0.9480862120608777\n",
      "Median Absolute Error on unseen data: 3.451629638671875\n",
      "Durbin-Watson Statistic on unseen data: 0.07080939887518617\n",
      "MAPE on unseen data: 7.98%\n",
      "Fib_30_High_Max: 35.64%\n",
      "30_day_Fib_23: 31.34%\n",
      "High: 5.57%\n",
      "Fib_30_Low_Min: 5.56%\n",
      "Low: 5.24%\n",
      "EMA_5: 3.63%\n",
      "30_day_Fib_50: 2.97%\n",
      "Volume: 2.96%\n",
      "Fib_5_Low_Min: 1.49%\n",
      "Fib_5_High_Max: 0.85%\n",
      "EMA_26_MACD: 0.55%\n",
      "10_day_Fib_38: 0.52%\n",
      "ATR_Prev_Close: 0.41%\n",
      "Fib_10_Low_Min: 0.40%\n",
      "5_day-Fib_23: 0.36%\n",
      "SMA_5: 0.35%\n",
      "VWAP: 0.35%\n",
      "Cumulative_Price_Volume: 0.29%\n",
      "Std_Dev: 0.21%\n",
      "Fib_10_High_Max: 0.21%\n",
      "Lower_Band: 0.15%\n",
      "ATR_True_Range: 0.12%\n",
      "30_day_Fib_61: 0.11%\n",
      "EMA_50: 0.09%\n",
      "SMA_50: 0.09%\n",
      "ATR: 0.06%\n",
      "Upper_Band: 0.06%\n",
      "Cumulative_Volume: 0.05%\n",
      "EMA_20_lag_20: 0.04%\n",
      "SMA_5_lag_15: 0.04%\n",
      "EMA_12_MACD_lag_12: 0.03%\n",
      "Volume_lag_1: 0.03%\n",
      "Volume_lag_20: 0.02%\n",
      "ATR_High_Low: 0.02%\n",
      "SMA_20_lag_20: 0.01%\n",
      "EMA_20_lag_1: 0.01%\n",
      "Volume_lag_3: 0.01%\n",
      "EMA_12_MACD_lag_20: 0.01%\n",
      "EMA_5_lag_1: 0.01%\n",
      "Volume_lag_15: 0.01%\n",
      "Volume_lag_12: 0.01%\n",
      "EMA_5_lag_15: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "MACD: 0.01%\n",
      "10_day_Fib_61: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "RSI: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "%K: 0.00%\n",
      "%D: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "30_day_Fib_38: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "Volume_lag_7: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "SMA_20: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "Volume_lag_10: 0.00%\n",
      "SMA_20_lag_15: 0.00%\n",
      "EMA_20_lag_10: 0.00%\n",
      "EMA_5_lag_12: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "5_day-Fib_50: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "EMA_20_lag_12: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "EMA_12_MACD: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "SMA_20_lag_12: 0.00%\n",
      "EMA_20: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "Close_lag_12: 0.00%\n",
      "SMA_5_lag_12: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "5_day-Fib_61: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_1_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4bcf0fd0-3c2f-4376-b4a4-593709c8828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with more than 1% contribution:\n",
      "Fib_30_High_Max: 35.64%\n",
      "30_day_Fib_23: 31.34%\n",
      "High: 5.57%\n",
      "Fib_30_Low_Min: 5.56%\n",
      "Low: 5.24%\n",
      "EMA_5: 3.63%\n",
      "30_day_Fib_50: 2.97%\n",
      "Volume: 2.96%\n",
      "Fib_5_Low_Min: 1.49%\n",
      "List of important features:\n",
      "['Fib_30_High_Max', '30_day_Fib_23', 'High', 'Fib_30_Low_Min', 'Low', 'EMA_5', '30_day_Fib_50', 'Volume', 'Fib_5_Low_Min']\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance from the baseline model (1-week prediction)\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_1_month.feature_importances_))\n",
    "\n",
    "# Filter features with importance greater than 1%\n",
    "important_features = {feature: importance for feature, importance in feature_importance.items() if importance > 0.01}\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_important_features = sorted(important_features.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the feature names (keys) into a list\n",
    "important_feature_names = [feature for feature, importance in sorted_important_features]\n",
    "\n",
    "# Print the sorted important features (optional)\n",
    "print(\"Features with more than 1% contribution:\")\n",
    "for feature in sorted_important_features:\n",
    "    print(f\"{feature[0]}: {feature[1] * 100:.2f}%\")\n",
    "\n",
    "# The list of important features that you can use to create a new dataframe\n",
    "print(\"List of important features:\")\n",
    "print(important_feature_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "469fa610-eb03-442c-bbf2-bdbc978ded41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Fib_30_High_Max</th>\n",
       "      <th>30_day_Fib_23</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Fib_30_Low_Min</th>\n",
       "      <th>Volume</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>30_day_Fib_50</th>\n",
       "      <th>Fib_5_Low_Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171141</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>50.058052</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>50.877780</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>49.918547</td>\n",
       "      <td>49.918547</td>\n",
       "      <td>1889800.0</td>\n",
       "      <td>50.058052</td>\n",
       "      <td>50.546317</td>\n",
       "      <td>49.918547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171142</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>47.580647</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>50.244295</td>\n",
       "      <td>49.028604</td>\n",
       "      <td>47.234290</td>\n",
       "      <td>47.234290</td>\n",
       "      <td>3747400.0</td>\n",
       "      <td>49.232250</td>\n",
       "      <td>49.204189</td>\n",
       "      <td>47.234290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171143</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>47.210232</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>50.140984</td>\n",
       "      <td>47.460380</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>3157800.0</td>\n",
       "      <td>48.558244</td>\n",
       "      <td>48.985308</td>\n",
       "      <td>46.796529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171144</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>48.759216</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>50.140984</td>\n",
       "      <td>49.153677</td>\n",
       "      <td>48.427291</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>1900000.0</td>\n",
       "      <td>48.625235</td>\n",
       "      <td>48.985308</td>\n",
       "      <td>46.796529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171145</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>49.629917</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>50.140984</td>\n",
       "      <td>49.716506</td>\n",
       "      <td>48.855427</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>2010600.0</td>\n",
       "      <td>48.960129</td>\n",
       "      <td>48.985308</td>\n",
       "      <td>46.796529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Symbol        Date      Close  Fib_30_High_Max  30_day_Fib_23  \\\n",
       "171141      A  2022-02-10  50.058052        51.174088      50.877780   \n",
       "171142      A  2022-02-11  47.580647        51.174088      50.244295   \n",
       "171143      A  2022-02-14  47.210232        51.174088      50.140984   \n",
       "171144      A  2022-02-15  48.759216        51.174088      50.140984   \n",
       "171145      A  2022-02-16  49.629917        51.174088      50.140984   \n",
       "\n",
       "             High        Low  Fib_30_Low_Min     Volume      EMA_5  \\\n",
       "171141  51.174088  49.918547       49.918547  1889800.0  50.058052   \n",
       "171142  49.028604  47.234290       47.234290  3747400.0  49.232250   \n",
       "171143  47.460380  46.796529       46.796529  3157800.0  48.558244   \n",
       "171144  49.153677  48.427291       46.796529  1900000.0  48.625235   \n",
       "171145  49.716506  48.855427       46.796529  2010600.0  48.960129   \n",
       "\n",
       "        30_day_Fib_50  Fib_5_Low_Min  \n",
       "171141      50.546317      49.918547  \n",
       "171142      49.204189      47.234290  \n",
       "171143      48.985308      46.796529  \n",
       "171144      48.985308      46.796529  \n",
       "171145      48.985308      46.796529  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = ['Symbol', 'Date', 'Close', 'Fib_30_High_Max', '30_day_Fib_23',\n",
    "                      'High', 'Low', 'Fib_30_Low_Min', 'Volume', 'EMA_5', '30_day_Fib_50',\n",
    "                      'Fib_5_Low_Min']\n",
    "df_important_feat_1_month = df_stock_data_1_month[important_features]\n",
    "df_important_feat_1_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "506ad4e4-9e00-4f0a-8232-ed5573072f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2130356763.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2130356763.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (804275, 13)\n",
      "Testing data shape: (412225, 13)\n",
      "X_train shape: (804275, 9), y_train shape: (804275,)\n",
      "X_test shape: (412225, 9), y_test shape: (412225,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 32079.230668815384\n"
     ]
    }
   ],
   "source": [
    "# baseline 1 month prediction model with only features contributing over 1%\n",
    "# not as good as baseline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_important_feat_1_month = df_important_feat_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train = df_important_feat_1_month[df_important_feat_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_important_feat_1_month[df_important_feat_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_if_1_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_if_1_month.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_if_1_month.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "20339820-f94b-4c03-83cd-3bf8ac18595d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 32079.230668815384\n",
      "Mean Absolute Error on unseen data: 21.58220780969353\n",
      "Root Mean Squared Error on unseen data: 179.10675774189923\n",
      "R-squared on unseen data: 0.8561944062071536\n",
      "Median Absolute Error on unseen data: 3.2651214599609375\n",
      "Durbin-Watson Statistic on unseen data: 0.04222523242030978\n",
      "MAPE on unseen data: 7.63%\n",
      "Fib_30_High_Max: 38.32%\n",
      "30_day_Fib_23: 25.10%\n",
      "High: 12.95%\n",
      "Low: 11.06%\n",
      "Fib_30_Low_Min: 5.84%\n",
      "Volume: 2.25%\n",
      "30_day_Fib_50: 1.67%\n",
      "EMA_5: 1.61%\n",
      "Fib_5_Low_Min: 1.19%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_if_1_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1861cb51-cd63-4973-944a-35e4679656d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3695983608.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3695983608.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (804275, 111)\n",
      "Testing data shape: (412225, 111)\n",
      "X_train shape: (804275, 107), y_train shape: (804275,)\n",
      "X_test shape: (412225, 107), y_test shape: (412225,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 12131.686683682172\n"
     ]
    }
   ],
   "source": [
    "# 1 month baseline model with learning_rate=0.1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_tr_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_tr_01.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_1_month_tr_01.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b3a25ea7-1c21-4032-9fcd-8e1963941b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 12131.686683682172\n",
      "Mean Absolute Error on unseen data: 16.7761954501155\n",
      "Root Mean Squared Error on unseen data: 110.14393620931736\n",
      "R-squared on unseen data: 0.9456157653758314\n",
      "Median Absolute Error on unseen data: 3.590789794921875\n",
      "Durbin-Watson Statistic on unseen data: 0.06393498702616922\n",
      "MAPE on unseen data: 8.30%\n",
      "Fib_30_High_Max: 39.63%\n",
      "30_day_Fib_23: 25.74%\n",
      "Low: 6.83%\n",
      "High: 6.59%\n",
      "Fib_30_Low_Min: 4.69%\n",
      "30_day_Fib_50: 4.25%\n",
      "Volume: 3.47%\n",
      "EMA_5: 2.92%\n",
      "Fib_5_Low_Min: 0.77%\n",
      "Fib_10_Low_Min: 0.64%\n",
      "Fib_5_High_Max: 0.63%\n",
      "10_day_Fib_38: 0.57%\n",
      "ATR_Prev_Close: 0.54%\n",
      "EMA_26_MACD: 0.53%\n",
      "Cumulative_Price_Volume: 0.36%\n",
      "VWAP: 0.35%\n",
      "Std_Dev: 0.24%\n",
      "Lower_Band: 0.15%\n",
      "5_day-Fib_23: 0.15%\n",
      "ATR_True_Range: 0.14%\n",
      "EMA_50: 0.08%\n",
      "Cumulative_Volume: 0.05%\n",
      "ATR: 0.05%\n",
      "SMA_5_lag_15: 0.05%\n",
      "30_day_Fib_38: 0.05%\n",
      "SMA_50: 0.05%\n",
      "30_day_Fib_61: 0.05%\n",
      "EMA_20_lag_20: 0.04%\n",
      "Upper_Band: 0.04%\n",
      "Volume_lag_1: 0.03%\n",
      "Fib_10_High_Max: 0.03%\n",
      "EMA_12_MACD_lag_12: 0.02%\n",
      "ATR_High_Low: 0.01%\n",
      "Volume_lag_3: 0.01%\n",
      "EMA_12_MACD_lag_20: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "MACD: 0.01%\n",
      "Volume_lag_15: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "SMA_20_lag_20: 0.01%\n",
      "Volume_lag_20: 0.01%\n",
      "RSI: 0.01%\n",
      "ATR_High_Close: 0.01%\n",
      "EMA_20_lag_10: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "Volume_lag_5: 0.00%\n",
      "SMA_20_lag_12: 0.00%\n",
      "%D: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "SMA_5_lag_12: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "SMA_20: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "%K: 0.00%\n",
      "Volume_lag_7: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "Volume_lag_12: 0.00%\n",
      "EMA_20_lag_12: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "SMA_20_lag_15: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "EMA_12_MACD: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "EMA_20: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "5_day-Fib_50: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "10_day_Fib_61: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "5_day-Fib_61: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "Close_lag_12: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "EMA_5_lag_12: 0.00%\n",
      "SMA_5: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_1_month_tr_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d241485c-b2a1-41a6-ae31-03bf3500db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/167776123.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/167776123.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (804275, 111)\n",
      "Testing data shape: (412225, 111)\n",
      "X_train shape: (804275, 107), y_train shape: (804275,)\n",
      "X_test shape: (412225, 107), y_test shape: (412225,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 11047.206778467535\n"
     ]
    }
   ],
   "source": [
    "# 1 month baseline model with learning_rate=0.01\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_tr_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_tr_1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_1_month_tr_1.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5dc2f654-7efe-4059-8f53-bbcfdfedf986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 11047.206778467535\n",
      "Mean Absolute Error on unseen data: 15.37042669814166\n",
      "Root Mean Squared Error on unseen data: 105.10569336847331\n",
      "R-squared on unseen data: 0.9504772995670925\n",
      "Median Absolute Error on unseen data: 3.3103179931640625\n",
      "Durbin-Watson Statistic on unseen data: 0.07154779960843687\n",
      "MAPE on unseen data: 8.15%\n",
      "Fib_30_High_Max: 39.63%\n",
      "30_day_Fib_23: 25.74%\n",
      "Low: 6.83%\n",
      "High: 6.59%\n",
      "Fib_30_Low_Min: 4.69%\n",
      "30_day_Fib_50: 4.25%\n",
      "Volume: 3.47%\n",
      "EMA_5: 2.92%\n",
      "Fib_5_Low_Min: 0.77%\n",
      "Fib_10_Low_Min: 0.64%\n",
      "Fib_5_High_Max: 0.63%\n",
      "10_day_Fib_38: 0.57%\n",
      "ATR_Prev_Close: 0.54%\n",
      "EMA_26_MACD: 0.53%\n",
      "Cumulative_Price_Volume: 0.36%\n",
      "VWAP: 0.35%\n",
      "Std_Dev: 0.24%\n",
      "Lower_Band: 0.15%\n",
      "5_day-Fib_23: 0.15%\n",
      "ATR_True_Range: 0.14%\n",
      "EMA_50: 0.08%\n",
      "Cumulative_Volume: 0.05%\n",
      "ATR: 0.05%\n",
      "SMA_5_lag_15: 0.05%\n",
      "30_day_Fib_38: 0.05%\n",
      "SMA_50: 0.05%\n",
      "30_day_Fib_61: 0.05%\n",
      "EMA_20_lag_20: 0.04%\n",
      "Upper_Band: 0.04%\n",
      "Volume_lag_1: 0.03%\n",
      "Fib_10_High_Max: 0.03%\n",
      "EMA_12_MACD_lag_12: 0.02%\n",
      "ATR_High_Low: 0.01%\n",
      "Volume_lag_3: 0.01%\n",
      "EMA_12_MACD_lag_20: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "MACD: 0.01%\n",
      "Volume_lag_15: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "SMA_20_lag_20: 0.01%\n",
      "Volume_lag_20: 0.01%\n",
      "RSI: 0.01%\n",
      "ATR_High_Close: 0.01%\n",
      "EMA_20_lag_10: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "Volume_lag_5: 0.00%\n",
      "SMA_20_lag_12: 0.00%\n",
      "%D: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "SMA_5_lag_12: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "SMA_20: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "%K: 0.00%\n",
      "Volume_lag_7: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "Volume_lag_12: 0.00%\n",
      "EMA_20_lag_12: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "SMA_20_lag_15: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "EMA_12_MACD: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "EMA_20: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "5_day-Fib_50: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "10_day_Fib_61: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "5_day-Fib_61: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "Close_lag_12: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "EMA_5_lag_12: 0.00%\n",
      "SMA_5: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_1_month_tr_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "07e3e270-fd95-4daa-86ef-fce7ca7ead0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1837604767.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1837604767.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (804275, 111)\n",
      "Testing data shape: (412225, 111)\n",
      "X_train shape: (804275, 107), y_train shape: (804275,)\n",
      "X_test shape: (412225, 107), y_test shape: (412225,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 12105.89455732451\n"
     ]
    }
   ],
   "source": [
    "# model with learning_rate = 0.01 is best again, so we keep that parameter\n",
    "# now we'll do max depth\n",
    "# max depth = 3\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_md_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_md_3.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_1_month_md_3.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4cdb1a2c-a665-4731-b977-0d3e517b69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 12105.89455732451\n",
      "Mean Absolute Error on unseen data: 15.257036908276845\n",
      "Root Mean Squared Error on unseen data: 110.02679018004892\n",
      "R-squared on unseen data: 0.9457313869779931\n",
      "Median Absolute Error on unseen data: 3.3381423950195312\n",
      "Durbin-Watson Statistic on unseen data: 0.04725178854812182\n",
      "MAPE on unseen data: 9.04%\n",
      "Fib_30_High_Max: 46.42%\n",
      "30_day_Fib_23: 6.82%\n",
      "Fib_30_Low_Min: 6.65%\n",
      "Upper_Band: 6.59%\n",
      "10_day_Fib_50: 5.20%\n",
      "10_day_Fib_38: 3.55%\n",
      "Low: 2.03%\n",
      "Volume: 1.97%\n",
      "Close_lag_1: 1.91%\n",
      "High: 1.82%\n",
      "EMA_20: 1.80%\n",
      "EMA_5: 1.52%\n",
      "30_day_Fib_61: 1.43%\n",
      "Fib_10_Low_Min: 1.35%\n",
      "EMA_5_lag_15: 1.30%\n",
      "Fib_5_High_Max: 0.90%\n",
      "SMA_50: 0.88%\n",
      "ATR: 0.79%\n",
      "VWAP: 0.69%\n",
      "EMA_50: 0.66%\n",
      "ATR_High_Low: 0.64%\n",
      "Fib_5_Low_Min: 0.57%\n",
      "EMA_26_MACD: 0.49%\n",
      "Fib_10_High_Max: 0.46%\n",
      "Close_lag_20: 0.43%\n",
      "30_day_Fib_50: 0.40%\n",
      "Lower_Band: 0.35%\n",
      "ATR_True_Range: 0.30%\n",
      "Volume_lag_1: 0.28%\n",
      "Std_Dev: 0.26%\n",
      "ATR_Prev_Close: 0.24%\n",
      "SMA_20_lag_20: 0.21%\n",
      "Cumulative_Price_Volume: 0.18%\n",
      "30_day_Fib_38: 0.13%\n",
      "Volume_lag_7: 0.08%\n",
      "Close_lag_3: 0.07%\n",
      "EMA_12_MACD_lag_20: 0.07%\n",
      "Close_lag_5: 0.06%\n",
      "5_day-Fib_23: 0.06%\n",
      "EMA_12_MACD_lag_12: 0.05%\n",
      "Cumulative_Volume: 0.04%\n",
      "Volume_lag_5: 0.04%\n",
      "Volume_lag_12: 0.04%\n",
      "SMA_5: 0.03%\n",
      "MACD: 0.02%\n",
      "EMA_12_MACD_lag_10: 0.02%\n",
      "Signal_Line: 0.01%\n",
      "RSI: 0.01%\n",
      "EMA_20_lag_20: 0.01%\n",
      "Volume_lag_15: 0.01%\n",
      "SMA_20: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "5_day-Fib_38: 0.01%\n",
      "Volume_lag_3: 0.01%\n",
      "ATR_High_Close: 0.01%\n",
      "SMA_20_lag_15: 0.01%\n",
      "SMA_5_lag_20: 0.01%\n",
      "SMA_20_lag_12: 0.01%\n",
      "EMA_12_MACD_lag_3: 0.01%\n",
      "EMA_12_MACD_lag_1: 0.01%\n",
      "SMA_20_lag_1: 0.01%\n",
      "Close_lag_7: 0.01%\n",
      "EMA_5_lag_20: 0.01%\n",
      "EMA_20_lag_12: 0.01%\n",
      "EMA_5_lag_3: 0.01%\n",
      "EMA_5_lag_12: 0.01%\n",
      "Volume_lag_20: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "%D: 0.00%\n",
      "EMA_12_MACD: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "%K: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "5_day-Fib_61: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "10_day_Fib_61: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_50: 0.00%\n",
      "5_day-Fib_100: 0.00%\n",
      "Close_lag_12: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "SMA_5_lag_12: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "EMA_20_lag_10: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_1_month_md_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7c640a6d-da1c-4cb3-9f2b-20c872bf533c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3637335806.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3637335806.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (804275, 111)\n",
      "Testing data shape: (412225, 111)\n",
      "X_train shape: (804275, 107), y_train shape: (804275,)\n",
      "X_test shape: (412225, 107), y_test shape: (412225,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 10844.724459170979\n"
     ]
    }
   ],
   "source": [
    "# model with learning_rate = 0.01 is best again, so we keep that parameter\n",
    "# now we'll do max depth\n",
    "# max depth = 7\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_month = df_stock_data_1_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to January 10, 2024 for training\n",
    "df_stock_data_train = df_stock_data_1_month[df_stock_data_1_month['Date'] <= '2024-01-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_month[df_stock_data_1_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-20)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-20)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_month_md_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_month_md_7.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_1_month_md_7.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d5f51e8b-000d-4892-9bd6-24ddb35a35f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 10844.724459170979\n",
      "Mean Absolute Error on unseen data: 15.639166248218757\n",
      "Root Mean Squared Error on unseen data: 104.13800679469037\n",
      "R-squared on unseen data: 0.9513849924746814\n",
      "Median Absolute Error on unseen data: 3.326946258544922\n",
      "Durbin-Watson Statistic on unseen data: 0.0750817215794377\n",
      "MAPE on unseen data: 7.82%\n",
      "30_day_Fib_23: 52.41%\n",
      "Fib_30_High_Max: 32.44%\n",
      "Fib_5_Low_Min: 2.25%\n",
      "Low: 2.08%\n",
      "10_day_Fib_23: 1.96%\n",
      "High: 1.77%\n",
      "Volume: 1.43%\n",
      "5_day-Fib_50: 1.11%\n",
      "30_day_Fib_50: 0.68%\n",
      "10_day_Fib_38: 0.62%\n",
      "5_day-Fib_23: 0.48%\n",
      "Fib_10_High_Max: 0.28%\n",
      "Fib_10_Low_Min: 0.25%\n",
      "Std_Dev: 0.21%\n",
      "VWAP: 0.21%\n",
      "ATR_Prev_Close: 0.20%\n",
      "EMA_5: 0.19%\n",
      "Fib_5_High_Max: 0.19%\n",
      "Cumulative_Price_Volume: 0.17%\n",
      "Lower_Band: 0.16%\n",
      "EMA_26_MACD: 0.14%\n",
      "30_day_Fib_61: 0.11%\n",
      "SMA_5: 0.11%\n",
      "SMA_20_lag_1: 0.06%\n",
      "Upper_Band: 0.05%\n",
      "5_day-Fib_61: 0.04%\n",
      "ATR_True_Range: 0.04%\n",
      "Cumulative_Volume: 0.04%\n",
      "30_day_Fib_38: 0.03%\n",
      "10_day_Fib_61: 0.03%\n",
      "EMA_50: 0.02%\n",
      "Fib_30_Low_Min: 0.02%\n",
      "ATR: 0.02%\n",
      "Volume_lag_1: 0.02%\n",
      "Close_lag_1: 0.02%\n",
      "EMA_20_lag_20: 0.02%\n",
      "SMA_50: 0.01%\n",
      "EMA_12_MACD_lag_15: 0.01%\n",
      "EMA_12_MACD_lag_12: 0.01%\n",
      "SMA_20_lag_15: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "Volume_lag_3: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "Signal_Line: 0.00%\n",
      "Volume_lag_20: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "Volume_lag_10: 0.00%\n",
      "Volume_lag_5: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "Volume_lag_7: 0.00%\n",
      "MACD: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "SMA_20_lag_12: 0.00%\n",
      "SMA_5_lag_12: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "SMA_20_lag_20: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "ATR_High_Low: 0.00%\n",
      "Volume_lag_12: 0.00%\n",
      "%D: 0.00%\n",
      "Volume_lag_15: 0.00%\n",
      "EMA_20_lag_10: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "%K: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "EMA_20_lag_12: 0.00%\n",
      "RSI: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_20: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "Close_lag_12: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "SMA_20: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "EMA_20: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "EMA_12_MACD: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "EMA_5_lag_12: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_1_month_md_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "66df734c-0863-4b15-91e7-7f6c3b4ca6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 6)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=16, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=16, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 1 Month Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8e2163b5-dd6b-4c0f-94d3-8df2186bca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will modify our feature set to add bigger lagging indicators.\n",
    "# Create a new dataframe called 'df_stock_data_3_month' as a copy of 'df_stocks_price_ta'\n",
    "df_stock_data_3_month = df_stocks_price_ta.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5edc29a6-e7e2-42b4-a34e-828d6db188ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2984211339.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>EMA_26_MACD_lag_10</th>\n",
       "      <th>EMA_26_MACD_lag_15</th>\n",
       "      <th>EMA_26_MACD_lag_20</th>\n",
       "      <th>EMA_26_MACD_lag_25</th>\n",
       "      <th>EMA_26_MACD_lag_30</th>\n",
       "      <th>EMA_26_MACD_lag_40</th>\n",
       "      <th>EMA_26_MACD_lag_50</th>\n",
       "      <th>EMA_26_MACD_lag_60</th>\n",
       "      <th>EMA_26_MACD_lag_75</th>\n",
       "      <th>EMA_26_MACD_lag_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.02</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.97</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.57</td>\n",
       "      <td>10.22</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.39</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close   High    Low     Volume      SMA_5     SMA_20  \\\n",
       "0   AAPL  2022-02-10  10.52  10.73  10.20  1037700.0  10.520000  10.520000   \n",
       "1   AAPL  2022-02-11  10.09  10.49  10.02   480300.0  10.305000  10.305000   \n",
       "2   AAPL  2022-02-14  10.22  10.46   9.97   724400.0  10.276667  10.276667   \n",
       "3   AAPL  2022-02-15  10.56  10.57  10.22   758700.0  10.347500  10.347500   \n",
       "4   AAPL  2022-02-16  10.44  10.55  10.39   685200.0  10.366000  10.366000   \n",
       "\n",
       "      SMA_50      EMA_5  ...  EMA_26_MACD_lag_10  EMA_26_MACD_lag_15  \\\n",
       "0  10.520000  10.520000  ...                 NaN                 NaN   \n",
       "1  10.305000  10.376667  ...                 NaN                 NaN   \n",
       "2  10.276667  10.324445  ...                 NaN                 NaN   \n",
       "3  10.347500  10.402963  ...                 NaN                 NaN   \n",
       "4  10.366000  10.415309  ...                 NaN                 NaN   \n",
       "\n",
       "   EMA_26_MACD_lag_20  EMA_26_MACD_lag_25  EMA_26_MACD_lag_30  \\\n",
       "0                 NaN                 NaN                 NaN   \n",
       "1                 NaN                 NaN                 NaN   \n",
       "2                 NaN                 NaN                 NaN   \n",
       "3                 NaN                 NaN                 NaN   \n",
       "4                 NaN                 NaN                 NaN   \n",
       "\n",
       "   EMA_26_MACD_lag_40  EMA_26_MACD_lag_50  EMA_26_MACD_lag_60  \\\n",
       "0                 NaN                 NaN                 NaN   \n",
       "1                 NaN                 NaN                 NaN   \n",
       "2                 NaN                 NaN                 NaN   \n",
       "3                 NaN                 NaN                 NaN   \n",
       "4                 NaN                 NaN                 NaN   \n",
       "\n",
       "   EMA_26_MACD_lag_75  EMA_26_MACD_lag_90  \n",
       "0                 NaN                 NaN  \n",
       "1                 NaN                 NaN  \n",
       "2                 NaN                 NaN  \n",
       "3                 NaN                 NaN  \n",
       "4                 NaN                 NaN  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of columns to create lags for (focusing on mid-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_5', 'EMA_5', 'Volume', 'SMA_20',\n",
    "       'SMA_50', 'EMA_5', 'EMA_20', 'EMA_50',  'EMA_12_MACD',\n",
    "       'EMA_26_MACD']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 12, 15, 20, 30, 60, 90, 180, 360] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7, 10, 15, 20, 25, 30, 40, 50, 60, 75, 90]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_stock_data_3_month[f'{col}_lag_{lag}'] = df_stock_data_3_month[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_stock_data_3_month.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "01894bf1-679f-4c3d-a95e-3fcb8be05a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1590437652.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1590437652.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1590437652.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1590437652.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (664084, 195)\n",
      "Testing data shape: (340665, 195)\n",
      "X_train shape: (664084, 191), y_train shape: (664084,)\n",
      "X_test shape: (340665, 191), y_test shape: (340665,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 31301.74125575056\n"
     ]
    }
   ],
   "source": [
    "# now we're going to move onto our next model: 3 month prediction\n",
    "# we'll start at our baseline model and then do the same as we just did\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_3_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_3_month.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_3_month.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4bfae4e6-6f28-4d01-9880-4878ce427875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 31301.74125575056\n",
      "Mean Absolute Error on unseen data: 28.611355863536343\n",
      "Root Mean Squared Error on unseen data: 176.9229811408076\n",
      "R-squared on unseen data: 0.8648566403012485\n",
      "Median Absolute Error on unseen data: 6.425842285156236\n",
      "Durbin-Watson Statistic on unseen data: 0.01850750963651389\n",
      "MAPE on unseen data: 14.09%\n",
      "Fib_30_Low_Min: 33.56%\n",
      "30_day_Fib_38: 19.87%\n",
      "5_day-Fib_61: 12.35%\n",
      "EMA_5: 5.42%\n",
      "5_day-Fib_23: 5.20%\n",
      "EMA_12_MACD: 4.18%\n",
      "Volume: 2.75%\n",
      "High: 2.65%\n",
      "Low: 2.18%\n",
      "30_day_Fib_61: 1.74%\n",
      "ATR_Prev_Close: 1.21%\n",
      "Fib_5_Low_Min: 1.01%\n",
      "Close_lag_1: 0.99%\n",
      "30_day_Fib_50: 0.83%\n",
      "EMA_26_MACD: 0.72%\n",
      "Upper_Band: 0.57%\n",
      "VWAP: 0.49%\n",
      "Lower_Band: 0.40%\n",
      "EMA_20: 0.37%\n",
      "Std_Dev: 0.29%\n",
      "SMA_5_lag_90: 0.23%\n",
      "SMA_50: 0.23%\n",
      "Fib_30_High_Max: 0.20%\n",
      "Fib_5_High_Max: 0.20%\n",
      "10_day_Fib_50: 0.20%\n",
      "Cumulative_Price_Volume: 0.19%\n",
      "EMA_50_lag_7: 0.15%\n",
      "Fib_10_Low_Min: 0.14%\n",
      "ATR_True_Range: 0.12%\n",
      "Fib_10_High_Max: 0.09%\n",
      "ATR_High_Low: 0.07%\n",
      "EMA_50_lag_20: 0.06%\n",
      "EMA_50_lag_60: 0.06%\n",
      "EMA_50_lag_15: 0.05%\n",
      "EMA_26_MACD_lag_15: 0.05%\n",
      "Volume_lag_30: 0.05%\n",
      "EMA_50_lag_25: 0.05%\n",
      "Volume_lag_20: 0.05%\n",
      "Volume_lag_25: 0.05%\n",
      "EMA_12_MACD_lag_60: 0.04%\n",
      "SMA_20_lag_90: 0.04%\n",
      "Volume_lag_3: 0.04%\n",
      "Cumulative_Volume: 0.04%\n",
      "30_day_Fib_23: 0.03%\n",
      "EMA_50_lag_10: 0.03%\n",
      "EMA_50: 0.03%\n",
      "Volume_lag_1: 0.03%\n",
      "EMA_20_lag_60: 0.03%\n",
      "ATR: 0.02%\n",
      "10_day_Fib_38: 0.02%\n",
      "EMA_5_lag_40: 0.02%\n",
      "10_day_Fib_61: 0.02%\n",
      "5_day-Fib_50: 0.02%\n",
      "SMA_5_lag_25: 0.02%\n",
      "EMA_50_lag_30: 0.02%\n",
      "EMA_50_lag_50: 0.02%\n",
      "EMA_26_MACD_lag_50: 0.01%\n",
      "EMA_5_lag_60: 0.01%\n",
      "EMA_12_MACD_lag_50: 0.01%\n",
      "EMA_20_lag_50: 0.01%\n",
      "EMA_50_lag_1: 0.01%\n",
      "SMA_50_lag_60: 0.01%\n",
      "SMA_20_lag_30: 0.01%\n",
      "Volume_lag_40: 0.01%\n",
      "SMA_20_lag_15: 0.01%\n",
      "EMA_20_lag_40: 0.01%\n",
      "SMA_50_lag_50: 0.01%\n",
      "EMA_12_MACD_lag_20: 0.01%\n",
      "MACD: 0.01%\n",
      "SMA_20_lag_50: 0.01%\n",
      "SMA_5_lag_50: 0.01%\n",
      "Close_lag_60: 0.01%\n",
      "SMA_50_lag_20: 0.01%\n",
      "EMA_50_lag_90: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "EMA_5_lag_50: 0.01%\n",
      "SMA_50_lag_10: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "EMA_12_MACD_lag_40: 0.01%\n",
      "SMA_20_lag_25: 0.01%\n",
      "SMA_50_lag_30: 0.01%\n",
      "EMA_26_MACD_lag_10: 0.01%\n",
      "SMA_50_lag_15: 0.01%\n",
      "Volume_lag_75: 0.01%\n",
      "SMA_20_lag_7: 0.01%\n",
      "EMA_50_lag_40: 0.01%\n",
      "Volume_lag_7: 0.01%\n",
      "EMA_5_lag_1: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "Close_lag_30: 0.01%\n",
      "SMA_50_lag_40: 0.01%\n",
      "EMA_26_MACD_lag_20: 0.00%\n",
      "SMA_50_lag_90: 0.00%\n",
      "SMA_20_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_30: 0.00%\n",
      "SMA_20_lag_40: 0.00%\n",
      "EMA_50_lag_75: 0.00%\n",
      "Volume_lag_90: 0.00%\n",
      "EMA_20_lag_30: 0.00%\n",
      "SMA_5_lag_30: 0.00%\n",
      "SMA_5_lag_40: 0.00%\n",
      "Close_lag_40: 0.00%\n",
      "EMA_5_lag_90: 0.00%\n",
      "SMA_50_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_90: 0.00%\n",
      "SMA_50_lag_25: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "Close_lag_90: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "SMA_20_lag_60: 0.00%\n",
      "Close_lag_75: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "EMA_5_lag_75: 0.00%\n",
      "Volume_lag_15: 0.00%\n",
      "SMA_50_lag_7: 0.00%\n",
      "Close_lag_25: 0.00%\n",
      "SMA_50_lag_75: 0.00%\n",
      "EMA_26_MACD_lag_90: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "Close_lag_50: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "SMA_5_lag_60: 0.00%\n",
      "EMA_20_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_30: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_25: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "EMA_20_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "EMA_20_lag_25: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "%K: 0.00%\n",
      "EMA_26_MACD_lag_60: 0.00%\n",
      "EMA_12_MACD_lag_25: 0.00%\n",
      "Volume_lag_60: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "EMA_20_lag_75: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "SMA_20: 0.00%\n",
      "EMA_50_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_75: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "SMA_5: 0.00%\n",
      "Volume_lag_50: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n",
      "EMA_50_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_40: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "EMA_26_MACD_lag_1: 0.00%\n",
      "RSI: 0.00%\n",
      "SMA_50_lag_3: 0.00%\n",
      "EMA_20_lag_90: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_75: 0.00%\n",
      "SMA_50_lag_1: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "%D: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_3: 0.00%\n",
      "SMA_20_lag_75: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_7: 0.00%\n",
      "SMA_5_lag_75: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_3_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a9ebb9a-2e8c-4b94-a240-f74295bb8afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with more than 1% contribution:\n",
      "Fib_30_Low_Min: 33.56%\n",
      "30_day_Fib_38: 19.87%\n",
      "5_day-Fib_61: 12.35%\n",
      "EMA_5: 5.42%\n",
      "5_day-Fib_23: 5.20%\n",
      "EMA_12_MACD: 4.18%\n",
      "Volume: 2.75%\n",
      "High: 2.65%\n",
      "Low: 2.18%\n",
      "30_day_Fib_61: 1.74%\n",
      "ATR_Prev_Close: 1.21%\n",
      "Fib_5_Low_Min: 1.01%\n",
      "List of important features:\n",
      "['Fib_30_Low_Min', '30_day_Fib_38', '5_day-Fib_61', 'EMA_5', '5_day-Fib_23', 'EMA_12_MACD', 'Volume', 'High', 'Low', '30_day_Fib_61', 'ATR_Prev_Close', 'Fib_5_Low_Min']\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance from the baseline model (1-week prediction)\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_3_month.feature_importances_))\n",
    "\n",
    "# Filter features with importance greater than 1%\n",
    "important_features = {feature: importance for feature, importance in feature_importance.items() if importance > 0.01}\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_important_features = sorted(important_features.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the feature names (keys) into a list\n",
    "important_feature_names = [feature for feature, importance in sorted_important_features]\n",
    "\n",
    "# Print the sorted important features (optional)\n",
    "print(\"Features with more than 1% contribution:\")\n",
    "for feature in sorted_important_features:\n",
    "    print(f\"{feature[0]}: {feature[1] * 100:.2f}%\")\n",
    "\n",
    "# The list of important features that you can use to create a new dataframe\n",
    "print(\"List of important features:\")\n",
    "print(important_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "df03ff90-2e8c-489c-8482-f189fd3cbc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Fib_30_Low_Min</th>\n",
       "      <th>30_day_Fib_38</th>\n",
       "      <th>5_day-Fib_61</th>\n",
       "      <th>5_day-Fib_23</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>EMA_12_MACD</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>30_day_Fib_61</th>\n",
       "      <th>ATR_Prev_Close</th>\n",
       "      <th>Fib_5_Low_Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171141</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>50.058052</td>\n",
       "      <td>49.918547</td>\n",
       "      <td>50.694471</td>\n",
       "      <td>50.398164</td>\n",
       "      <td>50.877780</td>\n",
       "      <td>50.058052</td>\n",
       "      <td>1889800.0</td>\n",
       "      <td>50.058052</td>\n",
       "      <td>51.174088</td>\n",
       "      <td>49.918547</td>\n",
       "      <td>50.398164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.918547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171142</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>47.580647</td>\n",
       "      <td>47.234290</td>\n",
       "      <td>49.669085</td>\n",
       "      <td>48.739293</td>\n",
       "      <td>50.244295</td>\n",
       "      <td>49.232250</td>\n",
       "      <td>3747400.0</td>\n",
       "      <td>49.676913</td>\n",
       "      <td>49.028604</td>\n",
       "      <td>47.234290</td>\n",
       "      <td>48.739293</td>\n",
       "      <td>50.058052</td>\n",
       "      <td>47.234290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171143</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>47.210232</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>49.501860</td>\n",
       "      <td>48.468756</td>\n",
       "      <td>50.140984</td>\n",
       "      <td>48.558244</td>\n",
       "      <td>3157800.0</td>\n",
       "      <td>49.297423</td>\n",
       "      <td>47.460380</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>48.468756</td>\n",
       "      <td>47.580647</td>\n",
       "      <td>46.796529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171144</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>48.759216</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>49.501860</td>\n",
       "      <td>48.468756</td>\n",
       "      <td>50.140984</td>\n",
       "      <td>48.625235</td>\n",
       "      <td>1900000.0</td>\n",
       "      <td>49.214622</td>\n",
       "      <td>49.153677</td>\n",
       "      <td>48.427291</td>\n",
       "      <td>48.468756</td>\n",
       "      <td>47.210232</td>\n",
       "      <td>46.796529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171145</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>49.629917</td>\n",
       "      <td>46.796529</td>\n",
       "      <td>49.501860</td>\n",
       "      <td>48.468756</td>\n",
       "      <td>50.140984</td>\n",
       "      <td>48.960129</td>\n",
       "      <td>2010600.0</td>\n",
       "      <td>49.278514</td>\n",
       "      <td>49.716506</td>\n",
       "      <td>48.855427</td>\n",
       "      <td>48.468756</td>\n",
       "      <td>48.759216</td>\n",
       "      <td>46.796529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Symbol        Date      Close  Fib_30_Low_Min  30_day_Fib_38  \\\n",
       "171141      A  2022-02-10  50.058052       49.918547      50.694471   \n",
       "171142      A  2022-02-11  47.580647       47.234290      49.669085   \n",
       "171143      A  2022-02-14  47.210232       46.796529      49.501860   \n",
       "171144      A  2022-02-15  48.759216       46.796529      49.501860   \n",
       "171145      A  2022-02-16  49.629917       46.796529      49.501860   \n",
       "\n",
       "        5_day-Fib_61  5_day-Fib_23      EMA_5     Volume  EMA_12_MACD  \\\n",
       "171141     50.398164     50.877780  50.058052  1889800.0    50.058052   \n",
       "171142     48.739293     50.244295  49.232250  3747400.0    49.676913   \n",
       "171143     48.468756     50.140984  48.558244  3157800.0    49.297423   \n",
       "171144     48.468756     50.140984  48.625235  1900000.0    49.214622   \n",
       "171145     48.468756     50.140984  48.960129  2010600.0    49.278514   \n",
       "\n",
       "             High        Low  30_day_Fib_61  ATR_Prev_Close  Fib_5_Low_Min  \n",
       "171141  51.174088  49.918547      50.398164             NaN      49.918547  \n",
       "171142  49.028604  47.234290      48.739293       50.058052      47.234290  \n",
       "171143  47.460380  46.796529      48.468756       47.580647      46.796529  \n",
       "171144  49.153677  48.427291      48.468756       47.210232      46.796529  \n",
       "171145  49.716506  48.855427      48.468756       48.759216      46.796529  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = ['Symbol', 'Date', 'Close', 'Fib_30_Low_Min', '30_day_Fib_38',\n",
    "                      '5_day-Fib_61', '5_day-Fib_23', 'EMA_5', 'Volume',\n",
    "                      'EMA_12_MACD', 'High', 'Low', '30_day_Fib_61',\n",
    "                      'ATR_Prev_Close', 'Fib_5_Low_Min']\n",
    "df_important_feat_3_month = df_stock_data_3_month[important_features]\n",
    "df_important_feat_3_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b907f0d3-cb76-4323-a038-1468b2b071ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3390042000.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3390042000.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (664084, 16)\n",
      "Testing data shape: (340665, 16)\n",
      "X_train shape: (664084, 12), y_train shape: (664084,)\n",
      "X_test shape: (340665, 12), y_test shape: (340665,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 38805.08580927107\n"
     ]
    }
   ],
   "source": [
    "# 3 month prediction with only important featuers\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_important_feat_3_month = df_important_feat_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_important_feat_3_month[df_important_feat_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_important_feat_3_month[df_important_feat_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_if_3_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_if_3_month.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_if_3_month.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52695914-d9ee-493e-8795-ad01a148d370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 38805.08580927107\n",
      "Mean Absolute Error on unseen data: 29.90465695916755\n",
      "Root Mean Squared Error on unseen data: 196.99006525525868\n",
      "R-squared on unseen data: 0.8324614075998154\n",
      "Median Absolute Error on unseen data: 6.2861328125\n",
      "Durbin-Watson Statistic on unseen data: 0.040873685600318704\n",
      "MAPE on unseen data: 13.25%\n",
      "Fib_30_Low_Min: 41.10%\n",
      "High: 16.47%\n",
      "Low: 11.00%\n",
      "30_day_Fib_38: 8.28%\n",
      "EMA_5: 4.93%\n",
      "ATR_Prev_Close: 3.57%\n",
      "5_day-Fib_23: 3.51%\n",
      "Volume: 2.92%\n",
      "30_day_Fib_61: 2.84%\n",
      "5_day-Fib_61: 2.61%\n",
      "EMA_12_MACD: 1.87%\n",
      "Fib_5_Low_Min: 0.91%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_if_3_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8314d71a-686c-4902-8101-1bc25fa85b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1211490279.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1211490279.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (664084, 195)\n",
      "Testing data shape: (340665, 195)\n",
      "X_train shape: (664084, 191), y_train shape: (664084,)\n",
      "X_test shape: (340665, 191), y_test shape: (340665,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 30689.514705268604\n"
     ]
    }
   ],
   "source": [
    "# learning rate = 0.1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_tf_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_tf_1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_3_month_tf_1.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "da607ba9-e76b-48e3-94b4-700d86cc9a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 30689.514705268604\n",
      "Mean Absolute Error on unseen data: 29.242334413401167\n",
      "Root Mean Squared Error on unseen data: 175.18423075513562\n",
      "R-squared on unseen data: 0.867499891111256\n",
      "Median Absolute Error on unseen data: 6.841819763183594\n",
      "Durbin-Watson Statistic on unseen data: 0.018820913363933614\n",
      "MAPE on unseen data: 14.94%\n",
      "Fib_30_Low_Min: 30.03%\n",
      "30_day_Fib_38: 23.30%\n",
      "5_day-Fib_61: 11.28%\n",
      "EMA_5: 8.27%\n",
      "5_day-Fib_23: 6.93%\n",
      "High: 3.61%\n",
      "Volume: 3.11%\n",
      "Low: 2.08%\n",
      "EMA_12_MACD: 2.02%\n",
      "30_day_Fib_61: 1.63%\n",
      "Fib_5_Low_Min: 0.85%\n",
      "ATR_Prev_Close: 0.67%\n",
      "EMA_26_MACD: 0.66%\n",
      "VWAP: 0.52%\n",
      "Upper_Band: 0.51%\n",
      "Lower_Band: 0.45%\n",
      "Std_Dev: 0.35%\n",
      "EMA_20: 0.33%\n",
      "Fib_30_High_Max: 0.32%\n",
      "Cumulative_Price_Volume: 0.24%\n",
      "30_day_Fib_50: 0.23%\n",
      "Fib_5_High_Max: 0.20%\n",
      "Fib_10_Low_Min: 0.16%\n",
      "SMA_5_lag_90: 0.16%\n",
      "ATR_True_Range: 0.14%\n",
      "SMA_50: 0.13%\n",
      "Close_lag_1: 0.11%\n",
      "EMA_50_lag_7: 0.09%\n",
      "EMA_50_lag_15: 0.07%\n",
      "ATR_High_Low: 0.07%\n",
      "10_day_Fib_61: 0.07%\n",
      "Volume_lag_25: 0.06%\n",
      "EMA_50_lag_60: 0.06%\n",
      "EMA_50_lag_25: 0.06%\n",
      "EMA_50_lag_20: 0.06%\n",
      "EMA_50: 0.05%\n",
      "SMA_20_lag_90: 0.05%\n",
      "Volume_lag_20: 0.05%\n",
      "Volume_lag_1: 0.04%\n",
      "Volume_lag_30: 0.04%\n",
      "Cumulative_Volume: 0.04%\n",
      "EMA_12_MACD_lag_60: 0.04%\n",
      "EMA_20_lag_60: 0.04%\n",
      "10_day_Fib_38: 0.04%\n",
      "5_day-Fib_50: 0.04%\n",
      "Fib_10_High_Max: 0.03%\n",
      "EMA_50_lag_10: 0.03%\n",
      "EMA_5_lag_40: 0.03%\n",
      "Volume_lag_3: 0.03%\n",
      "ATR: 0.03%\n",
      "30_day_Fib_23: 0.02%\n",
      "EMA_50_lag_30: 0.02%\n",
      "EMA_50_lag_50: 0.02%\n",
      "EMA_20_lag_50: 0.02%\n",
      "SMA_5_lag_25: 0.02%\n",
      "EMA_26_MACD_lag_50: 0.01%\n",
      "Close_lag_60: 0.01%\n",
      "SMA_50_lag_60: 0.01%\n",
      "SMA_50_lag_10: 0.01%\n",
      "SMA_50_lag_50: 0.01%\n",
      "EMA_5_lag_50: 0.01%\n",
      "EMA_50_lag_1: 0.01%\n",
      "EMA_50_lag_75: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "EMA_50_lag_90: 0.01%\n",
      "SMA_20_lag_50: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "SMA_5_lag_40: 0.01%\n",
      "Volume_lag_40: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "SMA_5_lag_50: 0.01%\n",
      "EMA_20_lag_40: 0.01%\n",
      "EMA_50_lag_3: 0.01%\n",
      "SMA_50_lag_30: 0.01%\n",
      "EMA_12_MACD_lag_50: 0.01%\n",
      "MACD: 0.01%\n",
      "SMA_50_lag_7: 0.01%\n",
      "Close_lag_90: 0.01%\n",
      "SMA_20_lag_25: 0.01%\n",
      "EMA_5_lag_1: 0.01%\n",
      "EMA_50_lag_40: 0.01%\n",
      "SMA_50_lag_90: 0.01%\n",
      "SMA_50_lag_40: 0.01%\n",
      "SMA_20_lag_30: 0.01%\n",
      "SMA_50_lag_15: 0.01%\n",
      "SMA_5_lag_30: 0.01%\n",
      "EMA_26_MACD_lag_40: 0.00%\n",
      "EMA_26_MACD_lag_30: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_60: 0.00%\n",
      "EMA_12_MACD_lag_25: 0.00%\n",
      "Volume_lag_15: 0.00%\n",
      "SMA_20_lag_40: 0.00%\n",
      "EMA_26_MACD_lag_90: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "SMA_50_lag_20: 0.00%\n",
      "SMA_20_lag_20: 0.00%\n",
      "EMA_20_lag_25: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "EMA_20_lag_90: 0.00%\n",
      "SMA_50_lag_3: 0.00%\n",
      "EMA_50_lag_5: 0.00%\n",
      "Volume_lag_90: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "Volume_lag_7: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "Volume_lag_75: 0.00%\n",
      "SMA_50_lag_1: 0.00%\n",
      "SMA_5_lag_60: 0.00%\n",
      "SMA_50_lag_25: 0.00%\n",
      "EMA_26_MACD_lag_20: 0.00%\n",
      "Close_lag_50: 0.00%\n",
      "EMA_20_lag_30: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "SMA_50_lag_75: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "EMA_5_lag_60: 0.00%\n",
      "EMA_5_lag_90: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "EMA_26_MACD_lag_25: 0.00%\n",
      "Close_lag_75: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_90: 0.00%\n",
      "EMA_5_lag_75: 0.00%\n",
      "Volume_lag_60: 0.00%\n",
      "%K: 0.00%\n",
      "SMA_20_lag_15: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "SMA_20_lag_60: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "Close_lag_25: 0.00%\n",
      "Close_lag_30: 0.00%\n",
      "EMA_26_MACD_lag_75: 0.00%\n",
      "SMA_50_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "RSI: 0.00%\n",
      "%D: 0.00%\n",
      "EMA_12_MACD_lag_40: 0.00%\n",
      "EMA_20_lag_20: 0.00%\n",
      "SMA_20_lag_75: 0.00%\n",
      "SMA_5: 0.00%\n",
      "EMA_26_MACD_lag_7: 0.00%\n",
      "EMA_26_MACD_lag_15: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_20_lag_75: 0.00%\n",
      "SMA_20: 0.00%\n",
      "Volume_lag_50: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "EMA_20_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_3: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "SMA_5_lag_75: 0.00%\n",
      "EMA_12_MACD_lag_75: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "EMA_12_MACD_lag_20: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "EMA_26_MACD_lag_1: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "EMA_12_MACD_lag_30: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "Close_lag_40: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_3_month_tf_1.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4c086d1f-6b98-4cc0-94b4-d7a2a347ac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1373129184.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1373129184.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (664084, 195)\n",
      "Testing data shape: (340665, 195)\n",
      "X_train shape: (664084, 191), y_train shape: (664084,)\n",
      "X_test shape: (340665, 191), y_test shape: (340665,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 29118.891815886836\n"
     ]
    }
   ],
   "source": [
    "# learning rate = 0.01\n",
    "# this is the best one again\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_tf_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_tf_01.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_3_month_tf_01.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e84e4f7c-a236-4bdf-b030-16da772eb9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 29118.891815886836\n",
      "Mean Absolute Error on unseen data: 26.64136862717554\n",
      "Root Mean Squared Error on unseen data: 170.64258500118555\n",
      "R-squared on unseen data: 0.8742809596900469\n",
      "Median Absolute Error on unseen data: 6.175811767578111\n",
      "Durbin-Watson Statistic on unseen data: 0.018949752249716446\n",
      "MAPE on unseen data: 14.00%\n",
      "Fib_30_Low_Min: 37.64%\n",
      "5_day-Fib_61: 21.52%\n",
      "30_day_Fib_38: 12.54%\n",
      "30_day_Fib_61: 4.24%\n",
      "EMA_5: 3.04%\n",
      "Volume: 2.85%\n",
      "5_day-Fib_23: 2.32%\n",
      "High: 2.11%\n",
      "EMA_12_MACD: 2.02%\n",
      "30_day_Fib_50: 1.62%\n",
      "Fib_5_Low_Min: 1.14%\n",
      "Low: 1.12%\n",
      "VWAP: 0.69%\n",
      "Upper_Band: 0.52%\n",
      "ATR_Prev_Close: 0.47%\n",
      "EMA_26_MACD: 0.43%\n",
      "Lower_Band: 0.41%\n",
      "10_day_Fib_61: 0.35%\n",
      "Close_lag_1: 0.34%\n",
      "Std_Dev: 0.34%\n",
      "Fib_30_High_Max: 0.34%\n",
      "EMA_50_lag_7: 0.29%\n",
      "Fib_10_Low_Min: 0.27%\n",
      "SMA_5_lag_90: 0.25%\n",
      "10_day_Fib_50: 0.19%\n",
      "EMA_20: 0.18%\n",
      "SMA_50: 0.17%\n",
      "Fib_5_High_Max: 0.16%\n",
      "EMA_50_lag_20: 0.15%\n",
      "Fib_10_High_Max: 0.15%\n",
      "ATR_True_Range: 0.14%\n",
      "30_day_Fib_23: 0.13%\n",
      "Cumulative_Price_Volume: 0.13%\n",
      "ATR_High_Low: 0.09%\n",
      "EMA_50: 0.08%\n",
      "EMA_26_MACD_lag_15: 0.08%\n",
      "Volume_lag_30: 0.07%\n",
      "EMA_50_lag_15: 0.07%\n",
      "EMA_50_lag_25: 0.06%\n",
      "Volume_lag_20: 0.06%\n",
      "EMA_50_lag_60: 0.06%\n",
      "EMA_12_MACD_lag_60: 0.05%\n",
      "SMA_20_lag_90: 0.04%\n",
      "Volume_lag_25: 0.04%\n",
      "SMA_5_lag_25: 0.04%\n",
      "EMA_12_MACD_lag_20: 0.04%\n",
      "Cumulative_Volume: 0.03%\n",
      "Volume_lag_1: 0.03%\n",
      "SMA_50_lag_7: 0.03%\n",
      "EMA_5_lag_40: 0.03%\n",
      "EMA_50_lag_10: 0.03%\n",
      "SMA_5: 0.03%\n",
      "EMA_20_lag_60: 0.02%\n",
      "SMA_20_lag_15: 0.02%\n",
      "Volume_lag_3: 0.02%\n",
      "SMA_5_lag_50: 0.02%\n",
      "EMA_50_lag_30: 0.02%\n",
      "10_day_Fib_38: 0.02%\n",
      "ATR: 0.02%\n",
      "SMA_5_lag_40: 0.02%\n",
      "SMA_50_lag_10: 0.02%\n",
      "EMA_50_lag_3: 0.02%\n",
      "EMA_26_MACD_lag_50: 0.02%\n",
      "EMA_50_lag_90: 0.02%\n",
      "EMA_50_lag_50: 0.02%\n",
      "SMA_50_lag_50: 0.02%\n",
      "EMA_5_lag_50: 0.02%\n",
      "SMA_20: 0.01%\n",
      "SMA_50_lag_20: 0.01%\n",
      "Close_lag_60: 0.01%\n",
      "EMA_5_lag_25: 0.01%\n",
      "SMA_50_lag_15: 0.01%\n",
      "SMA_50_lag_60: 0.01%\n",
      "EMA_50_lag_75: 0.01%\n",
      "EMA_12_MACD_lag_40: 0.01%\n",
      "EMA_12_MACD_lag_50: 0.01%\n",
      "EMA_5_lag_60: 0.01%\n",
      "EMA_20_lag_25: 0.01%\n",
      "SMA_20_lag_30: 0.01%\n",
      "EMA_20_lag_50: 0.01%\n",
      "Volume_lag_40: 0.01%\n",
      "EMA_20_lag_40: 0.01%\n",
      "SMA_20_lag_20: 0.01%\n",
      "Close_lag_90: 0.01%\n",
      "SMA_50_lag_30: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "EMA_50_lag_40: 0.01%\n",
      "SMA_20_lag_40: 0.01%\n",
      "SMA_20_lag_50: 0.01%\n",
      "MACD: 0.01%\n",
      "EMA_20_lag_15: 0.01%\n",
      "5_day-Fib_50: 0.01%\n",
      "EMA_26_MACD_lag_20: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "Close_lag_50: 0.01%\n",
      "SMA_20_lag_25: 0.01%\n",
      "EMA_26_MACD_lag_75: 0.01%\n",
      "SMA_50_lag_90: 0.00%\n",
      "Volume_lag_5: 0.00%\n",
      "SMA_5_lag_60: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "SMA_5_lag_30: 0.00%\n",
      "EMA_20_lag_10: 0.00%\n",
      "EMA_20_lag_90: 0.00%\n",
      "EMA_5_lag_75: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "Volume_lag_15: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "Volume_lag_7: 0.00%\n",
      "EMA_50_lag_1: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "SMA_20_lag_60: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "Close_lag_75: 0.00%\n",
      "EMA_12_MACD_lag_90: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "SMA_50_lag_1: 0.00%\n",
      "EMA_5_lag_90: 0.00%\n",
      "EMA_26_MACD_lag_30: 0.00%\n",
      "Volume_lag_75: 0.00%\n",
      "SMA_50_lag_40: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "Close_lag_25: 0.00%\n",
      "EMA_26_MACD_lag_60: 0.00%\n",
      "Volume_lag_90: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "EMA_26_MACD_lag_90: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "Volume_lag_60: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "SMA_20_lag_75: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "SMA_50_lag_75: 0.00%\n",
      "SMA_50_lag_25: 0.00%\n",
      "%K: 0.00%\n",
      "EMA_12_MACD_lag_30: 0.00%\n",
      "RSI: 0.00%\n",
      "EMA_20_lag_75: 0.00%\n",
      "EMA_12_MACD_lag_25: 0.00%\n",
      "EMA_26_MACD_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "Close_lag_30: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "Volume_lag_50: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "EMA_50_lag_5: 0.00%\n",
      "SMA_50_lag_5: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_3: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_25: 0.00%\n",
      "EMA_26_MACD_lag_40: 0.00%\n",
      "%D: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "EMA_20_lag_20: 0.00%\n",
      "SMA_50_lag_3: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "Close_lag_40: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "SMA_5_lag_75: 0.00%\n",
      "EMA_12_MACD_lag_75: 0.00%\n",
      "EMA_26_MACD_lag_7: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "EMA_20_lag_30: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "5_day-Fib_100: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_3_month_tf_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "348432af-38f0-446c-a678-3764e9a2fc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1537154618.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1537154618.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (664084, 195)\n",
      "Testing data shape: (340665, 195)\n",
      "X_train shape: (664084, 191), y_train shape: (664084,)\n",
      "X_test shape: (340665, 191), y_test shape: (340665,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 25855.13009374476\n"
     ]
    }
   ],
   "source": [
    "# learning rate = 0.01\n",
    "# max depth 3\n",
    "# this is actually the best one for 3 months now\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_md_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_md_3.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_3_month_md_3.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "89181266-05d0-4c3d-8f06-93fa693c9b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 25855.13009374476\n",
      "Mean Absolute Error on unseen data: 25.29591435606357\n",
      "Root Mean Squared Error on unseen data: 160.79530494931984\n",
      "R-squared on unseen data: 0.8883720519645201\n",
      "Median Absolute Error on unseen data: 6.283714294433594\n",
      "Durbin-Watson Statistic on unseen data: 0.017109218495924613\n",
      "MAPE on unseen data: 15.52%\n",
      "Fib_30_Low_Min: 27.17%\n",
      "30_day_Fib_38: 14.80%\n",
      "5_day-Fib_61: 11.17%\n",
      "Upper_Band: 8.25%\n",
      "Fib_10_Low_Min: 3.10%\n",
      "10_day_Fib_50: 2.76%\n",
      "Close_lag_1: 2.34%\n",
      "Volume: 1.87%\n",
      "EMA_5: 1.82%\n",
      "Fib_5_Low_Min: 1.69%\n",
      "Fib_5_High_Max: 1.51%\n",
      "30_day_Fib_23: 1.50%\n",
      "High: 1.48%\n",
      "EMA_50_lag_25: 1.37%\n",
      "30_day_Fib_50: 1.29%\n",
      "ATR_High_Low: 1.20%\n",
      "EMA_26_MACD: 1.11%\n",
      "EMA_50: 1.08%\n",
      "SMA_50: 1.02%\n",
      "ATR: 1.00%\n",
      "10_day_Fib_61: 0.87%\n",
      "Low: 0.86%\n",
      "EMA_12_MACD: 0.82%\n",
      "Close_lag_90: 0.76%\n",
      "5_day-Fib_23: 0.76%\n",
      "EMA_20: 0.73%\n",
      "SMA_20_lag_90: 0.60%\n",
      "EMA_12_MACD_lag_90: 0.58%\n",
      "10_day_Fib_38: 0.57%\n",
      "VWAP: 0.55%\n",
      "30_day_Fib_61: 0.54%\n",
      "Volume_lag_50: 0.53%\n",
      "ATR_True_Range: 0.39%\n",
      "ATR_Prev_Close: 0.32%\n",
      "SMA_5_lag_90: 0.31%\n",
      "EMA_50_lag_30: 0.30%\n",
      "Lower_Band: 0.28%\n",
      "Fib_30_High_Max: 0.24%\n",
      "Std_Dev: 0.23%\n",
      "Volume_lag_3: 0.16%\n",
      "Volume_lag_25: 0.15%\n",
      "SMA_50_lag_60: 0.13%\n",
      "EMA_26_MACD_lag_40: 0.13%\n",
      "Cumulative_Price_Volume: 0.13%\n",
      "EMA_50_lag_20: 0.12%\n",
      "Volume_lag_75: 0.11%\n",
      "EMA_12_MACD_lag_20: 0.10%\n",
      "SMA_50_lag_10: 0.07%\n",
      "Volume_lag_60: 0.07%\n",
      "EMA_20_lag_15: 0.06%\n",
      "EMA_50_lag_50: 0.05%\n",
      "SMA_50_lag_3: 0.05%\n",
      "EMA_5_lag_40: 0.04%\n",
      "SMA_50_lag_5: 0.04%\n",
      "EMA_50_lag_75: 0.04%\n",
      "Cumulative_Volume: 0.04%\n",
      "SMA_5_lag_60: 0.04%\n",
      "Fib_10_High_Max: 0.04%\n",
      "EMA_50_lag_60: 0.03%\n",
      "SMA_50_lag_30: 0.03%\n",
      "SMA_5_lag_25: 0.03%\n",
      "Volume_lag_90: 0.03%\n",
      "SMA_20_lag_25: 0.03%\n",
      "SMA_50_lag_7: 0.03%\n",
      "ATR_High_Close: 0.02%\n",
      "EMA_50_lag_90: 0.02%\n",
      "SMA_20_lag_40: 0.02%\n",
      "Signal_Line: 0.02%\n",
      "Volume_lag_1: 0.02%\n",
      "SMA_50_lag_20: 0.02%\n",
      "SMA_20_lag_30: 0.02%\n",
      "EMA_26_MACD_lag_50: 0.02%\n",
      "SMA_5_lag_50: 0.02%\n",
      "SMA_50_lag_15: 0.02%\n",
      "Volume_lag_40: 0.02%\n",
      "SMA_50_lag_25: 0.01%\n",
      "EMA_50_lag_15: 0.01%\n",
      "EMA_12_MACD_lag_40: 0.01%\n",
      "MACD: 0.01%\n",
      "SMA_20_lag_20: 0.01%\n",
      "EMA_12_MACD_lag_15: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "SMA_5: 0.01%\n",
      "SMA_5_lag_15: 0.01%\n",
      "EMA_26_MACD_lag_30: 0.01%\n",
      "Close_lag_20: 0.01%\n",
      "EMA_20_lag_50: 0.01%\n",
      "Close_lag_25: 0.01%\n",
      "Volume_lag_20: 0.01%\n",
      "Close_lag_40: 0.01%\n",
      "EMA_5_lag_50: 0.01%\n",
      "SMA_5_lag_40: 0.01%\n",
      "SMA_50_lag_90: 0.01%\n",
      "5_day-Fib_50: 0.01%\n",
      "SMA_50_lag_40: 0.01%\n",
      "EMA_5_lag_75: 0.01%\n",
      "SMA_20_lag_50: 0.01%\n",
      "Close_lag_10: 0.01%\n",
      "EMA_12_MACD_lag_5: 0.01%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "SMA_50_lag_75: 0.00%\n",
      "Close_lag_60: 0.00%\n",
      "Close_lag_75: 0.00%\n",
      "Close_lag_50: 0.00%\n",
      "EMA_20_lag_60: 0.00%\n",
      "Volume_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_60: 0.00%\n",
      "EMA_12_MACD_lag_50: 0.00%\n",
      "EMA_26_MACD_lag_60: 0.00%\n",
      "Volume_lag_7: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "RSI: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "Close_lag_30: 0.00%\n",
      "%D: 0.00%\n",
      "SMA_5_lag_75: 0.00%\n",
      "Volume_lag_15: 0.00%\n",
      "SMA_20: 0.00%\n",
      "EMA_12_MACD_lag_30: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "SMA_50_lag_50: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "Volume_lag_30: 0.00%\n",
      "%K: 0.00%\n",
      "SMA_20_lag_60: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "SMA_5_lag_30: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n",
      "EMA_5_lag_60: 0.00%\n",
      "EMA_5_lag_90: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "SMA_20_lag_15: 0.00%\n",
      "SMA_20_lag_75: 0.00%\n",
      "SMA_50_lag_1: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "EMA_20_lag_10: 0.00%\n",
      "EMA_20_lag_20: 0.00%\n",
      "EMA_20_lag_25: 0.00%\n",
      "EMA_20_lag_30: 0.00%\n",
      "EMA_20_lag_40: 0.00%\n",
      "EMA_20_lag_75: 0.00%\n",
      "EMA_20_lag_90: 0.00%\n",
      "EMA_50_lag_1: 0.00%\n",
      "EMA_50_lag_3: 0.00%\n",
      "EMA_50_lag_5: 0.00%\n",
      "EMA_50_lag_7: 0.00%\n",
      "EMA_50_lag_10: 0.00%\n",
      "EMA_50_lag_40: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_25: 0.00%\n",
      "EMA_12_MACD_lag_75: 0.00%\n",
      "EMA_26_MACD_lag_1: 0.00%\n",
      "EMA_26_MACD_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_7: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "EMA_26_MACD_lag_15: 0.00%\n",
      "EMA_26_MACD_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_25: 0.00%\n",
      "EMA_26_MACD_lag_75: 0.00%\n",
      "EMA_26_MACD_lag_90: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_3_month_md_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "86699263-cf02-41ab-8ce1-a26c9531fbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2146396137.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2146396137.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (664084, 195)\n",
      "Testing data shape: (340665, 195)\n",
      "X_train shape: (664084, 191), y_train shape: (664084,)\n",
      "X_test shape: (340665, 191), y_test shape: (340665,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 28832.803345344026\n"
     ]
    }
   ],
   "source": [
    "# learning rate = 0.01\n",
    "# max depth 7\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_3_month = df_stock_data_3_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_3_month[df_stock_data_3_month['Date'] <= '2023-11-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_3_month[df_stock_data_3_month['Date'] > '2024-02-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-60)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-60)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_3_month_md_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3_month_md_7.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_3_month_md_7.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2fd94b99-7fca-49a4-bcf0-716ad08f459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 28832.803345344026\n",
      "Mean Absolute Error on unseen data: 27.67476094296\n",
      "Root Mean Squared Error on unseen data: 169.80224776293164\n",
      "R-squared on unseen data: 0.8755161292214905\n",
      "Median Absolute Error on unseen data: 6.3470611572265625\n",
      "Durbin-Watson Statistic on unseen data: 0.024350243504711922\n",
      "MAPE on unseen data: 13.86%\n",
      "Fib_30_Low_Min: 56.87%\n",
      "5_day-Fib_61: 10.13%\n",
      "5_day-Fib_23: 6.61%\n",
      "30_day_Fib_61: 4.01%\n",
      "EMA_5: 3.16%\n",
      "High: 2.72%\n",
      "Volume: 2.34%\n",
      "30_day_Fib_38: 2.34%\n",
      "EMA_12_MACD: 1.84%\n",
      "Low: 1.22%\n",
      "30_day_Fib_50: 1.08%\n",
      "VWAP: 0.93%\n",
      "EMA_26_MACD: 0.82%\n",
      "Fib_10_High_Max: 0.63%\n",
      "Std_Dev: 0.57%\n",
      "Fib_5_High_Max: 0.49%\n",
      "Fib_5_Low_Min: 0.47%\n",
      "EMA_20: 0.41%\n",
      "10_day_Fib_61: 0.27%\n",
      "EMA_50_lag_20: 0.23%\n",
      "Cumulative_Price_Volume: 0.23%\n",
      "Close_lag_1: 0.20%\n",
      "EMA_50_lag_15: 0.16%\n",
      "EMA_26_MACD_lag_15: 0.14%\n",
      "ATR_Prev_Close: 0.14%\n",
      "SMA_5: 0.13%\n",
      "SMA_50: 0.10%\n",
      "EMA_50_lag_25: 0.09%\n",
      "Fib_10_Low_Min: 0.09%\n",
      "Volume_lag_25: 0.08%\n",
      "EMA_50_lag_7: 0.07%\n",
      "Volume_lag_30: 0.07%\n",
      "Fib_30_High_Max: 0.07%\n",
      "ATR_True_Range: 0.06%\n",
      "Lower_Band: 0.06%\n",
      "EMA_12_MACD_lag_60: 0.05%\n",
      "Volume_lag_20: 0.05%\n",
      "Cumulative_Volume: 0.05%\n",
      "Upper_Band: 0.04%\n",
      "EMA_20_lag_60: 0.04%\n",
      "10_day_Fib_50: 0.04%\n",
      "EMA_50: 0.03%\n",
      "30_day_Fib_23: 0.03%\n",
      "Volume_lag_3: 0.03%\n",
      "10_day_Fib_38: 0.03%\n",
      "EMA_5_lag_60: 0.03%\n",
      "ATR: 0.02%\n",
      "SMA_50_lag_60: 0.02%\n",
      "EMA_50_lag_50: 0.02%\n",
      "EMA_20_lag_50: 0.02%\n",
      "SMA_20_lag_5: 0.02%\n",
      "EMA_20_lag_15: 0.02%\n",
      "Volume_lag_60: 0.01%\n",
      "EMA_20_lag_40: 0.01%\n",
      "EMA_5_lag_90: 0.01%\n",
      "SMA_50_lag_7: 0.01%\n",
      "EMA_5_lag_50: 0.01%\n",
      "EMA_50_lag_75: 0.01%\n",
      "EMA_12_MACD_lag_90: 0.01%\n",
      "EMA_20_lag_25: 0.01%\n",
      "EMA_50_lag_60: 0.01%\n",
      "SMA_20_lag_15: 0.01%\n",
      "SMA_50_lag_10: 0.01%\n",
      "SMA_50_lag_75: 0.01%\n",
      "SMA_20: 0.01%\n",
      "EMA_50_lag_40: 0.01%\n",
      "SMA_50_lag_50: 0.01%\n",
      "SMA_50_lag_15: 0.01%\n",
      "EMA_50_lag_1: 0.01%\n",
      "EMA_26_MACD_lag_30: 0.01%\n",
      "Volume_lag_1: 0.01%\n",
      "SMA_50_lag_40: 0.01%\n",
      "SMA_50_lag_1: 0.01%\n",
      "SMA_20_lag_30: 0.01%\n",
      "SMA_50_lag_25: 0.01%\n",
      "EMA_26_MACD_lag_50: 0.01%\n",
      "EMA_26_MACD_lag_60: 0.01%\n",
      "Close_lag_50: 0.01%\n",
      "SMA_5_lag_25: 0.01%\n",
      "SMA_50_lag_90: 0.01%\n",
      "Close_lag_60: 0.01%\n",
      "SMA_5_lag_30: 0.01%\n",
      "SMA_20_lag_50: 0.01%\n",
      "EMA_12_MACD_lag_30: 0.01%\n",
      "SMA_50_lag_20: 0.01%\n",
      "SMA_20_lag_90: 0.01%\n",
      "SMA_50_lag_3: 0.01%\n",
      "EMA_20_lag_10: 0.01%\n",
      "SMA_20_lag_25: 0.01%\n",
      "ATR_High_Low: 0.01%\n",
      "EMA_50_lag_90: 0.01%\n",
      "SMA_20_lag_75: 0.01%\n",
      "EMA_26_MACD_lag_75: 0.01%\n",
      "EMA_50_lag_3: 0.01%\n",
      "EMA_50_lag_30: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "SMA_20_lag_7: 0.01%\n",
      "EMA_5_lag_75: 0.01%\n",
      "EMA_12_MACD_lag_40: 0.01%\n",
      "SMA_5_lag_60: 0.00%\n",
      "SMA_5_lag_40: 0.00%\n",
      "SMA_20_lag_40: 0.00%\n",
      "EMA_26_MACD_lag_25: 0.00%\n",
      "SMA_50_lag_30: 0.00%\n",
      "5_day-Fib_50: 0.00%\n",
      "Volume_lag_75: 0.00%\n",
      "SMA_20_lag_60: 0.00%\n",
      "Close_lag_30: 0.00%\n",
      "Signal_Line: 0.00%\n",
      "EMA_26_MACD_lag_20: 0.00%\n",
      "EMA_50_lag_5: 0.00%\n",
      "Volume_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_50: 0.00%\n",
      "Volume_lag_7: 0.00%\n",
      "SMA_50_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_90: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "Close_lag_75: 0.00%\n",
      "EMA_12_MACD_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "Volume_lag_90: 0.00%\n",
      "Close_lag_25: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_25: 0.00%\n",
      "MACD: 0.00%\n",
      "EMA_20_lag_90: 0.00%\n",
      "SMA_5_lag_90: 0.00%\n",
      "Volume_lag_40: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "EMA_20_lag_30: 0.00%\n",
      "EMA_26_MACD_lag_40: 0.00%\n",
      "EMA_50_lag_10: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_75: 0.00%\n",
      "Volume_lag_15: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "EMA_20_lag_75: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n",
      "SMA_20_lag_20: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "SMA_5_lag_75: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "Volume_lag_50: 0.00%\n",
      "%K: 0.00%\n",
      "Close_lag_40: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "EMA_20_lag_20: 0.00%\n",
      "Close_lag_90: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "SMA_5_lag_50: 0.00%\n",
      "EMA_26_MACD_lag_7: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "%D: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "RSI: 0.00%\n",
      "EMA_26_MACD_lag_3: 0.00%\n",
      "EMA_5_lag_40: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "EMA_26_MACD_lag_1: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_3_month_md_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2757dd-30da-423a-80a6-7b9bc4eeec73",
   "metadata": {},
   "source": [
    "best model: learning_rate = 0.01 and max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0f0a7d87-e136-4beb-985c-b1989d1d6e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNUAAALRCAYAAAB8l5U+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAADasklEQVR4nOzdd3gU1fv38c+mh5JQQocQpEovgnQCiFSVDiLSsdIUFQS+QBABwQo2pAVQpCpSBSmho7SAIgJCKNJCMQlIS8I8f/Ds/rLZTdnNhiTwfl3XXtmdOWfmni2T2XtPMRmGYQgAAAAAAABAqrlldAAAAAAAAABAVkNSDQAAAAAAAHAQSTUAAAAAAADAQSTVAAAAAAAAAAeRVAMAAAAAAAAcRFINAAAAAAAAcBBJNQAAAAAAAMBBJNUAAAAAAAAAB5FUAwAAAAAAABxEUg0AkKWFhobKZDJZ3eC8oKAgq+dy7NixVuvDwsJsnu9Tp05lSKySFBwcbBVLr169MiwW2Er8XgkNDXX5PjgHAACAjOKR0QEAyNyCg4O1ZcsWu+vc3NyULVs25cuXT+XKlVPTpk3Vu3dv5cmT5wFHmTqhoaFWX/6rVq2qtm3bOr292NhYLVmyRHv37tX+/ft17tw5Xb16VTExMfL19VX+/PlVvnx5NW/eXD179lTOnDmd2s+pU6dUokQJm+Xu7u6KiIhQsWLF7NY7c+aMHnvsMcXHx9usi4iIUFBQkFPxPAiJEzlt27ZV1apVMySWhHr16qW5c+cmud7Dw0P+/v4qXbq06tevr969e6t8+fIPMMKsLzw8XMuXL7dalvj98DB7FD/vWUFSr4sjevbsmS5JxfTg6PEOHTpUH374ocP7CQ0NVe/evW2Wly1bVkeOHEkyQfrdd9+pe/fuNsuLFy+eoUl+e9LjnBYUFKTTp09bHmfG484qli9frvDwcMvjoKCgLPPjjKuva8PCwtS4cWOH6nzyyScaMmSI0/tML5n1OhIPJ5JqAJx279493bhxQzdu3FBERITWrl2r9957T8uWLVPTpk0zOjwboaGhVgnCnj17puni4+rVq3rhhRfsrjM/LydPntSqVas0btw4LV26VA0bNnR6f4nFx8frq6++0oQJE+yu/+KLL+x+wc4KQkJCrB4HBQVliYuhuLg4Xb16VVevXtXu3bv1ySef6O2339bEiRMzOrQsIzw83Ob1f5SSakl5mD/vQGJHjx7VL7/8oqefftru+qlTpz7giJzHOS1zW758udWPZY0aNcpSSTVXXtc+TLLqdSSyJrp/AnCp6OhodenSRdHR0RkdSqZy+fJltWvXTteuXXPpdmfMmKHbt2/bLL9165Zmzpzp0n3BcfHx8Zo0aVKSiZCsqHbt2oqIiLC6FS1aNMPiWbhwoVUszrSWySr4vONRMm3aNLvL9+zZo99+++0BRwMAgH20VAPgsIiICEn3EwZHjhzR4MGDdfLkScv6q1ev6ueff1aXLl0yKsQHwmQyqVKlSmrevLnq1KmjQoUKKW/evIqKitLmzZs1fvx43bhxw1L+2rVrWrlypXr27OmyGK5cuaKFCxfa/Kr63XffuTyBB/u2bdumokWLyjAMnTt3Tl9//bW+++47qzLvv/++hgwZomzZsmVQlK7j4+OTqboSFixYMKNDeGD4vGecokWLWv73JfTPP/+oQYMGVss6dOhgN7mbI0eOdIsvvSV1TGb+/v4u3+eaNWt08uRJPfbYY1bLP/vsM5fvC4B9gwcPTrZ7Z2Yd8gV4oAwASEajRo0MSVa3xBYsWGBTZtKkSUlu89KlS8Z7771nNGzY0MiXL5/h6elp5MqVy6hWrZrxzjvvGGfPnk2ybmxsrDFnzhyjTZs2RmBgoOHr62t4enoaBQsWNCpVqmR07drV+Pjjj40DBw5Y6hQvXtwmvqRuERERaXm6rHzzzTcOPS9JiYiIsNmOyWSy3K9Ro4ZNnSpVqtgtm9Jx3r171/j222+N9u3bW55fHx8fo2jRosYzzzxjzJw507h9+7bdups3b7a7n/PnzxuDBg0yHnvsMcPb29vImzev8cwzzxi7d+9O8TiTuhUvXtxSb86cOXbfo6dOnTJeeeUVIzAw0PDy8jIKFChgdO3a1Thy5IjDr4FZz549U/Vc1q9f36bcL7/8Ylk/ZswYu8fzyy+/GG3atDHy5ctnuLm5GT179rTZ9u+//24MGjTIqFq1qpE7d27D09PTyJcvnxEcHGx8/PHHxo0bN5I9hqtXrxpvvfWWUbJkScPb29soUKCA0blzZ2Pfvn2GYdh+XsaMGWNVP6nX2Z7Y2Fhj4cKFRpcuXYySJUsaOXPmNLy9vY2iRYsa9evXN8aMGWOcOHHC7nOS3C1hTInPUfaeM7P9+/cbr732mlG5cmUjV65choeHh5E3b16jVq1axvDhw41Tp04lWdfe8xIXF2d8/fXXRp06dQw/Pz8jW7ZsRtWqVY3PPvvMiI+PT/Z1SEpW+bybnT171nj55ZeNYsWKGV5eXkaRIkWM3r17G3///bdhGIZNLHPmzElyW2vXrjW6d+9ulCpVysiRI4flvdKuXTtj8eLFxr179+zWS+ockNLzunnz5mSPLTXsbTe592B6nGP/+ecf4/XXXzdKlChh9Zk+ePCgy44ruWNKC3uvXcL38NChQ63KX7x40fDy8kry/Z7wf0Ni165dMyZPnmw0bdrUKFCggOHl5WXkyJHDKFWqlNG9e3fj559/TjbWPXv2GC+99JJRsWJFI2fOnIa7u7uRO3duo3Tp0kazZs2MESNGGD/++KMRGxtrGIbz57TUSHw+snfc9s6NsbGxxieffGJUrVrV8PX1NfLnz2907NjR6r1y5coV4+2337b8zy5YsKDxwgsvGMeOHbMbS1L/z3bv3m107NjR8lyXKFHCGDx4sHH58uVkj+2///4zvvzyS6NVq1ZG4cKFDW9vbyNbtmxGUFCQ0bFjR2PRokVGXFyc3bpJnQv27NljdOnSxShUqJDh7u5uNGrUyO71bVK3hOeKX375xRgzZozxzDPPGBUqVDAKFSpkeHl5GT4+PkahQoWMpk2bGh988EGyx2nvvHjr1i3jgw8+MKpWrWpkz57dyJEjh1GnTh3j22+/tamfXte19s4xjr43zZw9n1+/ft2YPXu2MWDAAKNBgwZGqVKljDx58hgeHh6Gn5+fUaZMGaNLly7GDz/8YLMNZ68jk3oPp7TtxP9DMura7tq1a8b7779vNGjQwMifP7/h5eVl+Pr6GoGBgUbNmjWN/v37GzNmzEj2+xXShqQagGSlJqn2/fff25SZPn263e3NmjXLyJYtW7L/6Ly9vY2ZM2fa1L1165ZRr169VP2zbN68uaVeRiXVpk+fbrP9xYsXO7wde//ImzdvbvV4586dlvJhYWFW61q0aJGq4/zjjz+Mxx9/PMXnqESJEsaePXts6tu7GJs5c6bh5+dndzteXl7GunXrkj3O1FwM2buIXrlypZE9e3a7df38/Izw8HCHXwfDSH1S7a233rIpt2DBAst6exde7733nk2dhBded+7cMQYOHJjic1OkSBGbhKXZsWPHjKJFi9qt5+HhYcyZM8dlSbX9+/cb5cqVSzHeTz75xO5zktzN0aTarVu3jP79+6e4XQ8PD+ODDz6w+9wlfl4GDRpkN3maXBypkVU+74ZhGDt37jT8/f3t1suePbvx888/2yy3l1Q7f/68ERwcnGIs9evXNy5cuGBTP6sk1dLjHDt//vwkXwMPDw9j4cKFLjmuggULGkFBQZZEVOnSpY0ePXqk+Tm099olfL/nzp3b+O+//yzlx44da3V8TZs2taqbVFJt2bJlRq5cuVJ87ps2bWpcunTJpv7UqVPtJqzt3czv0cyWVOvcuXOSnzNfX19jw4YNxp9//pnk/4hcuXIZhw4dstmPvf9nn376qeHm5mZ3OwULFjQOHz5s97i2bt1qFClSJMXnq1q1asbx48dt6tt7P4WGhhru7u5Wy9KSVEv4I0Zyt7x58yb5+UhcNiQkJNlzQ+L3R2ZOqqX1fH7gwIFUH1twcLARExNjqZsZk2rpfW139OhRo1ChQqk65okTJzr0WiL1SKoBSJa9i46IiAgjIiLC+Pvvv43Vq1cbpUqVslrv4eFhnD592mZb9pJMyd3mzZtnVf/DDz9Mdd0HnVS7cOGCERERYRw7dszYuXOnMWHCBCNHjhw2/1xv3brl8Lbt/SP/5ptvjMKFC1seP//885byHTp0sCwvXLiw3RZziY/z5MmTRr58+VL9PPn7+9tcFNu7GEvpS0jx4sUtvzi7Mqnm6emZbP369es7/DoYRuqTai1btrQpt3btWsv6xBdeiS/47V14devWLdXPT86cOW1en1u3bqWY5HJ3dze8vb2tljmTVDt06FCSX/QT3x5EUq1jx46p3rYk4/3337fZRuLzSGq+YG/YsMHu+yg5WeXzHhkZmeI27P2AkjipFhUVlapEk/lWuXJlm1/ss0JSLb3OsSmd67y8vJxqsebI+bhLly7GzZs3nXr+7L12q1atsnps/pHu7t27Vl8eO3XqZHNOtvdl+KeffkoywWPvVrVqVav32IULF1J8nhPeMmtSLaVzVrFixWyu5xLfGjZsaLOfxMfp4eGR4vGWLFnS5nP822+/Gb6+vql+zooVK2aTlLH3frIXz4NIqkn3E5FXrlyxec4Sl0vptXFzczOOHj2a5Ouf3O1BJtVccT53JKkmyejevbulbmZLqj2Ia7s2bdqkuj5JtfTDRAUAHFaiRAmVKFFCpUqVUuvWrfX3339b1nl4eOjLL79UYGCgVZ0LFy7YjMnQokULrV27Vn/99ZfCwsJsZiwaOHCg/v33X8vjhDMcSVK3bt20c+dOHT9+XAcPHtTy5cs1atQoPfnkk3Jz+7/T2/bt2xUREaEnn3zSqn6HDh1cNuB6165dVaJECZUpU0Z169bViBEjrMZTq1WrljZu3CgfHx+ntp+Yp6enXnnlFcvjpUuX6uLFizp79qyWL19uWf7qq6/K09Mzxe0NHDhQly9ftlr20ksvadu2bdq9e7fNaxcdHa3XXnstxe0ahqFu3bppz5492rFjhxo1amS1/vTp09q5c6ek/xuzyN64RVOmTLF6nbZv357sfmNjYzVkyBAdPHhQGzduVIUKFazWb9++XWfPnk0xfkcYhqF//vlHo0aN0tq1a63WeXp6qnbt2knWNc/a2K1bN23btk1HjhzRypUrLTPfLV++XAsWLLCqM3DgQO3YsUN//fWXfvzxR1WqVMmy7vr161bvD+n+IPd//fWX1bIaNWpo7dq1OnDggCZPnix3d3fduXPH8YNPwDAM9enTx2aykiZNmmjFihU6evSoDh48qFmzZql+/fqW9UOGDFFERISmTJlis83En9PkxndJbOnSpVq6dKnVsgoVKmj58uU6dOiQ5s6dq3z58lmtHzNmjNV5LanjLFmypH766Sf9/vvvGjdunEwmk1WZxK+ZszLj533SpEk222jevLk2btyovXv3avjw4bp161aKsYwZM0ZHjhyxPM6ZM6c+/vhj7d+/X3/88YemT5+u3LlzW9YfOnRIH3zwQYrbzWzS6xwbGxurPn36aPv27dq5c6f69u1rtf7u3bsaPnx4muNPzqJFi1w6VmiFChXUpEkTy+PPP/9ckrRkyRJduHDBsnzQoEEpbuu///7TSy+9pHv37lmWeXt7a8qUKdq/f79++eUXNW/e3KpOeHi41Xlo586dio2NtTwOCgrS8uXLdeTIER05ckSbN2/W1KlT1aFDB6tx89LrnOYswzBUpkwZ/fzzzzp06JDNe+Xs2bP6+++/Vbt2bW3dulX79u3TU089ZVVm69at+ueff5LdT1xcnLy8vDRp0iQdOHBA69atU926da3KnDhxQl988YVVbP369bM6Z7i5uWnEiBH67bfftHXrVnXv3t0m3tS8t+Pi4vT0009rw4YN+uuvv7R+/Xo9//zzlkluOnToYFX+ySeftHl9Ev7/zp8/v3r06KH58+dr48aNOnTokI4ePart27dr7Nix8vb2tpSNiorSjBkzUozRMAzVqFFDv/zyi8LDw/X6669brb93754WLVpkefwgrmvNQkJCZDKZ7N5y5cplVdYV53OTyaQqVapo5MiRWr58uXbs2KGjR4/q0KFDWrFihZ555hmr8gsWLNC5c+ckufY60hUexLVd4u9GEyZM0IEDB3T8+HHt2bNH3333nV5//XWVLFkyPQ4RZhmY0AOQBTjyS54k49VXX7Xb7z9x8+dKlSrZjDkUFxdn8+vbtGnTLOsTt/5JqoubYRhWzcGTOhZXjhGT3PPUpEkT4/fff3d62/Z+HZszZ45x6dIlq1ZFY8eONYYPH2557O3tbVy6dMnuL7cJf7k8c+ZMsr/8mb388ss25f7880/Lenu/cNapU8dqzIvIyEibMp9//rnNvuwdb1LsHV/Xrl2tyuzZs8emzKpVqxx4Fe6z11ItNbfBgwdbbcdeC4aOHTsmud/EXZxef/11mzJ///23zTYTvu9q1apltS5XrlxGdHS01TY++ugjm2042lJtx44dNus7dOiQ5PgpUVFRVo9T0+oooZQ+102aNLFa7+fnZ1y9etWqzK5du2z2+c4771iVSXxucnNzs3r/G4ZhtG7d2qrME088kWzs9mSVz3uBAgWs1pUuXdpmnCN7XVoSfpZv375t0017yZIlNrHMnDnTqkxAQIDV+ymzt1RLz3Pss88+a7OdZ555xqqMyWQyIiMjHT4ub29vo0OHDsaMGTOM3377zfjrr7+Mn3/+2ejatavd89ymTZsc2odh2H/tIiIijOXLl1stCwsLM2rXrm15XK1aNcMwbM/JiVuYzJs3z2b7iYeXiIuLM8qXL29VJn/+/Jb32KJFi6zWvfLKK0kez82bNy1jqiV3jGnlTEs1SVZDH0RFRdm0kDKfR8zCw8NttpH4f6e9/2cff/yxzfOS+JxRsWJFy/qtW7fabGPUqFE2x5S4e7uHh4fV/zF7z/WTTz6Z5BhshmH7HmrUqFGSZVNjwIABVttr0aKFTZnEMebIkcOmRVuFChWsyti7RnD1da29c0xyN39/f0tdV53PUxIXF2fTEt5eN/fEsSZ3HZleLdWSet3MXHFtl7B1p5+fn3Hnzp0k92fvuxFcg5ZqAFzqq6++Us2aNXXx4kWr5Yl/Sfn999/l7u5u9YuXh4eHTp8+bVVu69atlvs1atSwWte6dWv16NFDEydO1LJly/TXX3/JMAxJ938dyyw2bdqkypUra9KkSS7dbv78+dW5c2fL46+//trqF9EuXboof/78KW4n8Wsj3W9BkdjLL79ssyzh62PP66+/btV6J1++fMqbN69VmYStEV1lwIABVo/LlStnUyY99mtP165d7bZUSOx///uf3eXx8fE2v6h+8cUXNr8YlypVyqau+fWJjY3V/v37rdZ17NhRfn5+VssSt1xwxubNm22WjR8/3qYVl1l6zBpoZu+569ixo81sZbVr11blypWtlqX03m7SpIkef/xxq2WJ32eufI9lps/7qVOndOnSJat1PXv2lLu7u9WylN5Pe/fu1X///We1rFOnTjbv7X79+lmVuXLlilVriNQICgqScX/YE8stODjYoW04Kz3PsX369LFZlvh5NwxDv/32W0phWilYsKDOnTunpUuXql+/fqpZs6bKli2r5s2b6/vvv9c777xjU+f77793aB/JeeaZZ1SiRAnL4wEDBmj37t2WxwMHDkzVdhI/9z4+PurRo4fVMnd3d5v3WGRkpI4dOyZJql69utX565tvvlHz5s01fPhwzZ49Wzt37tTNmzclSb6+vvLw8EhVbA9apUqVVKVKFctjf39/m3Nh8+bNrc4jZcqUsdlOas5rid+Xvr6+ev75562WHT58WNevX5eU+s9I4mVxcXHatWtXsrGMGDHC5tyUVqtXr1avXr1UpUoV5c6dW56enpbzlbllpVlKLfuk+9cJia+N0vP/SXpw5fk8Ojpan376qVq2bKkSJUooR44ccnNzs3xXSNwSPjXPcUZJz2s7yfq7UUxMjCpVqqTXXntNn376qdauXWtpxSdlru9GDxuSagAcZv5Ccu/ePZ0/f14TJ060Wn/kyBENHjzYalnCk7ojEnb1GDx4sFXz5atXr2r+/PkaMWKEOnbsqMcff1z58uXT66+/bpPUS29hYWEyDEM3b95URESEpk+frscee8yy3jAMvfvuu1q/fr1L95uw+8vFixd19epVu+uSc/78eZtl9pqJJzye5OomZC+Z5evra/U4Li4upRAdlni/ifeZXvs1y58/v9q3b6+1a9fq+++/T7FLnqenpypWrGh33dWrV53ukmn+/Fy7ds3meBN+YTXz9/e36prhjMTviWzZstl9HzwIV69e1d27d62WJdUFIvH7OzO+tzPL5z1xQk2y/36ytywhZ/8vSNb/GzK79DzHpvZ5d/R/oo+Pj82X/IRGjhxpsyw8PNyhfSTHzc3NqvvrH3/8YbkfEBBgk6BJSuLnr1ixYnbPx8k996VKlbJK4t27d0/r16/XBx98oL59+6pevXrKlSuXmjVrpg0bNqQqroxg7xizZctm9Tjxe8eZ/525c+e2+2NJ4m0bhqHIyEhJtq+Tl5eX3W6LznxGqlWrlux6R9y8eVMtWrRQmzZtNHfuXB06dEhRUVHJPicJhwJJSkZdK6XG4MGDbbqUmm+HDh2ylHPV+fzXX39V6dKl9cYbb+jnn3/WqVOn9N9//1l+NLcnNc9xRkjvazvp/lAMCd8rx44d01dffaU33nhDrVq1UtGiRVWmTBl98MEHaR7eA0kjqQbAaSaTSYUKFdLw4cP13HPPWa1bunSpoqKi0ryPhONrBAQEaP/+/XrvvfdUpUoVuy1frl69qi+//FK1atVyyf4d5evrq6CgIL300kvasGGDza+jX331lUv398QTT6hOnTo2y+vWrWvTsi+tkmpplBx7X8pc/Ytxavabnvvctm2b5QLzn3/+UUxMjC5duqRly5apRYsWqdpGgQIFrMYBdBXz58fexWhSr2dyF66pkdb6WUVGvLczy+fdkfdTeknNeG1ZjTPPob06D+L18fPzU0BAgNUyV//P7du3r03SR7rfWslV45Om1meffaYff/xRrVq1sho3zSw2NlYbNmzQ008/rWXLlj3Q2FIr8fhXkmz+79gr4yhH/rckVdaV79fChQu7bFvjx4/XunXrHKqTmv+JGXWtlBq5cuVSUFCQ3Vvi8ZOdZT6fx8bGqnPnzjbjT6bE1dcd5rHQErpy5YrD20nvaztJqlevng4dOqTXXntNxYsXt1v++PHjGj58uDp16uTyWHBf5myfDCDLKV26tNXje/fu6cSJE5YveoULF7Zq3t2sWTN98803KW434YCv0v0L+VGjRmnUqFG6deuWjh8/rr///lv79u3T559/rpiYGEn3B7CdO3euTYu5B6lEiRLKlSuXVWuS48ePu3w/AwcOtOn+kNquMZL9C84TJ07YLD9x4oRNuUKFCqV6Pw+rokWLKigoKE3bSO7iOW/evPLy8rJqcfW///3PbrevxMytBfLmzSsPDw+rX7pPnjxpUz46OjrNX4yLFCli9fjmzZv666+/MqS1mr3nzt77WLJ9PjLrezszfN4LFChgs87e+8neYNEpxbJ69WqVL18+2XpJxZBZpec59uTJk1YDWUv3u+cm5urnKyYmxup/mySbroRplTt3bnXv3t3qWsHDw0OvvvpqqreR+Dk+e/as7t69Ky8vL6vl9t6/iZ/7tm3bqm3btrp3755Onz6tEydO6MiRI5o7d6727dsn6f6X+/Hjx9sMfv8ouXbtmqKiomwSdInflyaTyTJJTOLX6c6dO/rnn39UrFgxq+WpeZ0Sc2VyKnEX58DAQE2YMEGVK1e2dK374IMP9PXXX7tsn1mFK87nO3fu1JkzZ6zWtW/fXq+//rqKFi1q+dzWrFnTqSRXUhJ32TZ3507I3B3cEel9bWdWqlQpffHFF/riiy907do1HT9+XMePH1dYWJhmz55tSTquXLlSBw8etOoGDtegpRoAlzBfUCaU8J9J4rFrzLNpJfXrV7FixbRv3z6rpNrFixetfo3y9fVV5cqV1b59e73//vvq3bu31T4Sj9GQ+CI6rS0d9u7dm+z6Xbt22XzpsPere1p17NjR6mKmcOHCDl3QN2zY0GbZ9OnTU7XMXl1XSNw952FslZJa7u7uatCggdWylStXqkCBAkl+fvLkyaMdO3ZYunJ6enradIFZunSpJQltNmvWrDTH27hxY5tlo0ePTvKX5MRJvMSfU8n519/d3d1qhlHp/gyCicem2b17t1U3Fin93ttplRk+7yVKlLAZv23evHk2v+6n9H6qWbOmzTnxp59+SvJ9HRQUJJPJpCNHjtjtlpacU6dO2YxVExYW5tA2nJWe51h7z3HiZSaTSTVr1kwpTCv9+vVL9vl5//33bT7Trm4tKdl2a27Xrp1DsxkmnnH69u3bmjdvntWy+Ph4zZw502pZ/vz5VbZsWUn3v2AnHMPJzc1NJUqU0FNPPaWBAwfa1E3p2kN6+P+nzZ492+rxrVu3bBJS5cuXtySiEr9OUuo+Ix4eHjYzizrKkWvDxF0chwwZohdeeEGVKlVSUFCQihQpol9//TVN8TjC1de1aeGK87m9LqQzZ85UkyZNVKZMGQUFBenKlSupSqg5ch2ZOAF87do1m2EOvvzyyxT36QhXXNtJtt2f8+TJoyeffFLdu3fXzJkzbcaLdXQ8UqQOLdUAOMz8a6NhGLp48aJCQ0NtBifPli2b5YJUknr37q0JEyZY/qn9999/Cg4O1ltvvaW6desqT548io6O1l9//aVt27ZpxYoVunjxoiIiIiy/fn/44YdaunSpnnnmGdWpU0elS5dWrly5dPfuXe3fv9/mgi1xFw3zL6JmGzdu1Pr161WqVCm5ubnJx8dHBQsWTPXz0LFjR/n4+Khdu3aqW7euSpQoIS8vL126dEkbNmzQ1KlTbeqkx8DYnp6emjZtmnbu3CnpflPwlMbwSigwMFCtW7fW6tWrLcu+++47Zc+eXT169JCnp6cWLlxoczHbqFGjVP0C6Yx8+fJZXSjMnz9fTzzxhOU1zJUrl0u6qWQVr732mjZu3Gh5HB4ergYNGuiNN95QhQoVlC1bNl2+fFm///67NmzYoLVr1ypfvnx64YUXLHW6d++uPXv2WB5HRUWpadOmeu+991SoUCGtX79eo0aNSnOsderU0RNPPGGVdF6yZImioqI0aNAglS1bVrdv39bBgwcVGhqqNm3aaMiQIZayiT+n0v1f/rt27Wrp8lW0aNFUDwb+6quvatOmTZbH169fV4MGDfT++++rZMmS2r9/v9566y2rOh4eHurfv78jh/3AZJbP+wsvvKBPPvnE8vjYsWNq3bq1hg0bJj8/Py1dutRmwO7EvL291bdvX02bNs2y7JtvvtHVq1fVp08fSwvQc+fO6cCBA1q9erW2b9+uF198US1btkz1MWe09DzHrly5Un379lXfvn1lMpk0e/ZsrVy50qpM4sHnU2Pv3r2aNWuWatSooeeff97yf/r06dMKDQ21+X9rMpnUs2dPh/aRGhUqVNDkyZMtX3ATTzKQkvbt2+vtt9+2+oI8cOBARUVF6amnntLVq1c1ZcoU/fnnn1b1Xn31VUsXxJMnT6pWrVpq0aKFmjRpogoVKqhgwYLy8PDQP//8o8mTJ1vVTenaQ0rbOS0rePfddxUXF6enn35akZGRCgkJsUlSvPjii5b79evXV+XKla1+3Jg4caIMw1C7du10+/ZtTZ8+3abr5QsvvGAz4Y6jEr8+4eHhWrZsmapUqSIPDw95eHhYErmJr01mzJihxx9/XCVKlFBERIQmT56sAwcOpCmetMSe1uvatHDF+dzeZ+Wdd97Rq6++Kk9PT23fvl1jx45NVTyOXEcmTjxJUufOnTVhwgT5+Pjo008/1Y4dO1K1X0e44trOPPxOy5YtVaNGDQUFBSlHjhyKiYnRmjVrrMajlGzPT3CRBzTLKIAsyt6U7Km5vfHGGzbb+uqrrxzeTkREhKX+0KFDHaq7adMmq/1PmzYt2fKOTqOeeEr7lG4FChSwmqo+texN453c1OCJ2ZtiPuHzahiGceLECSMgICDVx+Lv728cPnzYahv2pmJPvB97z9uYMWNsynTo0CHZ/SesY+/47EnLc2jWs2fPVB1jSlIzfXtiXbt2dej9lnibN2/eNMqUKZNiPXd392Rfn9S8zuHh4UbOnDlTFecnn3xiVffq1auGp6dnqs8Lic9RPXv2tNrevXv3Unw/Jb6NHz/e5vlPzfvWmdc1sazyeb906VKK2/Dw8EjxWK5du2aUK1fOodcn8WucmnOAved18+bNqX5ek2Jvu4njM4z0O8dmy5Yt2e14enoaBw4ccPi4qlSp4tBrMnjwYKeev9S8X5OT+Jxs7zO3fPlyw83NLdXHUrVqVePGjRuW+r///nua3p+OntNSI/H5yN5xp3RutLcde+e1lD7Dic973t7eNv9HEt9KlChhXL9+3Wo7v/76q+Hr65vq57lYsWLGhQsXrLaR2uuBhFauXJnsfhI+twMGDEgxrkKFCqX42qT0nBqG7Xvb3jWqq69r7Z1j7L0nkpLW8/nNmzeNfPnyJVs+R44cNtcXab2OjIuLMwIDA5MtbzKZbJYl/h+SEdd2NWrUSHXdnDlzGtHR0al+PZF6dP8E4HLPP/+8Jk2aZLP8lVde0axZs5Q9e/ZUbScgIMDhLj5m7777rk03tBdffNFlg6o66oknntC2bdscbi3woDz22GPavHlzqsa9CgoK0oYNG9KtlZp0/5dJR1rfPArmzp2rgQMHpnoA58Rj0fj6+uqnn35KctBmk8mkyZMnO9S1KilVqlTR5s2bVaZMGYfr5smTx6Exk1JiMpn07bffql+/fimW9fDw0KRJk+zObPgwccXnPX/+/Fq+fHmSrUQ8PT1tutnZkzt3bm3atElNmjRJVewmk8kl79EHLb3Osd9++63NhAFmHh4eCg0NVdWqVR0N19ItLyXu7u569913rVotZjbPPfecFi9ebHdGysSaNGmidevWpfo6JbEqVapoypQpVstcfU7L7AoWLKiZM2cm2fIuf/78WrFihU2LmVq1aunnn39O1cQCVatW1aZNm1zSCqtFixaqXr16qsqGhIQk+7ns0aPHA23lnJHXtfak9Xzu6+urWbNmJXn95+vrq4ULF6Zq/EZHriPd3d31zTffJFk+Z86c6TZOXlqv7VLL19dX8+fPT3PLTthHUg1Amnh6eip37tyqXr26Xn31VW3fvl0LFiywO4aIJPXp00enTp3SpEmT1LRpUxUsWFDe3t7y8vJSwYIF1aBBA7355ptau3atzp8/bzW48rvvvqtly5bpzTffVIMGDVSqVCn5+fnJ3d1dfn5+qly5sl5++WXt3r1bEyZMsNm3v7+/du7cqZdeesnSVTMtli1bpg8//FDt2rVThQoVVKBAAXl6esrb21v58uXTk08+qddee03r1q3Tnj17bCZzyGwqVqyoQ4cOaf78+WrXrp2KFSsmHx8feXt7q3DhwmrTpo1mzJihI0eO6IknnkjXWGrVqqWtW7eqbdu2KlCgQKaZBSsjeXl5aerUqTp8+LCGDh2qWrVqKU+ePPLw8FC2bNkUFBSkli1b6r333tOePXu0bds2m22UK1dOhw4d0ptvvqnHHntMXl5eCggIUJs2bbR582a9/fbbLou3Ro0a+uOPP7RgwQJ16tRJJUqUUPbs2eXl5aWiRYuqQYMGGj16tJ599lmbup988ommTp2qmjVruqSrgo+Pj2bMmKF9+/bptddeU6VKleTv7y8PDw/lyZNHNWvW1LBhw3T8+HENGzYszfvLClzxea9Xr55+//139e/f3zKIdMGCBdWlSxft2bNHzz//fKpiKVSokDZu3KhffvlFvXv31uOPP251bn/88cfVqVMnTZs2TRERERo/frwrn4oHJj3OsdWqVdMff/yhgQMHWv6v5cuXT506ddKePXvUrVs3p2INCwvTxo0bNWzYMDVu3FhFixaVj4+PPDw8lDdvXtWuXVvDhg3TkSNHNGHChAc++6ujOnTooJMnT+qDDz5Q48aNlT9/fnl6eip79uwqWbKkXnjhBa1Zs0YbN260+fGrXLly2rZtmyZNmqRnn31WFStWVP78+eXh4SFfX18VL15czz77rGbPnq09e/bY7cLm6nNaZterVy/9+uuv6ty5swoUKCAvLy8FBQVp0KBB+v3331WxYkW79Ro2bKjjx4/ryy+/VMuWLVWoUCF5eXnJ19dXgYGB6tChgxYuXKi9e/eqVKlSLonVw8NDGzdu1NChQ1W2bFmbCbISypMnj3bv3q0RI0aoTJky8vLyUq5cuVS/fn3Nnz9fc+fOfaCfBVdf17pCWs/nzzzzjHbv3q2OHTsqX7588vT0VJEiRdS9e3ft3btXrVu3TlUcjl5HNm/eXNu3b9czzzyjPHnyWN6zr732mv788089/fTTTj8nyUnrtd2iRYs0e/Zs9e3bVzVr1lRgYKB8fX3l6empgIAA1alTRyNHjtTRo0ctXUXheibDcPEctAAAAABcKiwszKYFdkRERJpnHwbSYuzYsQoJCbE8Ll68uN0ZaAHgYUVLNQAAAAAAAMBBJNUAAAAAAAAAB5FUAwAAAAAAABxEUg0AAAAAAABwEBMVAAAAAAAAAA6ipRoAAAAAAADgII+MDgDIKu7du6fz588rZ86cMplMGR0OAAAAAACwwzAMXb9+XYULF5abW/q1JyOpBqTS+fPnVaxYsYwOAwAAAAAApMLZs2dVtGjRdNs+STUglXLmzCnp/ofSz88vg6MBAAAAAAD2xMTEqFixYpbv8emFpBqQSuYun35+fiTVAAAAAADI5NJ76CYmKgAAAAAAAAAcRFINAAAAAAAAcBBJNQAAAAAAAMBBJNUAAAAAAAAAB5FUAwAAAAAAABxEUg0AAAAAAABwEEk1AAAAAAAAwEEk1QAAAAAAAAAHkVQDAAAAAAAAHOSR0QEAWU2nZ0Lk6eGd0WEAAAAAADKJVRsnZHQIyAC0VAMAAAAAAAAcRFINAAAAAAAAcBBJNQAAAAAAAMBBJNUAAAAAAAAAB5FUAwAAAAAAABxEUg0AAAAAAABwEEk1AAAAAAAAwEEk1QAAAAAAAAAHkVQDAAAAAAAAHERSDQAAAAAAAHAQSTUAAAAAAADAQSTVAAAAAAAAAAeRVAMAAAAAAAAcRFINAAAAAAAAcBBJNQAAAAAAAMBBJNUAAAAAAAAAB5FUAwAAAAAAABxEUg0AAAAAAABwEEk1AAAAAAAAwEEk1QAAAAAAAAAHkVQDAAAAAAAAHERSDQAAAAAAAHAQSTUAAAAAAADAQSTVAAAAAAAAAAeRVAMAAAAAAAAcRFINAAAAAAAAcBBJNQAAAAAAAMBBJNUAAAAAAAAAB5FUAwAAAAAAABxEUg0AAAAAAABwEEk1AAAAAAAAwEEk1QAAAAAAAAAHkVQDAAAAAAAAHERSDQAAAAAAAHAQSTUAAAAAAADAQSTVAAAAAAAAAAeRVAMAAAAAAAAcRFINAAAAAAAAcBBJNQAAAAAAABcwDEPbt2/X22+/rdq1aytXrlzy8vJS4cKF1aFDB23evNluPZPJlKrb3LlzUx1LVFSUFi1apKFDh6p+/frKli2bTCaTnnrqqWTrBQcHpyqW3r1729SdOnWqSpUqJW9vb5UpU0Zff/11kvu5cOGC/Pz81KlTp1QfU2bjkdEBAAAAAAAAPAw2bdpkSVq5ubmpVKlSyp49u44fP64ffvhBP/zwg0aNGqX33nvPql69evWS3Oa///6rP//8U5JUu3btVMcSFhamrl27OnwMlSpVUlxcnN119+7d065duyRJderUsVr31VdfafDgwfLx8VG5cuV07Ngxvfrqq4qNjdXAgQNttvXWW28pPj5eH330kcMxZhYk1QAAAAAAAFzAMAyVKlVKb775prp27arcuXNLku7evauxY8dq4sSJGj9+vJ588km1adPGUm/79u1JbnPUqFH6888/VatWLZUtWzbVsfj6+qphw4aqVauWatWqpb///lsjRoxIsd60adOSXLdhwwY1a9ZM3t7eVi3M4uPjNXbsWOXMmVP79u1T6dKldfToUdWoUUMhISF69dVX5eHxfymobdu2acGCBXrvvfcUGBiY6mPKbB6a7p+hoaEymUwKDQ21Wm4ymRQcHJwhMeHBOHXqlEwmk3r16pXRoQAAAAAAHmG1atXSkSNH9Oqrr1oSapLk5eWlCRMmqGXLlpKkGTNmpGp7hmHou+++kyS9+OKLDsXSvHlzbdmyRVOmTFGnTp1UqFAhh+rbM3/+fElSmzZtrI7vzJkzioyMVIcOHVS6dGlJUtmyZdWhQwddvXpVf/31l6VsfHy8Bg4cqJIlS+rtt99Oc0wZKUsk1cxJk+RuD9oPP/ygjh07qnTp0vLz81OOHDlUoUIFDRkyROfOnUuy3rp16xQcHCw/Pz/lzJlTwcHBWrduXZpiMZlMKleuXJq2kZGCgoIsr2PCD1pCcXFxKliwoKXcxYsXH3CUAAAAAAAkz8/Pz6pFVmLNmjWTJB07dixV29u2bZtOnTolT09Pp7pyutLNmzf1ww8/SLJN8EVGRkqSChQoYLXcnMiLjo62LPvyyy918OBBffrpp/L29k7PkNNdlur+WbJkSXXv3t3uunbt2ql27douybymxo8//qiDBw+qZs2aln2Gh4dr6tSpmjt3rrZv364KFSpY1fnuu+/UvXt3BQQEqGfPnjKZTFq8eLFatGihb7/9Vi+88MIDiT0zcnO7n9+dPXu2Jk+ebLN+1apVunTpkjw8PGz6dhcpUkRHjhyRv7//A4kVAAAAAABn3L59W9L9rpmp8e2330qSWrRooYCAgHSLKzWWL1+uGzduKG/evGrVqpXVOnMXzsTJwqNHj0qSChYsKEm6fPmyRo8erVatWll1f82qslRSrVSpUho7dmyS6x9kUmXGjBny8fGxWT5r1iz169dPY8eO1ZIlSyzL//33Xw0YMEABAQHav3+/ihUrJkl69913Vb16dQ0YMECtWrWyaj75KPH09FTDhg01f/58TZgwwSazP3v2bAUEBKh06dKWQRET1s3KLfUAAAAAAA8/wzAseYLkJiYwu3PnjqW8o10/04O562eXLl3k6elpta5QoUKqWLGiVqxYoUWLFqlVq1Zas2aNVq5cqZIlS6pkyZKSpOHDh+vWrVv67LPPHnj86SFLdP9MjaTGVDM7e/asunTporx58yp79uwKDg7Wzp07nd6fvYSaJMtAfX///bfV8iVLligqKkoDBw60JNSk+2+8IUOGKCoqyioJl17OnDmjvn37qkiRIvLy8lLRokXVt29fnT171qrckCFDZDKZFB4ebrW8devWMplM6tevn9XytWvXymQy6YMPPnA6tt69e+vixYtas2aN1fKLFy9q7dq1euGFF+Tl5WVTL6kx1czTAMfFxem9995TiRIlLNP6fvnll07HCQAAAACAo2bMmKEDBw7Iy8tLQ4YMSbH8ypUrFRUVJX9/fz3zzDPpH2AyLl26pF9++UVS0gm+jz/+WCaTSV27dpWfn5+6du0qk8lkmfjgt99+05w5czR06FCVKlVK0v3x1S5cuGBpwZfVPDRJteT8+++/qlevnk6dOqWXXnpJHTp00K5du9S4cWOFhYW5dF+rV6+WJFWsWNFquXk/Tz/9tE2d5s2bS5K2bNni0lgSO378uGrWrKnZs2erRo0aGjp0qKpXr67Zs2friSeesEoENm7cWJK0efNmy7L4+HjLjCQJl0v/d3zmes5o166dcufOrTlz5lgtnzdvnuLi4tSnTx+ntvv8889rxowZevrpp9W3b19du3ZNr7/+eqoHhgQAAAAAIC3279+vwYMHS5LGjx9vabmVHHPXz06dOiXZsOdB+f777xUfH6/SpUurdu3adss0a9ZM27ZtU48ePdS4cWP17NlTO3bsUMuWLWUYhgYMGKCiRYtq5MiRku7PMpo/f34VLlxY/v7+6t+/v+7cufMgDyvNslT3z7///ttu988WLVokW+/QoUN68cUXNXfuXMukBn379lXjxo3Vv39/HT161DKml6OWL1+u8PBw3bx5U4cPH9a6detUokQJjRs3zqrc8ePHJckyC0ZC5mXmMunllVdeUWRkpKZPn66XXnrJsvybb77Ryy+/rFdeeUUbNmyQJDVq1Ehubm7avHmz3njjDUnSvn37FBMTo6ZNm2rjxo06c+aMpd/05s2blTNnTtWoUcPp+Hx8fCwJsMjISOXPn1+SLEnAypUrO7Xds2fP6o8//pCfn58kafDgwapYsaI++ugj9e/fP8l6d+7csfpAx8TEOLV/AAAAAMCjKyIiQm3atNHt27fVrVs3vfXWWynWuXr1qqUXV48ePdI7xBSZu36m1A21du3adpNuM2fO1J49e7Ro0SJly5ZN3377rQYNGqR69eqpX79+2rJli2bOnCkp9TOjZgZZKql24sQJhYSE2CzPlSuXcuXKlWQ9d3d3vf/++1azhDZq1EitWrXS6tWrtXPnTtWvX9+pmJYvX665c+daHj/xxBNauHChSpQoYVXOPNOFvXHfsmfPLnd3d6vZMFzt7Nmz2rRpk8qXL2+TSOrfv78+/fRTbdy4UWfPnlWxYsWUK1cuValSRVu3blV8fLzc3d21efNmmUwmjR07Vhs3btSmTZvUq1cvxcTEaP/+/WrevLnc3d3TFGefPn305Zdfav78+Ro6dKh27Niho0eP6osvvnB6mxMnTrQk1KT70/rWq1dPW7Zs0fXr15UzZ84k69l7vwEAAAAAkBoXL15Us2bNdOHCBbVu3doydFVKFi1apNjYWAUFBTmdr3CVI0eOaP/+/ZKU5OSRyfn33381YsQINWnSRJ07d5YkTZo0Sf7+/lq7dq1y5sypXr166fTp05ozZ47Gjx9vM4toZpWlun82b95chmHY3FLqi1y8eHGrcczMGjRoIEk244Y5IjQ0VIZhKCoqSps3b5aXl5dq1KihTZs2Ob3N9HDgwAFJ95OJiT/AJpNJDRs2lCQdPHjQsrxx48aKjo62fHg2b96sKlWqqH79+ipYsKClC6g58ZaWrp9m5hZp5i6gs2fPlo+Pj7p16+b0NqtXr26zrGjRopKkqKioJOu9++67io6OttwSjzsHAAAAAEBSrl27pmbNmunEiRNq1KiRlixZYjPAf1LMXT+7d++eqiRcejK3Uqtfv75NA6LUGDVqlKKiojR16lRJ0vXr13X48GHVq1fPqpFLy5YtFR8fr71797om8AcgSyXVnGXuRpiYOfPpihZi/v7+Cg4O1tq1a+Xr66sePXooNjbWan1S+/rvv/8UHx+frrOXmrsuJpXtNU9vmzC+hOOqxcXFaceOHZZlwcHBlqSa+a8rkmrS/QkLDh8+rE2bNmnx4sVq27Ztsi0RU2LveTXPLhofH59kPW9vb/n5+VndAAAAAABIyY0bN9SqVSv98ccfqlmzplauXClfX99U1T1x4oR27dolybmWYa5kGIa+++47Sc7NQHrw4EFNnz5dAwcOVIUKFSTdf24k2fQaMz9OrvFLZvNIJNUiIyPtLr906ZIk+0kXZ/n5+al27do6d+6c1cD/yY2bltx4a66MS/q/Y07MvDxh4qhhw4aWbp979uzRjRs3LImzxo0b6+zZszpx4oTCwsLk7++vatWquSTW7t27y8vLSz169NCNGzecnqAAAAAAAIAH7c6dO3ruuef066+/qkKFCvr555+THHbIHnPLsFq1aqls2bLpFWaqbNmyRWfOnJG3t7c6derkcP0BAwYoICDAanz8AgUKyMvLSydOnLAqa34cEBCQppgfpEciqXb69Gm7Xfe2bdsmSapatapL93f+/HlJ/9caSrrf7VKS1q9fb1N+3bp1VmXSg/kYt27dKsMwrNYZhmH3ufDz81O1atW0fft2rV+/Xu7u7pZuok2aNJEk/fDDDwoPD1fDhg2dnuwhsYCAAD3zzDM6d+6cAgMD1bRpU5dsFwAAAACA9BQfH6+uXbtq06ZNKlmypH755RflyZPHoW040jJs6dKl6Trumrkbaps2bZQ7d26H6s6fP1/bt2/XBx98YNWAx83NTTVr1tTevXst+ZCzZ89q3rx58vb2Vq1atVx3AOnskUiqxcfHa+TIkVbJpC1btmjNmjUqVaqU6tat69D27ty5o927d9tdN2fOHP32228qVaqUVcuzzp07y9/fX9OmTbNK8F24cEGffvqpcuXK5VTWN7UCAwPVuHFjHT58WLNnz7ZaN3v2bB0+fFhNmjSxGXuucePGunHjhr744gtVr17d0qqvVKlSKlq0qKZMmaJ79+65rOun2ZQpU/Tjjz/qxx9/dFmyDgAAAACA9LR48WItX75c0v3kUadOnVS/fn2bW1Lf/3ft2qW///5bnp6e6tq1a4r7u3Hjhk6fPq1//vnH7vqAgADLbeDAgZLuN7ZJuHzhwoV2696+fVtLly6V5HjXz+vXr2vYsGGqU6eO3dlLQ0JCZDKZ1Lp1a1WqVEmPP/64IiMjNWTIEIeTdxkpS83+6azKlSsrLCxMtWvXVpMmTXT+/HktXLhQnp6emjFjhsNJm1u3bqlOnTqqWLGiqlatqiJFiig6Olq//fab9u/frxw5clgG2jfLnTu3Pv/8c7344ouqXr26unbtKjc3Ny1atEiXLl3S/Pnz0/TGuXDhgnr16mV3XWBgoMaNG6evvvpK9evXV//+/bVy5UqVL19ef/75p1asWKF8+fLpq6++sqnbuHFjTZkyRZcvX1bv3r1t1pmbpbo6qVaiRAmnBkAEAAAAACCj3Llzx3L/+PHjdoeAku5PqGiP+Tt2ixYtXNIN8urVqzbLYmNjrZbfvn3bbt0VK1YoOjpaefPmVatWrRzab0hIiC5duqRVq1bZnWihadOmWrx4sUJCQnT06FEVKFBAw4YN08iRIx3aT0Z7JJJquXPn1sqVK/XWW29p+vTpun37tmrXrq0JEyaoXr16Dm8ve/bsCgkJ0ebNm7Vx40ZduXJFnp6eCgoK0pAhQ/TGG28oMDDQpl737t0VEBCgiRMnKjQ0VNL9mSnnzp2r5s2bp+kYY2JiNHfuXLvrqlSponHjxqls2bLau3evQkJC9PPPP2v16tXKly+fevXqpTFjxtj9UDdo0EAeHh6Ki4uzSZyZk2q5c+dW5cqV0xQ/AAAAAABZXa9evZJs8JIaX375pb788kuX7S/x8E+O6Ny5szp37uxU3Q8//FAffvhhsmU6duyojh07OrX9zMJkpOUZBh4hMTEx8vf319MN35Snh3dGhwMAAAAAyCRWbZyQ0SEgAfP39+joaKvx3FyNwaoAAAAAAAAAB5FUAwAAAAAAABz0SIyplhphYWEKCwtLsVzVqlXVtm3bdI3l008/VVRUVIrlevXqpaCgoHSNxRmhoaE6depUiuXatm2rqlWrpns8AAAAAAAArkZS7f8LCwtTSEhIiuV69uz5QJJqp0+fTrFccHBwpk2qbdmyJcVyQUFBJNUAAAAAAECWxEQFQCoxUQEAAAAAwB4mKshcmKgAAAAAAAAAyKRIqgEAAAAAAAAOIqkGAAAAAAAAOIikGgAAAAAAAOAgkmoAAAAAAACAg0iqAQAAAAAAAA4iqQYAAAAAAAA4iKQaAAAAAAAA4CCSagAAAAAAAICDSKoBAAAAAAAADiKpBgAAAAAAADiIpBoAAAAAAADgIJJqAAAAAAAAgINIqgEAAAAAAAAOIqkGAAAAAAAAOIikGgAAAAAAAOAgkmoAAAAAAACAg0iqAQAAAAAAAA4iqQYAAAAAAAA4iKQaAAAAAAAA4CCSagAAAAAAAICDSKoBAAAAAAAADiKpBgAAAAAAADiIpBoAAAAAAADgIJJqAAAAAAAAgINIqgEAAAAAAAAOIqkGAAAAAAAAOIikGgAAAAAAAOAgkmoAAAAAAACAg0iqAQAAAAAAAA4iqQYAAAAAAAA4iKQaAAAAAAAA4CCSagAAAAAAAICDSKoBAAAAAAAADiKpBgAAAAAAADiIpBoAAAAAAADgII+MDgDIapasHCM/P7+MDgMAAAAAAGQgWqoBAAAAAAAADiKpBgAAAAAAADiIpBoAAAAAAADgIJJqAAAAAAAAgINIqgEAAAAAAAAOIqkGAAAAAAAAOIikGgAAAAAAAOAgkmoAAAAAAACAg0iqAQAAAAAAAA4iqQYAAAAAAAA4iKQaAAAAAAAA4CCSagAAAAAAAICDSKoBAAAAAAAADiKpBgAAAAAAADiIpBoAAAAAAADgIJJqAAAAAAAAgINIqgEAAAAAAAAOIqkGAAAAAAAAOIikGgAAAAAAAOAgkmoAAAAAAACAg0iqAQAAAAAAAA4iqQYAAAAAAAA4iKQaAAAAAAAA4CCPjA4AyGqaD/5AHl4+GR0GAABIwbbp/8voEAAAwEOMlmoAAAAAAACAg0iqAQAAAAAAAA4iqQYAAAAAAAA4iKQaAAAAAAAA4CCSagAAAAAAAICDSKoBAAAAAAAADvJIj43eu3dPq1ev1pEjR5QjRw41b95cJUuWTI9dAQAAAAAAAA+c00m148eP65NPPpEkmUwmTZ48WdmzZ1dMTIyaNm2q/fv3/99OPDw0bdo0vfTSS2mPGAAAAAAAAMhgTifVfvnlF3399dcymUyqVKmSsmfPLkmaNGmS9u3bZ1U2NjZWAwYMUKNGjVS2bNm0RQwAAAAAAABkMKfHVNuzZ4/l/tNPP225P3/+fJlMJplMJkmy/I2Pj9eMGTOc3R0AAAAAAACQaTidVDt06JDlfu3atSVJEREROnfunCTJ09NTzz33nLJly2Ypt3XrVmd3BwAAAAAAAGQaTifVIiMjLffNkxD88ccflmWvv/66fvzxR02aNEmSZBiGTp486ezuAAAAAAAAgEzD6aTalStXLPdz5swpSTp69KhlWZ06dSRJjRo1siy7fv26s7sDAAAAAAAAMg2nk2oJRUVFSbJOqplbr/n6+lqW+fj4uGJ3AAAAAAAAQIZyOqmWN29ey/0FCxbo8uXLWrdunWVZmTJlJEkxMTGS7k9YkC9fPmd3BwAAAAAAAGQaTifVypcvb7n/ySefqGDBgpZJCsqUKaPs2bNLktU4agUKFHB2dwAAAAAAAECm4XRSrU2bNpb7hmFYbiaTSW3btrWs+/XXXy33q1ev7uzuAAAAAAAAgEzD6aRa//79VbZsWUsizWQySZICAgL0xhtvWMqtXr3acr9evXppCBUAAAAAAADIHDycrejr66tt27YpJCREW7duVVxcnGrUqKHRo0crf/78kqQLFy6ofPnylq6iwcHBLgkaAAAAAAAAyEhOJ9Wk+63Spk2bluT6QoUKacmSJWnZBQAAAAAAAJDpOJ1Ua9KkieX+888/r/79+7skIAAAAAAAACCzczqptm3bNt27d0+SNHLkSJcFBAAAAAAAAGR2Tk9UkD9/fhmGIUkKDAx0WUAAAAAAAABAZud0Uq1hw4aW+//8849LggEAAAAAAACyAqeTakOHDpW7u7skacqUKZZWawAAAAAAAMDDzumk2hNPPKFZs2bJ3d1d69atU/369bVq1SqdO3eOBBsAAAAAAAAeak5PVGBupSZJhmFo9+7deu6555KtYzKZFBcX5+wuAQAAAAAAgEzB6aRawtZoJpOJ1mkAAAAAAAB4ZDidVJPuJ9Ps3beHpBsAAAAAAAAeFmlKqpEoAwAAAAAAwKPI6aTanDlzXBkHAAAAAAAAkGU4nVTr2bOnK+MAAAAAAAAAsgy3jA4AAAAAAAAAyGrSNKZaQjdu3NCuXbt0/vx5SVKhQoVUt25d5ciRw1W7AAAAAAAAADKFNCfVIiMjNXz4cH333XeKi4uz3riHh7p3766JEycqf/78ad0VAAAAAAAAkCmkqfvniRMnVKtWLc2dO1exsbEyDMPqFhsbq9DQUD355JOKiIhwVcwAAAAAAABAhnI6qXbv3j117txZZ86ckWEYMplMdm+GYej06dPq1KmTDMNwZewAAAAAAABAhnA6qbZs2TIdOHDAKnlm72YymSRJBw4c0LJly1wWOAAAAAAAAJBRnE6qLVmyxHLfMAy1aNFCP/zwg8LDwxUeHq4ffvhBTz/9tFVibfHixWmPGAAAAAAAAMhgTifV9u7da7n/4osvas2aNWrbtq0qV66sypUrq23btvr555/1wgsvWFqt7dmzxyVBAwAAAI6IiIjQjBkz1L9/f1WpUkUeHh4ymUwaP368Q9uZOXOmpadGv379nIrl7t27+uyzz1S7dm35+/vL09NThQoVUrt27bRp06Zk6547d04vvfSSihUrJm9vbwUGBurll1/WuXPnkqwzdepUlSpVSt7e3ipTpoy+/vrrJMteuHBBfn5+6tSpk1PHBgDAo8TppFpkZKTl/ksvvZRkuVdeecVy//Lly87uDgAAAHDaZ599ppdeekkzZ87UoUOHFB8f7/A2Ll++rGHDhqUpjps3byo4OFhDhgzRr7/+qjx58qhy5cq6ffu2li9frqZNm2ry5Ml26/7555+qXLmyZsyYoevXr6tixYqKiYnRN998oypVquivv/6yqfPVV19p8ODBOnfunMqVK6ezZ8/q1Vdf1bRp0+zu46233lJ8fLw++uijNB0nAACPAqeTagkvRHx8fJIsl3CdMxcvAAAAQFoFBASoTZs2GjdunNauXasOHTo4vI033nhDUVFRat26tdNxfPzxx9q1a5fy5cun3bt3KyIiQvv27VNkZKTGjh0rSRoxYoT+/vtvq3rx8fHq1KmTrl27pg4dOuj8+fPat2+fzp07p/bt2+vq1avq0qWL7t27Z1Vn7Nixypkzpw4dOqSDBw8qPDxc2bNnV0hIiOLi4qz2sW3bNi1YsEDvvvuuAgMDnT5GAAAeFU4n1fLmzWu5/9NPPyVZ7scff7Rb51E3duxYmUwmhYWFZXQoTjl16pRMJpN69epltTw4ONgyhh4AAEBmMWrUKK1cuVL/+9//1KJFC+XIkcOh+hs2bNB3332nl19+WU888YTTcaxevVqS9L///U9PPvmkZbmnp6fGjBmjqlWrKj4+Xr/88otVvR9++EF//vmn8ubNqzlz5ihbtmySpOzZsys0NFR58+bVoUOHrK7Lz5w5o8jISHXo0EGlS5eWJJUtW1YdOnTQ1atXrVq2xcfHa+DAgSpZsqTefvttp48PAIBHidNJtapVq0q6P0nBxIkTNXbsWKuxHM6dO6fRo0frgw8+sIw7Ya6T3qKiojRo0CDVqVNHBQsWlLe3t4oUKaImTZpo2bJlMgzDpk5MTIzefPNNFS9eXN7e3ipevLjefPNNxcTEPJCYM5OgoCDLa2bvlhGJwIMHD6p3796qXLmy8ubNKx8fH5UsWVKdO3e2Gt8vsc2bN6tVq1YqVqyYfH19VbJkSXXr1k0HDx58gNEDAICs7Pbt23r11VeVP39+TZgwIU3bunXrliTpscces7u+ZMmSkmTTiuyHH36QJHXu3Fk5c+a0WpczZ07LGGgJJxMzD9dSoEABq/KFChWSJEVHR1uWffnllzp48KA+/fRTeXt7O3ZQAAA8ojycrdi6dWutWbNGJpNJ8fHxeu+99/Tee+/Jy8tLJpNJd+7ckSRLAstkMumZZ55xTdQpuHLlimbPnq3atWurbdu2ypMnjyIjI7Vy5Up17NhR/fv31zfffGMp/99//6lRo0YKDw9Xs2bN9Pzzz+vgwYP65JNPtHnzZm3fvl3Zs2d/ILFnFu7u7ho1apTddUFBQSpSpIiOHDkif3//BxLPnj17tGbNGtWpU0eNGjVS9uzZdfLkSa1cuVJLly7VvHnz1L17d6s606ZN06BBg5QrVy61b99e+fLl07Fjx7RkyRItXbpUa9as0VNPPfVA4gcAAFnX+PHj9ffff2vu3LnKlStXmrZVuXJlHTx4UDt37rTpRnrnzh3t27dPklSzZk2rdbt375Yk1atXz+5269Wrp6+//lq//vqrZZm5C+exY8esyh49elSSVLBgQUn3x4obPXq0WrVqpTZt2jh7aAAAPHKcTqr16tVL77//vi5cuCCTyWRJnpmTaWbmdYULF1bPnj3TFm0qlShRQlFRUfLwsD6869evq3bt2poxY4YGDx6sChUqSJImT56s8PBwvfPOO/rggw8s5ceMGaNx48Zp8uTJCgkJeSCxZxYeHh6WcT2SUq5cuQcTjKTu3bvbnWHr8OHDeuKJJzR06FC98MILlq6nsbGxGjVqlPz8/HTo0CEVK1bMUmf58uVq166dJkyYQFINAAAk68iRI5oyZYoaNGigHj16pHl7w4cP148//qgpU6Yob9686tKli/LkyaOjR49q1KhROnXqlLp3767atWtb6ty9e1dnzpyRlHQLN/PyU6dOKTY21jKjaMWKFbVixQotWrRIrVq10po1a7Ry5UqVLFnS0ipu+PDhunXrlj777LM0Hx8AAI8Sp7t/+vr66vvvv7dMRJBUV0HDMGzKpjd3d3ebhJp0v2l88+bNJcky+KthGJo5c6Zy5Mih0aNHW5V/9913lTt3bs2aNctul9HUOHv2rJ5//nnlyZNHOXLkUKNGjbR161a7Ze/evatp06apefPmlmnS8+fPr/bt2+vAgQNWZefMmSOTyaQpU6bY3Za5FeHgwYOdijslSY2pZnb79m298847KlasmHx8fFSpUiXNnj3b6f0l9d6pUKGCHn/8cUVGRlp11b169apiYmJUqVIlq4SaJLVq1Uomk8lqBlsAAIDEDMPQyy+/rHv37unLL790yTbLly+vHTt2qFmzZnrrrbdUtGhRZcuWTdWqVdPu3bs1bdo0zZ0716pOdHS0ZQKC3Llz292uefm9e/esrok+/vhjmUwmde3aVX5+furatatMJpNl9s/ffvtNc+bM0dChQ1WqVClJ98dXu3Dhgm7fvu2SYwYA4GHldFJNkho0aKCwsDBVrFhRhmHYvVWuXFlhYWGqX7++q2J22u3bt7Vp0yaZTCaVL19eknT8+HGdP39e9erVs+ni6ePjo4YNG+rcuXM2MzClxoULF1SnTh0tXLhQtWrV0qBBg5QnTx41a9bM0oQ/oWvXrmnIkCG6c+eOWrVqpTfeeEPBwcFas2aN6tatqz179ljKdunSRf7+/po5c6bdfZuX22vd9SB06tRJixYtUqdOndS/f39FRkaqb9++mjhxokv3c+LECR09elTFihWz6opaoEABBQQE6Pfff7ca60+S1q5dK8Mw1KRJE5fGAgAAHi6zZs3Stm3bNGTIEFWsWNFl2z1z5owuXbpk6c1RtWpV5ciRQ1evXtWcOXN06NAhq/IJk1teXl52t5lwHDTzuG2S1KxZM23btk09evRQ48aN1bNnT+3YsUMtW7aUYRgaMGCAihYtqpEjR0q6P3xG/vz5VbhwYfn7+6t///42PVEAAMB9Tnf/NKtZs6YOHjyo7du3a8uWLTp//rwkqXDhwmrUqFGGJtOioqL06aef6t69e4qMjNSaNWt09uxZjRkzxjID0vHjxyXJ8jixhOWSKpOUd999V+fOndP48eMtFyqS9M033+jll1+2KZ87d26dOXNGRYoUsVp++PBh1a5dWyNGjLDMBJUtWzZ1795dX3zxhbZu3aqGDRtaykdGRmrVqlV68sknValSJYdiNouLi7Pb/bNcuXLq2rVrivVPnjypP/74wzKQ7siRI1W9enWNHj1aXbp0SbLrQkrCw8O1fPlyxcbG6vTp01qxYoUk6euvv7YqZ/4F9sUXX1TlypXVrl075cuXT8ePH9fKlSvVrl07jR8/Ptl93blzx+oi8lGctAIAgEfV5cuXNWzYMBUtWlRjxoxx2Xa/++47vfjiiypQoIDCwsLUqFEjSfd7LLz33nsaP368GjZsqIMHD6pEiRKSrFvs37171+52E16z+Pr6Wq2rXbu2VXdSs5kzZ2rPnj1atGiRsmXLpm+//VaDBg1SvXr11K9fP23ZssXyQ+2MGTPSduAAADyE0pxUM6tfv36maI2WUFRUlNVYaJ6enpoyZYqGDh1qWWae9SipAff9/PysyqXW3bt3tWjRIuXPn99qf9L91mMfffSRzaCx5llKE6tQoYIaN26sdevWWcbIkKSXX35ZX3zxhWbOnGmVVJs7d65iY2PVv39/h2JOKD4+3u44cs8991yqkmojR460mpmqYMGCevPNN/X2229rwYIFSU6CkJLw8HCruAoUKKB58+bp6aeftinbtWtXBQQE6IUXXtCsWbMsy8uXL69evXpZXtukTJw48ZEbSw8AANz3zjvv6Nq1a5o+fbpy5Mjhkm3GxsZq6NChMgxDn376qSWhJt1vgfbee+/pt99+0/r16zVp0iRNnz5d0v3rVDc3N927d0///vuv3W2bl7u5uaV4jWMuP2LECDVp0kSdO3eWJE2aNEn+/v5au3atcubMqV69eun06dOaM2eOxo8fbzOLKAAAjzqnu3/26dPHcrt27VqS5e7cuaOtW7dabg9SUFCQDMNQXFycIiIiNG7cOI0cOVIdOnSwmabc1Y4eParbt2/riSeesBkPzM3NTXXr1rVbLzw8XN26dVNgYKBlJlWTyaSVK1fq7t27unLliqVspUqVVKdOHS1dutQq6Td79mzlyJFDXbp0cTp+b29vu915ly9fnqr6DRo0SHJZeHi403H16tVLhmHo1q1bOnTokFq0aKGWLVvqww8/tCk7Z84ctW7dWt26ddOJEyd08+ZNHThwQIGBgXruuec0derUZPf17rvvKjo62nI7e/as03EDAICsxTye7YABA1SwYEGrm/m6Y8GCBZZlqXH8+HFdunRJktS0aVO7ZcyTKO3du9eyzMvLyzKT58mTJ+3WMy8PCgqy/ACbnFGjRikqKspyPXT9+nUdPnxY9erVs/phtGXLloqPj7eKBwAA3Od0S7XQ0FDLTItjx45Vnjx57Ja7ePGigoODLcmh9E5m2ePu7q6goCANHz5c7u7ueueddzRjxgy9+uqrlhZqSbVEM3f5S6olW1LM28ufP7/d9fZ+6du5c6dlnK+nn35apUuXVo4cOWQymbR8+XIdPHjQZkyLl156Sb1799Z3332n1157Tdu3b9dff/2l/v37u+xXVWfYO27zMTva6s8e8+QHoaGhlu4ZLVq0sIx3cvToUb388stq06aNPvnkE0u9qlWr6scff1S5cuU0YsQI9enTJ8nnydvb22p8EgAA8OgxJ8HsuXXrltX4ZSm5fv16imXMk2MlniTgySef1KlTp7Rjxw698MILNvV27NhhKZeSgwcPavr06Ro0aJAqVKggSbpx44YkWSXUEj6OiopKcbsAADxq0jRRgSMzYppbOmU0czfBsLAwSbIZWy2xlMZcS4o5CZfUDJP2LtDef/993blzRxs3btSKFSv00UcfKSQkRGPHjk3yF9AuXbooV65clvEuzH/T0vXTFewdt/mYHU1QpuTpp5/WvXv3tG3bNsuy9evXKzY2Vo0bN7Yp7+Pjo7p16+q///7TX3/95dJYAADAwyE8PDzJibjMY6z17dvXoWvckiVLWn6U3rhxo90yGzZskCSVKVPGann79u0lSYsXL7ZJzl2/fl1LliyRJHXs2DHFOAYMGKCAgACr8XMLFCggLy8vnThxwqqs+XFAQECK2wUA4FGTpqRaavz333/pvQuHmCdS8PC430ivdOnSKly4sHbs2GET6+3bt7V161YVLlzYMsV4apUtW1Y+Pj7au3evzS+N9+7d086dO23qnDhxQnny5FG9evWslt+8eVP79++3ux9fX191795dBw4c0JYtW7RkyRJVrlxZNWvWdCheV0uY4Eq8rGrVqi7dV+LXVPq/QXwvX75st455OS3RAACAqy1dulRBQUE24w0HBASoefPmkqQhQ4ZYDY1y9+5d/e9//7NMSvXiiy9a1e3QoYPKlSunq1evqnfv3rp586ak+9favXv31tWrV1WxYkW1bds22djmz5+v7du364MPPrAae83NzU01a9bU3r17tW7dOknS2bNnNW/ePHl7e6tWrVrOPRkAADzEUp1Ui4mJ0ZkzZyy3hM6dO2e1znw7evSo1bhV5l/m0lt4eLjdLobXrl3TiBEjJN0fH8IcU79+/XTjxg2NGzfOqvzEiRP177//ql+/fg7H7uXlpc6dOysyMlIfffSR1bqZM2faTFIgScWLF9e///6rw4cPW5bFx8frrbfeSjI5JMkyk2i3bt108+bNDG+lJt1vdZfwV9RLly7p448/loeHh7p16+bw9nbs2GG363B4eLi+/vpreXh4qFmzZpbl5sTkN998o3/++ceqzqZNm7R582YVKFBA5cuXdzgWAACQ9ezYsUMBAQGW28KFCyXdv95LuNwVY6jeuHFDp0+ftrkGke7PWB4YGKiLFy+qUaNGKlq0qKpVq6aAgADLzOT9+/e3tEwzc3d315IlS5Q7d24tW7ZMhQsX1hNPPKEiRYpo2bJlypMnjxYtWiQ3t6Qv769fv65hw4apTp066tGjh836kJAQmUwmtW7dWpUqVdLjjz+uyMhIDRkyRLlz507jswIAwMMn1WOqffLJJzZJJ+l+t86UZv00mUwyDCNVMxG5QmhoqGbOnKnGjRurePHiyp49u06fPq3Vq1frxo0b6tChg1Vi55133tGKFSs0efJkHThwQDVq1NDBgwe1du1aVa1aVe+8845TcUyaNEkbN27UqFGjtH37dlWrVk1HjhzRmjVr9PTTT2v9+vVW5QcOHKj169erfv366ty5s3x8fBQWFqZz584pODjY0mU1sYoVK6pu3brauXOnfHx81L17d6fidaXHHntMFStWVIcOHRQbG6vFixcrMjJS77//vh577DGHt/f666/r8uXLqlevngIDAxUXF6ejR49q/fr1MgxDH3/8sYKCgizla9eure7du+vbb79V+fLl1a5dOxUsWFBHjx7VypUrJUlTp06Vu7u7qw4ZAABkYrGxsbp69arN8ps3b1pafUn3f9BMT8WLF9fBgwf16aefasWKFZbJC3Lnzq369eurX79+Ngk1s4oVK+rgwYMaN26c1q5dq99//1358uVT586dNXr0aBUtWjTZfYeEhOjSpUtatWqV3R+MmzZtqsWLFyskJERHjx5VgQIFNGzYMI0cOdIlxw4AwMPGZKRyEIiQkBCFhIQ4vyOTSQ0aNEgyMeRK27dv16xZs7R7926dP39eN2/eVJ48eVS9enX16NFDXbt2tbmQiI6OVkhIiJYuXaqLFy+qYMGC6tixo8aMGZOmMcDOnDmjd955R+vWrdPdu3dVo0YNjR8/Xps2bVJISIg2b96s4OBgS/lly5ZpwoQJ+uuvv5QtWzY1adJEEydO1Lhx4zR37lxFRERYJY/MvvnmG7388svq3r275s+f73S80v1Zoy5evGjTbTWhU6dOqUSJEurZs6dCQ0Mty4ODg7VlyxbdvHlTo0eP1vfff6/Lly+rdOnSeuONN9S3b1+nYpo/f75++OEHHThwQJGRkYqPj1ehQoVUr149DRgwQHXq1LGpc+/ePc2cOVNz587V77//rps3bypv3ryqW7eu3nrrLZtutimJiYmRv7+/avcaIQ8vn5QrAACADLVt+v8yOgQAAJABzN/fo6Oj07WBl8NJNXMyKmG1lLpGGoYhk8mkb7/9Vs8//3wawkVSXnvtNX311VfasmWLGjZsmNHhPJRIqgEAkLWQVAMA4NH0oJJqqe7+aWYvB5dSXi5fvnx69913Sailk8uXL2vevHl6/PHHSagBAAAAAAA8AKlOqvXq1cvSTdEwDDVp0sTSQm3BggUqWLCgTR0vLy/ly5fPavpwuM7q1au1f/9+LV26VP/9959lencAAAAAAACkr1Qn1YoXL67ixYtbLTN366xTp44CAwNdHlxmEx4eruXLl6dYLigoSL169Ur3eJYsWaK5c+eqcOHCmjBhgrp06WK3XGhoqE6dOpXi9tq2bauqVau6NsgknDp1ymostqTkypVLQ4YMSfd4AAAAAAAAHOFw90+zOXPmWO4HBAS4JJjMLjw8PFWTNTRq1OiBJNVCQ0NTlZgKDQ3Vli1bUiwXFBT0QJNqqXkuixcvTlINAAAAAABkOqmeqAB41DFRAQAAWQsTFQAA8GjKtBMVJOXy5cv6559/dOPGjWQnLmAgfQAAAAAAAGR1aU6qzZkzR1OmTNHRo0dTLGsymRQXF5fWXQIAAAAAAAAZKk1Jtbffflsff/yxJCXbOg0AAAAAAAB4mDidVNu7d68++ugjmUwmSbL8TQpJNwAAAAAAADwsXDL7Z3IJM5PJREINAAAAAAAADxU3Zyvu3r1b0v2EWsWKFbV3717LOpPJpA0bNmjy5Mny8PBQ4cKFtXnzZp08eTLtEQMAAAAAAAAZzOmWaqdOnZJ0P4E2bNgwVa9e3Wp9qVKl1KRJE127dk2TJk1S7969deDAgTQFCwAAAAAAAGQGTrdUu379uuV+xYoVbdbHx8dLktq1aydJOn36tCZNmuTs7gAAAAAAAIBMw+mkmq+vr+V+zpw5JUk+Pj6WZVeuXJEk5ciRw7Lsxx9/dHZ3AAAAAAAAQKbhdFItT548lvv//vuvJClXrlyWZStWrJAkrV+/XtL9sdfOnj3r7O4AAAAAAACATMPppFq+fPks9y9fvizp/jhq0v0E2qRJk1StWjW98847MplMkiRPT8+0xAoAAAAAAABkCk4n1RKOo3bs2DFJUoMGDSTdn7wgPj5eBw8eVGxsrAzDkMlkUrVq1dIYLgAAAAAAAJDxnE6qmRNkhmFo5cqVkqQ+ffrIw+P+hKImk8lyMxs4cGBaYgUAAAAAAAAyBQ9nKz777LO6d++eJMnLy0uSVLJkSX3++ed6/fXXLbN/mg0bNkzt27dPQ6gAAAAAAABA5uB0Uq148eIaPHiwzfKXXnpJDRo00JIlS3Tu3Dnly5dP7dq1U40aNdIUKAAAAAAAAJBZOJ1US87jjz+u0aNHp8emAQAAAAAAgAzn9JhqAAAAAAAAwKPK6ZZqv//+u+bMmSPp/qQEY8aMkZ+fn1WZ6OhojRs3ToZhSLo/kUHCWUMBAAAAAACArMjppNp3332nTz/9VCaTSa1bt7ZJqEmSv7+/Tp8+rR9//FGS5O3trYkTJzofLQAAAAAAAJAJON39MywszHL/hRdeSLJct27dLC3VEtYBAAAAAAAAsiqnk2pnz5613K9cuXKS5R5//HG7dQAAAAAAAICsyumk2tWrVy333d3dkyxnXmcYhq5cueLs7gAAAAAAAIBMw+mkmq+vr+X+wYMHkyx36NAhy30fHx9ndwcAAAAAAABkGk4n1QoVKiSTySRJmjx5smJjY23KxMbGavLkyVZ1AAAAAAAAgKzO6aRarVq1LBMQ7N+/X40bN9Yvv/yiq1ev6urVq1q/fr2aNGmivXv3SpJMJpOefPJJ10QNAAAAAAAAZCAPZyt27dpV8+bNk3R/vLRdu3apRYsWNuVMJpMl+da5c2dndwcAAAAAAABkGk63VGvRooWefPJJGYZhSZzZu0n3E2s1a9ZUq1atXBY4AAAAAAAAkFGcTqpJ0vfff6+iRYtaEmv2boZhqHDhwvr+++9dFTMAAAAAAACQodKUVAsKCtLu3bvVvn37JFuqtW/fXrt371aJEiVcFTMAAAAAAACQoZweU82scOHCWrp0qc6fP6+wsDCdP39ehmGoSJEiCg4OVuHChV0RJwAAAAAAAJBppDmpZla4cGF169bNVZsDAAAAAAAAMq00df8EAAAAAAAAHkWpaqm2detWy/3atWvLy8vLapkjGjZs6FQ9AAAAAAAAILNIVVItODhYJpNJkhQREaHAwECrZallMpkUFxfneJQAAAAAAABAJpLqMdUMw7CbRDMMw6UBAQAAAAAAAJldqpNqSbVKS21rNZJveFis+2yY/Pz8MjoMAAAAAACQgRxqqZaaZQAAAAAAAMDDLlVJtYiICMv9okWL2iwDAAAAAAAAHiWpSqoVL148VcsAAAAAAACAR4FbRgcAAAAAAAAAZDUk1QAAAAAAAAAHpar7Z58+fVyyM5PJpFmzZrlkWwAAAAAAAEBGMRmpmMLTzc1NJpMpTTsyDEMmk0nx8fFp2g6QUWJiYuTv76/o6Gj5+flldDgAAAAAAMCOB/X9PVUt1cxSkX8DAAAAAAAAHnoOJdWSkrAVmznxlrhlGwk5AAAAAAAAPCxSlVRr2LBhkt0/9+zZo1u3bskwDOXJk0fFixeXYRg6c+aMrl27JpPJJA8PD9WtW9elgQMAAAAAAAAZJVVJtbCwMLvL33//fW3ZskX+/v6aPXu22rZta0m+GYahH3/8UX369NH169dVuXJlffbZZy4LHAAAAAAAAMgobs5W3Lx5s0aPHi2TyaQPP/xQ7dq1s2rNZjKZ1L59e02ZMkWGYejzzz/XihUrXBI0AAAAAAAAkJGcTqp99NFHlnHSateunWS5OnXqWO5PmzbN2d0BAAAAAAAAmYbTSbXffvvNcv/EiRNJlvv7778l3e8OeuDAAWd3BwAAAAAAAGQaTifVbty4IZPJJMMwNHToUJ05c8amzOnTp/X2229buoXevHnT+UgBAAAAAACATCJVExXY89hjj+nIkSMymUw6ceKESpUqpcaNG6tEiRIymUw6efKkNm/erPj4eKs6AAAAAAAAQFbndFKtS5cuGjNmjKUVWlxcnDZs2GBVxjAMS2s2k8mkrl27pi1aAAAAAAAAIBNwuvvn22+/rQoVKlgSZubkWcJbwtlAy5cvr7feesslQQMAAAAAAAAZyemkmo+PjzZt2qTg4GCrJFrCm3l5w4YNtWHDBvn4+LgydgAAAAAAACBDOJ1Uk6R8+fJp06ZN+umnn9S5c2cFBgbK29tbXl5eKlasmDp16qSffvpJYWFhKlCggKtiBgAAAAAAADKUyTAMI6ODALKCmJgY+fv7Kzo6Wn5+fhkdDgAAAAAAsONBfX9PU0s1AAAAAAAA4FFEUg0AAAAAAABwkEdaN7B582YtWLBAhw4d0r///qu4uLgky5pMJp04cSKtuwQyVL3JE+Xu453RYQAA0ih81NiMDgEAAABZmNNJNcMw1LdvX82dO9fyOCUmk8nZ3QEAAAAAAACZhtNJtc8//1yhoaGWxyklzJgPAQAAAAAAAA8Lp5Nqc+bMkfR/yTSSZgAAAAAAAHhUOJ1UO3bsmEwmkwzDkMlkUuvWrVWuXDn5+vrK3d3dlTECAAAAAAAAmYrTSTUPj/tVTSaTxo0bp5EjR7osKAAAAAAAACAzc3O2YoUKFSxdPtu1a+eygAAAAAAAAIDMzumkWo8ePSz3IyIiXBIMAAAAAAAAkBU4nVTr37+/6tevL8Mw9MYbb+jYsWOujAsAAAAAAADItJweU238+PGqXr26duzYoRMnTqhChQpq0qSJypUrp7x58yZZb/To0c7uEgAAAAAAAMgUTIZ5YDQHubm5yWQySZJlbDXz4+TEx8c7szsgw8XExMjf318VRw6Xu493RocDAEij8FFjMzoEAAAApAPz9/fo6Gj5+fml236cbqmWUOLkWkrlAAAAAAAAgKzMJUk1s+SSZk42iAMAAAAAAAAyHaeTaoGBgbQ8AwAAAAAAwCPJ6aTaqVOnXBgGAAAAAAAAkHW4ZXQAAAAAAAAAQFZDUg0AAAAAAABwEEk1AAAAAAAAwEGpHlOtSZMmad6ZyWTSxo0b07wdAAAAAAAAICOlOqkWFhaWptk+DcNgtlAAAAAAAAA8FBye/dMwDId3QjINAAAAAAAADxOHk2okyAAAAAAAAPCocyip5kwrNQAAAAAAAOBhk+qk2r1799IzDgAAAAAAACDLcMvoAAAAAAAAAICshqQaAAAAAAAA4CCSagAAAAAAAICDSKoBAAAAAAAADiKpBgAAAAAAADiIpBoAAAAAAADgIJJqAAAAAAAAgINIqgEAAAAAAAAOIqkGAAAAAAAAOIikGgAAAAAAAOCgB5JUMwxDa9euVZcuXR7E7gAAAAAAAIB05ZGeGz9y5IhCQ0P17bff6uLFi+m5KwAAAAAAAOCBcXlSLSoqSt9//71CQ0O1d+9eSfdbqkmSyWRy9e4AAAAAAACAB84lSTXDMPTzzz8rNDRUK1eu1J07dyyJNOl+Mi3hYwAAAAAAACArS1NSzV73zsTJtITLvLy80rI7AAAAAAAAIFNwOKmWXPdOyTaRVrBgQbVs2VJt2rRRs2bNXBEzAAAAAAAAkKFSPfunefbOwoULa8CAAdqzZ48Mw5BhGDKZTDbJNLNdu3Zp5syZatu2rbJnz+7a6AEAAFwgIiJCM2bMUP/+/VWlShV5eHjIZDJp/PjxSda5ePGi5s2bpwEDBqhWrVry9vaWyWRSv379nI4jKipKixYt0tChQ1W/fn1ly5ZNJpNJTz31VLL1goODLddjyd169+5tU3fq1KkqVaqUvL29VaZMGX399ddJ7ufChQvy8/NTp06dnD5GAACAh0WqW6q1bt3aamy0hJMOGIYhNzc3BQcHq3v37urbt6/rIwUAAEgnn332mT777DOH6ixcuFBvvPGGS+MICwtT165dHa5XqVIlxcXF2V1379497dq1S5JUp04dq3VfffWVBg8eLB8fH5UrV07Hjh3Tq6++qtjYWA0cONBmW2+99Zbi4+P10UcfORwjAADAw8bh7p+JW6RVqlRJ3bt3V7du3VSkSBFJIqkGAACylICAALVp00a1atVSzZo1NXPmTC1btizZOn5+fmrWrJlq1aqlWrVqacOGDZo2bVqa4vD19VXDhg0t2/z77781YsSIFOslt98NGzaoWbNm8vb2tmphFh8fr7Fjxypnzpzat2+fSpcuraNHj6pGjRoKCQnRq6++Kg+P/7tU3LZtmxYsWKD33ntPgYGBaTpOAACAh4HDSTVzd89nn31W77//vipUqJAecT1ygoKCJEmnTp3K0Dgc0atXL82dO1cRERGW+MPCwtS4cWONGTNGY8eOzdD4AABIrVGjRlk9XrhwYYp1+vTpoz59+lge79+/P81xNG/eXM2bN7c8Dg0NTfM258+fL0lq06aNcufObVl+5swZRUZGqlevXipdurQkqWzZsurQoYPmzZunv/76SxUrVpR0PwE3cOBAlSxZUm+//XaaYwIAAHgYpHpMNTNzF9CVK1eqT58++uKLL3TlypX0iC1DBQUFJTkeySuvvJLR4aWr0NDQZMdjCQ4OztD4Dh48aGkZ6e3trcKFC6tly5bavHmzVbnIyEhNnDhRHTt2VIkSJazG/gMA4FFw8+ZN/fDDD5KkF1980WpdZGSkJKlAgQJWywsVKiRJio6Otiz78ssvdfDgQX366afy9vZOz5ABAACyDIdbqkn/l1jbu3ev9u7dqzfffFPNmzdXjx499Mwzz7g6xgzj7++vIUOG2Cx/4oknHnwwGaBp06aqX7++zXJzq7SJEydq+PDhlm6/D8K8efPUp08f+fv7q02bNipSpIiuXLmivXv3aufOnWrcuLGl7J9//qkRI0bIZDKpdOnSypYtm27evPnAYgUAIKMtX75cN27cUN68edWqVSurdeYunMeOHbNafvToUUn3Z3CXpMuXL2v06NFq1aqV2rRp8wCiBgAAyBpSnVR75pln9PPPPys2NlaS9dhqsbGxWr16tVavXi0/P7/0iTQD5MqV65HuwvjUU09p+PDhSa4vVKiQ5dfsB2Hfvn3q27evatWqpdWrV1t1YZFkM0Dz448/ri1btqhatWrKmTOnypUrZ/miAADAo8Dc9bNLly7y9PS0WleoUCFVrFhRK1as0KJFi9SqVSutWbNGK1euVMmSJVWyZElJ0vDhw3Xr1i2HJ3IAAAB42KW6++dPP/2kc+fO6aOPPlKVKlVkGIZlfLWECbbo6GirLnZTp07V3r17XR95FvXTTz+pZs2a8vX1VYECBdS/f3/9+++/dsseO3ZM77zzjqpXr668efPKx8dHZcqU0fDhw3Xjxg2rso0aNZKnp6cuXLhgd1udO3eWyWTSgQMHXHYsvXr1kslkSnIcuK1bt6pRo0bKkSOH8uTJo27duumff/5xen+jRo1SfHy85s2bZ5NQk2Q1mLJ0vztLw4YNlTNnTqf3CQBAVnXp0iX98ssvkmy7fpp9/PHHMplM6tq1q/z8/NS1a1eZTCbLxAe//fab5syZo6FDh6pUqVKS7o+vduHCBd2+ffvBHAgAAEAm5dCYagEBAXrjjTd04MABHThwQAMHDlRAQIDdBJvZJ598oieffFKFCxdW//79XRp8ertz547mzp2rCRMm6KuvvtLBgwfTtL158+apbdu2OnbsmF588UX17NlTO3bs0FNPPaW7d+/alP/hhx80a9YsPfbYY+rZs6deeeUV5cmTRx988IGaNWtmaTUoSS+//LLi4uI0Z84cm+1cuXJFP/30k2rUqKFq1aql6RhSa/fu3WrWrJny5s2rQYMGqVatWvr+++9Vt25dXbp0yeHtRUVFaf369apWrZpKlSqlLVu2aMqUKfrkk0+0c+fOdDgCAACytu+//17x8fEqXbq0ateubbdMs2bNtG3bNvXo0UONGze2XJu0bNlShmFowIABKlq0qEaOHCnp/iyj+fPnV+HCheXv76/+/fvrzp07D/KwAAAAMg2nxlSTpCpVquizzz7TRx99pFWrVik0NFRr16612z1Uki5evKjZs2drxowZLgj7wbh48aJ69epltaxFixaaP3++AgICHNpWTEyMBg4cqOzZs2vPnj0qU6aMJOn999/XU089pQsXLqh48eJWdV588UW9+eab8vLyslo+btw4jRkzRosXL9YLL7wgSerQoYMGDRqk2bNn691337VKbs6fP193795Vv379HIp5w4YNdn+FfuWVVyzjrCRl3bp1mjlzpvr27WsT94gRIzRr1iyHYtm/f7/u3bunYsWK6dlnn9XKlSut1jdr1kxLliyRv7+/Q9tNzp07d6y+KMTExLhs2wAApDdz18+kWqmZ1a5d227SbebMmdqzZ48WLVqkbNmy6dtvv9WgQYNUr1499evXT1u2bNHMmTMlKUtd3wEAALiKw7N/Jubh4aG2bdtq+fLlKXYPzUr69OmjsLAwXb58WTExMdq9e7datmypn3/+Wc8++6wlWZhay5cvV0xMjPr06WNJqEmSp6en3n//fbt1ihQpYpNQk6QBAwZIup/0MvP29lbPnj114sQJm1kwZ82apWzZsqlbt24Oxbxx40aFhITY3C5evJhi3bJly6pPnz5Wy95++23ly5dP33//vd2Weckxz1C2atUq/fbbb1q+fLmio6N15MgRPfvss/rll1/00ksvObTNlEycOFH+/v6WW7FixVy6fQAA0suRI0e0f/9+SVL37t0drv/vv/9qxIgRatKkiTp37ixJmjRpkvz9/bV27Vr16tVLc+bMUePGjTVnzhynWqEDAABkdalOqo0bN85yS6rFTnLdQ7Oa0aNHq1GjRgoICFDOnDn15JNPatWqVapfv7527dqlNWvWOLQ9c9fRBg0a2KyrU6eOzXhg0v1WfrNnz1bDhg2VJ08eubu7y2QyKW/evJKk8+fPW5U3J5XMvxpL97thHj58WJ07d7ZMIhEeHq6xY8da3UJDQ232P3HiREtyNOGtatWqKR5vvXr1bJKpvr6+qlGjhm7dumUz01hK7t27J+n+OC5ff/21nnvuOfn5+alcuXJavHixAgMDtWTJEp09e9ah7Sbn3XffVXR0tOXmym0DAJCezK3U6tevrxIlSjhcf9SoUYqKitLUqVMlSdevX9fhw4dVr149q7FKW7Zsqfj4eMbPBQAAj6RUd/8cO3asJUnSq1evFGf5TNw9dM6cOfr555/TFm0Gc3NzU+/evbV9+3bt2LFDrVu3TnXd6OhoSVL+/Plt1rm7u1sSZQkNGjRIn3/+uaXLY6FCheTt7S1JCgkJsRnDpGzZsmrUqJF++OEHXbt2TXny5LEk2BKOZxceHq6QkBCruo0aNbLp6poW9o5Tuj95gPR/z0dqmbt1uru72zzv3t7eevrppzVz5kzt27fPZS3KvL29Lc83AABZhWEY+u677ySl3PXTnoMHD2r69OkaNGiQKlSoIEmWCZIST/5jfhwVFZWGiAEAALImh8ZUM3fndGgH/797aNu2bXXlyhWH6mZG5rHUbt686VA9c1LI3I0xofj4eF29elVFihSxLIuMjNQXX3yhypUra9euXcqWLZtl3cWLF22SYmYvv/yytmzZom+//VZ9+vTRokWLVL58edWtW9dSplevXi5NoNlj7zglWbqHODr2WdmyZSVJ2bJlk6enp836XLlySZJu3brl0HYBAHjYbNmyRWfOnJG3t7c6derkcP0BAwYoICBAY8eOtSwrUKCAvLy8dOLECauy5seOjjULAADwMEjzmGqOeBguuH799VdJUlBQkEP1qlSpIknatm2bzbpdu3YpLi7OatnJkydlGIaeeuopq4RaUtsw69ChgwICAjRz5kwtWrRIN27ccHiCAlfYsWOHTbffW7duad++ffL19bUaVy41SpYsqcDAQF2/fl3//POPzfo///xTkuOvCwAAD9rSpUsVFBSk+vXrp8v2v/32W0lSmzZtlDt3bofqzp8/X9u3b9cHH3xg1SvBzc1NNWvW1N69e7Vu3TpJ0tmzZzVv3jx5e3urVq1arjsAAACALOKBJtWyij///NNuN4bt27fr448/lre3t9q3b+/QNs1jgM2ePdtqPLHY2FiNGjXKprx5JtCdO3daxhOTpH/++UfDhw9Pcj9eXl7q2bOnfv/9d40ePVpeXl7q0aOHQ7G6wtGjRzV79myrZVOmTNHly5f1/PPP252AITkmk0mvvPKKJGnEiBFWz8mWLVu0du1aBQUFqWbNmmkPHgDwyNmxY4cCAgIst4ULF0q6P75owuUJx9c8e/as1brJkydLup/USrh8x44dVvu6ceOGTp8+bfdHIklWdQcOHChJ2rp1q934Ert9+7aWLl0qyfGun9evX9ewYcNUp04du9cOISEhMplMat26tSpVqqTHH39ckZGRGjJkiMPJOwAAgIeBQ90/HxWLFy/W5MmT1bRpUwUFBcnb21t//PGH1q9fLzc3N3399dcKDAx0aJv+/v6aOnWqevXqpZo1a6pr167y9/fXqlWr5Ovrq0KFClmVL1SokDp06KBly5bpiSeeUNOmTXXp0iWtWrVKTZo00cmTJ5Pc10svvaSPPvpI58+fV5cuXeyO15benn76ab322mtavXq1ypUrp/3792vdunUqVqyYJkyY4NQ233zzTa1atUrz58/Xn3/+qYYNG+rChQtatmyZvL29NXv2bJsJHxJ2c71w4YLNsg8//PChaEEJAEib2NhYXb161Wb5zZs3rYZ8iI+Pt7pvr86dO3esxj2NjY11KBZ720wc3+3bt+3WXbFihaKjo5U3b161atXKof2GhIRYrjXsDffRtGlTLV68WCEhITp69KgKFCigYcOGaeTIkQ7tBwAA4GFhMlI5Naebm5vlAmvw4MGWMawcNXr0aKfqPUhbtmzRl19+qf379+vSpUu6ffu2ChQooPr16+uNN95IUxeH5cuXa/z48frjjz/k7++vZ599VpMnT1a1atUkSadOnbKUvXHjhsaOHatly5bpwoULCgwMVI8ePTRs2DB5eXmpUaNGCgsLs7ufunXrateuXdqwYYOaNm3qUIyhoaHq3bu3Jk6cmGyruF69emnu3LmKiIiwdLsMCwtT48aNNWbMGDVp0kSjRo3Svn375OXlpRYtWmjy5Mlpmkjg5s2bmjRpkr7//nudOXNGOXLkUKNGjTRmzBhLF9uEUhoDMGHsKYmJiZG/v78qjhwudx8mMACArC581NiMDgEAAADpwPz9PTo6OsWJNtPC4aSaM5MVJJTwF16kj9u3b6tIkSLKlSuX/v777zS9Xvg/JNUA4OFCUg0AAODh9KCSak6NqWYYhlM3PBizZ8/WtWvX9PLLL5NQAwAAAAAASAdOjanmTKKGpFr6mzRpki5fvqzp06crf/78loH9AQAAAAAA4FpOJdVIkN0XGhpqNQZaUtq2bauqVaumezzvvvuuvLy8VKVKFU2dOjVdmzi6QmZ7/gAAAAAAAFLL4aSayWTSggULVLBgwfSIJ0sJDQ3Vli1bUiwXFBT0QJJCWS3ZmdmePwAAAAAAgNRyKKlmnqSgTp06CgwMTK+YsoykZt5E6vD8AQAAAACArMqpiQoAAAAAAACARxlJNQAAAAAAAMBBDiXVnJn1EwAAAAAAAHjYOJRUy2oD4QMAAAAAAADpIdUTFWzevNlyn5k/AQAAAAAA8ChLdVKtUaNG6RkHAAAAAAAAkGUwUQEAAAAAAADgIJJqAAAAAAAAgINIqgEAAAAAAAAOIqkGAAAAAAAAOIikGgAAAAAAAOAgkmoAAAAAAACAg0iqAQAAAAAAAA7ycLbi1q1bLfdr164tLy8vlwQEAAAAAAAAZHZOJ9WCg4NlMpkkSREREQoMDLRb7ty5c2rQoIEkyWQy6cSJE87uEgAAAAAAAMgUnE6qSZJhGJbEWlLi4uJ06tQpSUqxLAAAAAAAAJAVpGlMtdQkyeLi4tKyCwAAAAAAACDTSVNSzTCMFMscPHgwLbsAAAAAAAAAMp1Ud/+cO3eu5s6da3dd165d5ePjY7P81q1bCg8Pl8lkkmEY8vb2dj5SAAAAAAAAIJNIdVLt1KlTCgsLs+nyaRiGfv311yTrJRx3LSgoyLkoAQAAAAAAgEwkTd0/U8OcUDOZTGrTpk167w4AAAAAAABIdw7P/mlvHLXUjK1WvXp1jRo1ytHdAQAAAAAAAJlOqpNqVatWVc+ePS2P586da2mF1r59e+XIkcOmjpeXl/Lly6c6deqoRYsWcnd3d0HIAAAAAAAAQMYyGalpZmaHm9v9nqMmk0kREREKDAx0aWBAZhMTEyN/f39VHDlc7j5MugEAWV34qLEZHQIAAADSgfn7e3R0tPz8/NJtPw53/zRr2LChpaWavZk/AQAAAAAAgIeV00m1sLAwF4YBAAAAAAAAZB3pPvsnAAAAAAAA8LBxuqWa2ebNm7VgwQIdOnRI//77r+Li4pIsazKZdOLEibTuEgAAAAAAAMhQTifVDMNQ3759NXfuXMvjlJjHYAMAAAAAAACyMqeTap9//rlCQ0Mtj1NKmDk5ySgAAAAAAACQ6TidVJszZ46k/0umkTQDAAAAAADAo8LppNqxY8dkMplkGIZMJpNat26tcuXKydfXV+7u7q6MEQAAAAAAAMhUnE6qeXjcr2oymTRu3DiNHDnSZUEBAAAAAAAAmZmbsxUrVKhg6fLZrl07lwUEAAAAAAAAZHZOJ9V69OhhuR8REeGSYAAAAAAAAICswGQ4OcPAvXv3FBwcrO3bt6tUqVJatWqVypQp4+r4gEwjJiZG/v7+io6Olp+fX0aHAwAAAAAA7HhQ39+dHlNt/Pjxql69unbs2KETJ06oQoUKatKkicqVK6e8efMmWW/06NHO7hIAAAAAAADIFJxuqebm5iaTySRJlrHVzI+TEx8f78zugAxHSzUAAAAAADK/TN9SLaHEybWUygEAAAAAAABZmUuSambJJc2cbBAHAAAAAAAAZDpOJ9UCAwNpeQYAAAAAAIBHktNJtVOnTrkwDAAAAAAAACDrcMvoAAAAAAAAAICshqQaAAAAAAAA4CCSagAAAAAAAICD0pxUO3r0qF577TVVqFBB/v7+cnd3T/Lm4eHSyUYBAAAAAACADJGmLNfixYvVs2dP3b17V4ZhuComAAAAAAAAIFNzOql29uxZ9enTR3fu3JEkmUymZMuTdAMAAAAAAMDDwumk2syZM3Xz5k1LMs0wDJlMJpvkmb1lAAAAAAAAQFbm9JhqYWFhku4n03LlyqUPP/zQkjwzmUwaP368WrduLcMwVLhwYU2fPl2zZ892SdAAAAAAAABARnI6qXb06FFJ9xNoo0eP1ptvvmm1vnv37lq5cqXat2+vCxcuaNmyZerRo0faogUAAAAAAAAyAaeTatHR0Zb7devWTbLcgAEDZBiGfvnlF3311VfO7g4AAAAAAADINJxOqrm5/V/VPHnySJK8vLwsy8xJt8KFC1uWzZ0719ndAQAAAAAAAJmG00k1cyJNkm7evClJypEjh2XZr7/+Kkk6fPiwpPtjr5m7jAIAAAAAAABZmdOzf+bNm1fnzp2TJEVGRkqSgoKCdO3aNRmGoeHDh+vw4cNasmSJZQbQu3fvuiZqAAAAAAAAIAM53VKtdOnSlvtnzpyRJNWqVUvS/ckLrl27pqlTp+r8+fMyDEMmk0llypRJY7gAAAAAAABAxnM6qVa9enXL/XXr1kmSnn/+ecsyc+s0k8lkWda9e3dndwcAAAAAAABkGk53/2zatKl+++03SZKvr68kqUGDBurRo4fmzZsn6f8Sa5LUqFEjDRkyJI3hAgAAAAAAABnPZJizXi5iGIZmzJihhQsX6ty5c8qXL5/at2+vAQMGWM0OCmQ1MTEx8vf3V3R0tPz8/DI6HAAAAAAAYMeD+v7u8qQa8LAiqQYAAAAAQOb3oL6/O93909zFU5I6duyobNmyuSQgILNru3SsPLJ5Z3QYALKw9V0nZnQIAAAAANLI6aRar169LJMQBAcHKzAw0G6506dP67HHHpN0f4y1uLg4Z3cJAAAAAAAAZApOJ9Uk2czumVw5AAAAAAAA4GHhlt47uHv3bnrvAgAAAAAAAHigXJJUS6612ubNm12xCwAAAAAAACDTSHX3z88++0yfffaZ3XX16tWTh4ftpm7duqXIyEiZTCYZhsFkBgAAAAAAAHgopDqpFhUVpVOnTtksNwxD//zzT4r1Tab/1959R0dV7X8f/0x6CCSEhE5IaFKlIyC9SZGLKIiAaKjCRZrCvah46UVRFEQF6VW44KUIoiKQ0KtIlVAiTYoRSKElkOQ8f/BkfpnMhGQmCQnwfq01yzn77L3PPpOdI/PNLiaVKlXKrsYBAAAAAAAAOZHdGxUkTfVMvvlAejYrkKSOHTvaezkAAAAAAAAgx7E7qGZrJ8/07O7Zrl07DR8+3N7LAQAAAAAAADlOuoNqjRs3tjgeM2aMeYTaoEGDlDdvXqsybm5uyp8/v+rWrauKFStmqKEAAAAAAABATmEy0jPMzAYnpwcbh5pMJp09e1bFixfP1IYBOU1MTIx8fHzUZO47csnlnt3NAfAY29h5UnY3AQAAAHhiJX1/j46Olre3d5Zdx+7pn0mCg4MlPQiq5c6dO9MaBAAAAAAAAOR0DgfVQkNDzdM/Fy5cqHfeeSfTGgUAAAAAAADkZA4H1S5fvqz4+HhJUq1atTKtQQAAAAAAAEBO5+RowUKFCpl3/SxUqFCmNQgAAAAAAADI6RwOqjVr1sz8/tSpU5nSGAAAAAAAAOBx4HBQ7YMPPpCnp6ckady4cYqNjc20RgEAAAAAAAA5mcNrqrm5uWnatGnq37+/9u3bp/Lly2vo0KGqVq2aihQpImdnZ5vlihcv7nBjAQAAAAAAgJzA4aBaUFCQefdPwzB0/vx5DR48+KFlTCaTeXMDAAAAAAAA4HHlcFAtiWEYFsE1AAAAAAAA4EmX4aBaUkAt5fuUCLgBAAAAAADgSZGhoBqBMgAAAAAAADyNHA6qhYSEZGY7AAAAAAAAgMeGw0G1Ro0aZWY7AAAAAAAAgMeGU3Y3AAAAAAAAAHjcZHijgiQnT57Utm3bdPnyZUlS4cKF1bBhQ5UrVy6zLgEAAAAAAADkCBkOqoWFhal///7aunWrzfONGzfW119/rbJly2b0UgAAAAAAAECOkKHpn7/++quef/55bd26VYZh2HyFhISobt26+u233zKrzQAAAAAAAEC2cjiodu/ePXXq1ElRUVEyDEMmk8nmS5KioqL06quv6t69e5nWcAAAAAAAACC7OBxUW7Jkic6ePWsOnqU2Ui0psHb27FktXbo00xoOAAAAAAAAZBeHg2pr166VJHPwrG/fvjp48KCioqIUHR2tgwcP6q233rIIrK1ZsyZTGg0AAAAAAABkJ4c3KkhaI81kMmnIkCGaMmWKxfmqVatq5syZypUrl6ZOnSpJOnjwoOMtBQAAAAAAAHIIh0eqXbt2zfy+c+fOqebr0qWL+f3169cdvRwAAAAAAACQYzgcVEua0ilJ9+/fTzVf8nPJywAAAAAAAACPK4eDavnz5ze/X7JkSar5Fi1aZLMMAAAAAAAA8LhyeE21mjVr6sKFCzIMQ998843u3LmjPn36qFSpUpKk8PBwffPNN1q6dKl5hFrNmjUzp9UAAAAAAABANnI4qPbyyy9r1apVMplMMgxDixcv1uLFi63yGYYh6cHUz1deecXxlgIAAAAAAAA5hMPTP1977TWVK1dOksyBNVsvk8kkk8mkcuXK6bXXXsu0hgMAAAAAAADZxeGgmouLi1auXCk/Pz+L4FnKl2EY8vPz08qVK+Xs7JyZbQcAAAAAAACyhcNBNUmqWLGi9u3bpzZt2qQ6Uu3FF1/U3r17VaFChcxqMwAAT5SIiAgNGzZMFStWVK5cueTh4aFSpUrprbfe0pkzZ+yq67ffftPIkSPVqFEj+fv7y9XVVQUKFFDr1q21evXqVMstWLAg1T+QJb1++uknq3IJCQkaOXKkAgIC5O7ursqVK2vVqlWpXufw4cNycXHRv/71L7vuCwAAAMhpTEbSomcZ9Oeff2rr1q26fPmyJKlIkSJq2LChAgICMqN6INvFxMTIx8dHTea+I5dc7tndHACPsY2dJ5nfnzx5Ug0bNlRERIRcXV1VsmRJubq66syZM4qNjVWuXLm0YcMGNWrUKM16w8PDVbp0afNxiRIllC9fPv3xxx+KjIyUJAUHB2vevHlycrL8u9qCBQvUo0cPFShQQGXKlLFZ/5QpU1S7dm2LtOHDh2vy5MnKkyePgoKCdOLECSUkJGjNmjVq166dVR0NGjRQeHi4Tp48qTx58qR5TwAAAIC9kr6/R0dHy9vbO8uu4/BGBSkVK1ZMr7/+emZVBwDAU+Htt99WRESE6tWrp+XLl6tYsWKSpOvXr6tnz576/vvv1aNHD4WHh5t3006NYRgqXLiwhgwZojfeeEOFCxeWJCUmJurrr7/WoEGDtHDhQtWsWVMDBgywWUfr1q21YMGCdLX92rVr+uKLLxQYGKi9e/eqYMGC2rZtm5o0aaKRI0daBdUWL16sHTt2aPHixQTUAAAA8NjL0PTPlE6cOKGQkBCFhobqxIkTyqRBcECm6969u0wmk86dO5fdTQHwFLtz545CQkIkSTNmzDAH1CTJz8/PPCXz7NmzCgsLS7O+YsWK6cyZM/r3v/9tDqhJkpOTkwYMGKC+fftKkmbPnp0p7T969KhiY2PVo0cPFSxYUJLUsGFD1a9fX4cPH9bNmzfNeW/evKnhw4erfv366tatW6ZcHwAAAMhOGQ6q/fXXXxowYIB8fX1VqVIlNW/eXM2aNVOlSpWUL18+DRgwQFeuXMmMtuYYoaGhMplM6t69e4byPElsrb3j6empsmXLaujQofr777+zu4kAkOPcu3dPiYmJkqSSJUtanff19VW+fPkkSfHx8WnW5+HhoVy5cqV6/oUXXpAknTp1ypHmWomIiJAkc0AtSVJALyYmxpw2evRoRUREaPr06ZlybQAAACC7ZWj6544dO9S+fXtFRkbaHJUWHR2tGTNmaNmyZVq9erUaNmyYkcshh/Pz87OYTnT9+nWFhobqs88+09q1a/Xbb78x3QcAksmbN68CAgJ08eJF7dq1Sy1atLA4f/LkSV2/fl158+ZNdZ0ze8TGxkqSPD09U81z+PBhde3aVVevXpW3t7eqVaumbt26qVSpUlZ5ixcvLsk6SHfy5Em5uLjIz89P0oOR7NOnT1ffvn1VtWrVDN8HAAAAkBM4HFQ7f/682rRpo1u3bklSquu8GIahyMhItW3bVkeOHFFQUJCjl0QO5+/vr9GjR1ukGYahf/zjH/rhhx/03XffqUePHtnTOADIocaPH6/g4GD17NlTU6dOVePGjeXi4qI9e/ZoyJAhMplMmjx5sjw8PDJ8rRUrVkiS6tWrl2qeQ4cO6dChQ+bjtWvXaty4cRozZoxGjBhhkbdKlSoqUKCA5s6dqzZt2qh27dqaN2+eDh06pKZNm5rbPHDgQPn4+Gj8+PEZvgcAAAAgp3B4+ue4ceN069Yt81Q/wzBsvpKCbbdv39aECRMyreGPo6CgIAUFBSkyMlJ9+vRRwYIF5enpqeeee07ff/+9Vf7Ro0fLZDIpNDRUs2fPVsWKFeXh4aHixYvr/fffN484SOnIkSPq3LmzChcuLDc3NwUGBmrgwIG6fv26Rb5z586Zp6iGhYXplVdekb+/f6auNWYymdSyZUtJspoCGhISop49e6ps2bLKnTu3cufOrZo1a2rWrFmp1tW4cWP9/fff6tmzpwoUKCBPT0/VqVNHoaGhNsscP35cbdu2VZ48eeTj46M2bdro2LFjmXJvAJAZ3nzzTf3vf/+Tv7+/OnbsKH9/f+XNm1etWrWSm5ubNmzYoD59+mT4Ohs3btSaNWskSf/617+szufNm1cDBw7Uzp079ddffyk2Nla//fab3njjDSUkJOjDDz/Ul19+aVEmV65cmjRpkmJiYtSiRQt5e3tryJAhyp07t6ZMmSJJWrlypTZv3qyJEyfK19dXknT//n1duXJF9+7dy/B9AQAAANnF4ZFqP//8szmYJj34a3WPHj1UokQJGYahc+fOaf78+Tp8+LA5308//ZRpDX9c3bt3T82bN9fdu3cVHBysqKgoLV++XO3bt9fixYtt7qA6ZcoUhYaG6rXXXlPbtm21YcMGffTRR/rtt9/0448/WowS/P7779WpUyc5OzurXbt2CggI0O+//64vv/xSP//8s/bu3Wv+UpPkzJkzqlOnjipWrKjg4GDduHFDbm5umXbPv/zyiySpevXqFukff/yx+dovv/yyoqKi9NNPP6lv3746efKk+QtZclFRUapXr568vb31+uuvKyIiQv/973/VsmVL/frrr6pUqZI577Fjx1SvXj3dunVLr7zyisqUKaN9+/apXr16qlKlSqbdHwBkhGEY+uOPP3T9+nU5OzurRIkScnNz05kzZ3Ts2DHNmjVLzz33nHltNUdcuHDB/P+X/v3721yOoX379mrfvr1FWtWqVbVo0SL5+flp6tSp+vDDDxUcHGwxlb9nz54qUqSI5s+fr7///lvPPPOM3nnnHZUtW1Z37tzRsGHDVLNmTfXq1UuGYejDDz/UtGnTdPv2bXl5eWnQoEGaMGFCmjubAgAAADmNw0G1pFFHJpNJ7dq10+rVq63yDBo0SC+99JLWrVsnSbp27Zqjl3tiXLlyReXLl9eePXvk6uoqSXr33XdVvXp1DRgwQO3atbNad2zTpk06cOCAKlasKEmaMGGC2rRpo59//llLlizRG2+8IenBGmZvvPGG8ufPr507d5rXupGkZcuWqWvXrho5cqTVItE7d+7Uf/7zH40dOzZD93bt2jWL6Z+RkZEKDQ3V77//rsGDB6t58+YW+WfMmKESJUpYpMXHx6tNmzaaNm2aBg8ebHEP0oO1fvr376/p06fLyenBQMumTZuqd+/e+vLLLzVz5kxz3gEDBigmJkZLliyxCFZ+8MEHmjRpUpr3ExcXp7i4OPNx8gW3ASCz9OvXT7NmzdLzzz+vbdu2mZdJiIiIUK9evbR69WqFh4fr4MGDcnZ2trv+GzduqHXr1rp27ZoaN26szz77zO46xowZoxkzZig6OlpbtmzRSy+9ZHG+VatWatWqlVW5CRMm6OLFi1qxYoWcnJw0fvx4TZw4UW3btlXHjh21atUqTZo0SV5eXlZTSwEAAICczuHpn/nz5zePUnvnnXdSzTd06FBJD4JvBQoUcPRyT5Rx48aZA2qSVK5cOfXs2VNRUVFau3atVf433njDHFCTJBcXF02cOFGStHDhQnP6okWLFBMTo0mTJlkFo7p06aLq1atr+fLlVvUXKlRIH374YYbv6/r16xozZoz59cUXX+jIkSOqW7euOnfubJU/ZUBNenBv/fr1U0JCgkJCQqzOe3l56eOPPzYH1CQpODhYLi4u2r9/vzntwoUL2rp1qypXrmw1+u+DDz5Q3rx507yfSZMmycfHx/wKCAhIswwA2OPw4cOaPXu2XF1dtXz5cot1RwsUKKClS5fK399fR44cMa+HZo9bt26pTZs2+v3331WjRg19//33cnd3t7seb29v8/+Hzpw5k64y4eHhmjJlirp3767atWvr/v37mjJlikqXLq21a9cqODhYq1evVunSpTVlypR07W4KAAAA5CQOB9WaNWtmfp87d+5U83l5eZnfp9zV7Gnk6uqqOnXqWKU3aNBAkiwWh055LrmaNWvK09PTIv+ePXvM/x09erTVKzY2VteuXbMaMVilSpVMme5ZtmxZi/X0rl+/rvXr1+vSpUtq3Lixtm/fbpH/5s2bGjVqlKpUqaLcuXOb1+fr0KGDJOny5ctW1yhTpoxVf3NxcVHBggUVFRVlTjt8+LAkqX79+lZ15M6dO127z73//vuKjo42vy5evJhmGQCwx86dO2UYhp555hmbgXtvb28999xzkqQDBw7YVXdcXJxeeukl7d27VxUqVNBPP/2UoR2Yk/4YlN7g1+DBg+Xh4aGPPvpIkhQWFqaoqCi98MIL5j+MODk56YUXXlBkZKROnjzpcNsAAACA7ODw9M/33ntPy5cv1/379/Xdd99ZrZeV5LvvvpP0ILj2wQcfOHq5HCXpy0BiYmKqeZLOJR9RJUl+fn5WaZJUsGBBSVJ0dLTVudRG+BUoUECXLl0yH9+4cUOS9NVXXz2s+bp9+7b8/f2trp3Z8uXLpxdffFGenp5q1qyZRo0apS1btkh6sLZc48aNdfDgQVWrVk1vvPGG/Pz85OLionPnzmnhwoUWUy+T+Pj42LyWi4uLEhISzMdJn2Nqn1167tnd3d2hER0AkF43b95MM0/SqPDUNqexJT4+Xp06ddKWLVtUsmRJ/fLLLxbPfXslJCSYg17FihVLM//69ev1ww8/aNq0aebncNJu4SkDe0nHyf8wAgAAADwOHA6qlStXTkuXLlXXrl01efJkxcbGmjcqkKSzZ89q7ty5mj59ury8vPS///1PJUuWzLSGZ6ekwE7K3TSTSxoNljIIdP36dSUmJloF1v766y+b+aUH6+rYEhERYZHf29tbknT06FGLBfvTktWLQyeNsjh48KA5be3atTp48KB69+6t2bNnW+Rfvny5xbRWRyR9Lql9dkmfNwBkpzJlykiSTp06pYsXL1qNVouJiTFPbX/mmWfSVadhGOrevbu+//57FSlSRJs2bVKRIkUy1M65c+cqKipKzs7Oaty48UPzxsXFaciQIapUqZL69+9vTk+6t/DwcIv8SccZCfoBAAAA2cHh6Z/Ozs7q1KmT4uPjlZiYqGnTpqlq1arm9aeqVq2q6dOnyzAM3blzR61atZKzs7PVy8XF4bhetilbtqzc3Ny0f//+VKfB7N69W5JUuXJli/T79++bp2kmlzQ10ta0xJTTJqUH04Du3r1rkb927doW184pkkbQJR/Zl/Qlql27dlb5bd2vvZJ299yxY4fVuVu3btmcZgsAj9oLL7wgf39/3b9/X507d9a5c+fM5yIiIvT666/r2rVr8vDwUMeOHc3npk6dqqCgIJvrVQ4ePNi8FtumTZtsrl+ZUkxMjLp06aJ9+/ZZpCckJGj27NkaPHiwJKlXr14qWrToQ+uaPHmywsPD9eWXX1r8P75o0aIKCAjQunXrdOTIEUkP/gi0bt06FSpUyBxgBAAAAB4XDgfVkq+dZTKZLI7Tey7p9bjx8PBQp06d9Pfff2v8+PFW548ePao5c+YoT548evnll63O/+c//9H9+/fNx2FhYZo3b558fHysdlSTpMWLF+v48ePm4/j4ePNU2uDgYHN6jx49lCdPHo0YMcIif5I7d+7YDOhltalTp0qyXBsuMDBQknXQa+vWrVYj1xxRvHhxNWzYUEeOHNHSpUstzk2cOJFpRgByhNy5c2vRokXy8PDQrl27VLp0aT3zzDOqWLGiAgICtH79erm4uGjmzJkWwayoqCidP39eV69etahv9+7d5h2ePT091adPH9WvX9/mK7nExEQtX75ctWvXlq+vr6pXr67nnntO/v7+euuttxQbG6vWrVtr2rRpD72fCxcu6KOPPlLnzp3VqFEji3Mmk0mjR49WXFycatWqpcqVK6tWrVqKi4vTqFGjbC6NAAAAAORkGRomlnza4MOmEKZ27nEMqCWZMmWK9u7dqzFjxmj9+vVq1KiRPDw8dOrUKX3//fcyDENLly612mWycOHCioqKUtWqVfXiiy8qOjpay5YtU2xsrGbPnm1zEenmzZurTp066ty5s/Lly6cNGzbo2LFjatmypbp162bOlz9/fi1btkyvvvqqqlSpolatWqlcuXKKjY3V+fPntXXrVj3//PP66aefsuQzuXbtmkaPHm0+joyM1O7du7V//375+Pjo448/Np/7xz/+oaCgIE2ePFnHjh1TpUqVdPLkSa1fv17t27fX//73vwy356uvvlK9evX05ptvas2aNSpTpoz279+vffv2qUGDBpkyIg4AMqp169Y6fPiwpkyZoi1btujChQsyDEOFCxdWw4YNNWTIkFTXLU0p+VqUFy9eTPcGK15eXpo8ebJ27dqlY8eOKTw8XHfv3pWfn59efPFFvfnmm3r11VfTXC7g3Xfflclk0qeffmrzfM+ePRUbG6vPP/9cYWFhCgwM1NChQ9WvX790tRMAAADISUyGg5GtzPqLsslkslhg/nESHR2tzz//XGvWrNGZM2d07949FSpUSA0aNNCwYcNUrVo1i/xBQUGSHqwtNnz4cH3//feKjo7Ws88+q//85z9WUyFHjx6tMWPGKCQkRKdOndK0adMUHh6u/Pnzq1u3bho5cqQ8PT2t2nXy5El98skn2rRpk65cuSIvLy8VK1ZMTZo0Ubdu3VSrVi1J0rlz51SiRAkFBwdrwYIFGfosbH3RcnNzU9GiRdWiRQu9//775vtPcvbsWf3rX//Stm3bdPv2bVWsWFFDhw5VwYIF1aRJE40aNcoiSGcymdSoUSOFhoZaXSup7uRTpyTp2LFjGj58uLZt2yaTyaT69etr8uTJ+vTTT7Vw4UKdPXvWql2piYmJkY+Pj5rMfUcuudjAAIDjNnaelN1NAAAAAJ5YSd/fo6OjzevPZwWHg2oZXUg+ueRTGJ9kqQV+UpM8qJbWwtDIegTVAGQWgmoAAABA1nlUQTWHp38+LYEwAAAAAAAAICVWBQYAAAAAAADslKGNCvDkmjp1arp2yOzevXu61yQDAAAAAAB4Uji8plpyu3bt0uHDhxUZGan4+PiH5h05cmRGL4dHICgoSOfPn08z39O03htrqgHILKypBgAAAGSdHL+mmvQgoNK7d+90L7wvEVR7XNjzMwUAAAAAAHjaOBxUO3jwoNq0aaN79+4pvYPdTCaTo5cDAAAAAAAAcgyHg2rjxo1TXFycTCZTuoJlmTDLFAAAAAAAAMgRHA6q7dy50xxMMwxDTk5O8vf3l6enZ6Y1DgAAAAAAAMiJHA6q3bp1y/y+W7du+uKLL5Q3b97MaBMAAAAAAACQozk5WrBEiRLmKZ3//ve/CagBAAAAAADgqeFwUK1Lly7m93/99VemNAYAAAAAAAB4HDgcVBs6dKgqVKggwzA0cOBAHTt2LDPbBQAAAAAAAORYDq+p5unpqS1btqht27Y6cOCAqlSposqVK6tEiRKpTgU1mUyaO3euo5cEAAAAAAAAcgSHg2qStGnTJp08eVImk0mGYejw4cM6cuSIzbyGYRBUAwAAAAAAwBPB4aDa9u3bFRwcrMTEREkPRqFJMm9ekFzSOQAAAAAAAOBJ4PCaapMmTVJCQoJFmq2A2sPSAQAAAAAAgMeRwyPV9u3bZzE6zc/PT0FBQcqVK5ecnByO1QEAAAAAAAA5nsNBtXv37pnff/jhhxo7dmymNAgAAAAAAADI6RweUvbss8+ap3V27do10xoEAAAAAAAA5HQOB9X69u1rfn/ixIlMaQwAAAAAAADwOHA4qPbmm2+qS5cuMgxDAwYM0Pr169mQAAAAAAAAAE8Fh9dUa9q0qRITE2UymXTlyhW99NJL8vX1VfHixZU3b16bZUwmkzZv3uzoJQEAAAAAAIAcweGgWmhoqHn3T5PJJMMwdOPGDd24ccOcnpxhGDbTAQAAAAAAgMeNw0G1lAiYAQAAAAAA4GmRoaAaa6gBAAAAAADgaeRwUC04ODgz2wEAAAAAAAA8NhwOqs2fPz8z2wEAAAAAAAA8NpyyuwEAAAAAAADA44agGgAAAAAAAGAngmoAAAAAAACAndK9ppqzs3OGL2YymRQfH5/hegAAAAAAAIDslO6gmmEYWdkOAAAAAAAA4LFh1+6fJpPJ4QsRlAMAAAAAAMCTwq6gmkRwDAAAAAAAAEh3UK1hw4YZGqkGAAAAAAAAPCnSHVQLDQ3NwmYAAAAAAAAAjw+n7G4AAAAAAAAA8LghqAYAAAAAAADYiaAaAAAAAAAAYCeCagAAAAAAAICd0r1RAYAH1nQcLW9v7+xuBgAAAAAAyEaMVAMAAAAAAADsRFANAAAAAAAAsBNBNQAAAAAAAMBOBNUAAAAAAAAAOxFUAwAAAAAAAOxEUA0AAAAAAACwE0E1AAAAAAAAwE4E1QAAAAAAAAA7EVQDAAAAAAAA7ERQDQAAAAAAALATQTUAAAAAAADATgTVAAAAAAAAADsRVAMAAAAAAADsRFANAAAAAAAAsBNBNQAAAAAAAMBOBNUAAAAAAAAAOxFUAwAAAAAAAOxEUA0AAAAAAACwE0E1AAAAAAAAwE4E1QAAAAAAAAA7EVQDAAAAAAAA7ERQDQAAAAAAALATQTUAAAAAAADATgTVAAAAAAAAADu5ZHcDgMfN13u6y8PLNbubARuG1PtvdjcBAAAAAPCUYKQaAAAAAAAAYCeCagAAAAAAAICdCKoBAAAAAAAAdiKoBgAAAAAAANiJoBoAAAAAAABgJ4JqAAAAAAAAgJ0IqgEAAAAAAAB2IqgGAAAAAAAA2ImgGgAAAAAAAGAngmoAAAAAAACAnQiqAQAAAAAAAHYiqAYAAAAAAADYiaAaAAAAAAAAYCeCagAAAAAAAICdCKoBAAAAAAAAdiKoBgAAAAAAANiJoBoAAAAAAABgJ4JqAAAAAAAAgJ0IqgEAAAAAAAB2IqgGAAAAAAAA2ImgGgAAAAAAAGAngmoAAAAAAACAnQiqAQAAAAAAAHYiqAYAAAAAAADYiaAaAAAAAAAAYCeCagAAAAAAAICdCKoBAAAAAAAAdiKoBgAAAAAAANiJoBoAAAAAAABgJ4JqAAAAAAAAgJ0IqgEAAAAAAAB2IqgGAAAAAAAA2ImgGgAAAAAAAGAngmoAAAAAAACAnQiqAQAAAAAAAHYiqAYAAAAAAADYiaAaAAAAAAAAYCeCagAAAAAAAICdCKoBeCKtWbNGffv2VY0aNVS4cGG5ubkpb968ev755zVt2jTdu3fPrvquXr2qRYsWacCAAXruuefk7u4uk8mk3r17p6v8iRMn9Prrr6tw4cLy8PBQqVKlNGzYMEVFRdnMn5CQoJEjRyogIEDu7u6qXLmyVq1alWr9hw8flouLi/71r3/ZdV8AAAAAAMeYDMMwsrsRwOMgJiZGPj4+mvTzy/Lwcs3u5sCGIfX+a35fv3597dy5U+7u7ipSpIj8/Px05coVXbp0SZJUo0YNbdq0SXnz5k1X3VOnTtU777xjld6rVy/NmTPnoWVDQkL04osv6u7du8qfP78CAgIUFhamO3fuqGTJktq1a5cKFixoUWb48OGaPHmy8uTJo6CgIJ04cUIJCQlas2aN2rVrZ3WNBg0aKDw8XCdPnlSePHnSdU8AAAAA8CRK+v4eHR0tb2/vLLsOI9UAPJF69+6tkJAQ3bx5U3/88Yf279+vP//8U7t371axYsX066+/asSIEemuz9vbWy1atNCIESO0du1aDRw4MF3lbt68qddee013797VoEGDdOnSJf3666+6cOGC6tWrpz/++EO9evWyKHPt2jV98cUXCgwM1OnTp3XkyBFt3rxZJpNJI0eOtLrG4sWLtWPHDnMQDgAAAACQ9Z6YoNqCBQtkMpm0YMECi3STyaTGjRtnS5vwaJw7d04mk0ndu3fP7qYgB+nevbsaN24sV1fLUYV16tTRZ599JunBFNH06tmzpzZu3Kjx48erXbt2ypcvX7rKzZw5U3///bfKly+vzz77zNwePz8/ffvtt3JxcdEPP/yggwcPmsscPXpUsbGx6tGjh3kEW8OGDVW/fn0dPnxYN2/eNOe9efOmhg8frvr166tbt27pvh8AAAAAQMY8FkG1pKDJw16PWnh4uEaPHq127dqpaNGiMplMCgoKSrPczz//rMaNG8vb21t58uRR48aN9fPPP2eoLSaTSeXKlctQHdkpKCjI/HMMCwuzmSc+Pl6FChUy57t69eojbiWeJEm/L3fu3MnyayWtg9a9e3c5OztbnCtevLiaN28uSfruu+/M6REREZJkNSW0cOHCkh4MZU4yevRoRUREaPr06ZnfeAAAAABAqlyyuwH2KFWqVKojMV5++WXVqVPH/KUzq23fvl1jxoyRs7Ozypcvn64gz9KlS9WtWzf5+/srODhYJpNJK1asUKtWrbRkyRK9/vrrj6DlOZOT04P47rx58zR58mSr8+vXr9dff/0lFxcXxcfHW5wrWrSoTpw4IR8fn0fSVjz+du/eLUmqXr16ll4nPj5ev/76qySpXr16NvPUq1dPP/30k/bu3WtOK168uCTp1KlTFnlPnjwpFxcX+fn5SXqw+cH06dPVt29fVa1aNQvuAAAAAACQmscqqFa6dGmNHj061fOPMqjSsGFD7d69W1WqVJGnp6c8PDwemj8yMlIDBgyQv7+/Dh48qICAAEnS+++/r+rVq2vAgAFq06aNfH19H0XzcxxXV1c1bNhQixcv1sSJE+XiYtk1582bJ39/f5UpU8YcEEle9nEeqYdHIyEhQVeuXNH333+v9957T15eXpo0aVKWXvPcuXO6f/++JKlkyZI28ySlnz592pxWpUoVFShQQHPnzlWbNm1Uu3ZtzZs3T4cOHVLTpk3Nz5uBAwfKx8dH48ePz9L7AAAAAABYeyymf6ZHamuqJbl48aJee+01+fn5ycvLS40bN9auXbscvl7JkiVVp04deXp6piv/ypUrFRUVpYEDB5oDatKD6VxDhgxRVFSUVq5c6XB70uvChQvq1auXihYtKjc3NxUrVky9evXSxYsXLfINGTJEJpNJhw4dskh/8cUXZTKZ1Lt3b4v0H3/8USaTSR9//LHDbevRo4euXr2qDRs2WKRfvXpVP/74o15//XW5ublZlUttTbXGjRvLZDIpPj5e48aNU4kSJeTu7q5nnnlGX3/9tcPtxONl6tSpMplMcnFxUUBAgN5++201a9ZMe/bs0XPPPZel146MjDS/Ty1gnpSePG+uXLk0adIkxcTEqEWLFvL29taQIUOUO3duTZkyRdKDZ8rmzZs1ceJEcx3379/XlStXdO/evay6JQAAAADA//fEBNUeJjIyUvXq1dO5c+f01ltvqUOHDtq9e7eaNGmi0NDQR9KGpOu88MILVudatmwpSdq6dWuWtuH06dOqVauW5s2bpxo1amjo0KGqXr265s2bp5o1a+rMmTPmvE2aNJEkhYSEmNMSEhK0Y8cOq3Tp/+4vqZwjXn75Zfn6+mr+/PkW6YsWLVJ8fLx69uzpUL1dunTR7Nmz9cILL6hXr166ceOG3n77bc2ePdvhtuLxUbRoUdWrV0/PPfeceY2ykJAQLVu2TAkJCVl67djYWPN7WwFhSXJ3d5ck3b171yK9Z8+e+vHHH9WpUyc1adJEffv21YEDB1S1alXduXNHw4YNU82aNdWrVy8ZhqERI0bI19dXRYoUUb58+fTBBx/IMIysuzkAAAAAeMo9VtM/z5w5Y3P6Z6tWrR5a7siRI3rjjTe0cOFC86YGvXr1UpMmTdSnTx+dPHnSvKZXVkma2lWmTBmrc0lpyad/ZYV+/fopIiJC33zzjd566y1z+qxZs9S3b1/169dPmzZtkiQ1atRITk5OCgkJ0TvvvCNJ+vXXXxUTE6NmzZpp8+bNunDhgnntp5CQEOXJk0c1atRwuH0eHh7mAFhERIQKFCggSeYgYOXKlR2q9+LFizp27Ji8vb0lSYMHD1alSpU0ZcoU9enTJ9VycXFxiouLMx8nXxwej49XX31Vr776qvl479696tu3ryZOnKgbN25oxowZWXbt5NPC7927Z3OaeFIfszXqtVWrVjafbxMmTNDFixe1YsUKOTk5afz48Zo4caLatm2rjh07atWqVZo0aZK8vLw0YsSITLwjAAAAAECSx2qkWnh4uMaMGWP12rNnz0PLOTs7a8KECRa7hDZq1Eht2rTRmTNnMjQNNL2io6Ml2V73zcvLS87OzuY8WeHixYvasmWLKlSoYBVI6tOnj8qXL6/Nmzebp4HmzZtXVapU0bZt28yjeUJCQmQymcyBzS1btkh6EGw6ePCgGjRoYLW7ob169uyp+/fva/HixZKknTt36uTJkw6PUpOkSZMmmQNqklS2bFnVq1dPJ0+e1M2bNx9azsfHx/xKPm0Xj6/atWtrw4YNcnd316xZs3T+/Pksu1byKZ/Jp3cml5Se3vUUw8PDNWXKFHXv3l21a9fW/fv3NWXKFJUuXVpr165VcHCwVq9erdKlS2vKlClWG3sAAAAAADLHYxVUa9mypQzDsHoNGTLkoeUCAwNtBkQaNGggSVbrhj2JfvvtN0kPgonJg4uSZDKZ1LBhQ0nS4cOHzelNmjRRdHS0Dh48KOlBUK1KlSqqX7++ChUqZJ4CmhR4y8jUzyRJI9KSpoDOmzdPHh4e6tq1q8N12trhsVixYpKkqKioVMu9//77io6ONr9SrjuHx1eRIkVUtWpVJSYmWvT5zBYUFCRXV1dJ0h9//GEzT1K6rVGstgwePFgeHh766KOPJElhYWGKiorSCy+8YB5x6+TkpBdeeEGRkZE6efJkRm8DAAAAAGDDYxVUc1TSNMKUktZXysoRYkmSRqjZutbt27eVkJCQpbuXJk1dTLrnlAoVKmTVvuTrqsXHx2vnzp3mtMaNG5uDakn/zYygmvRgw4Ljx49ry5YtWrFihdq3b6+8efM6XJ+tzzVpd9GHranl7u4ub29vixeeHEkjuLJyJJeLi4s5qLtz506beZLSa9eunWZ969ev1w8//KCxY8ean2u3bt2SJOXJk8cib9LxwwLHAAAAAADHPRVBtYiICJvpf/31lyTbQZfM9rB10x623lpmSQoIJd1zSknpyQNHDRs2lLOzs0JCQrR//37dunXLHDhr0qSJLl68qPDwcIWGhsrHx0fVqlXLlLZ269ZNbm5uevPNN3Xr1q0MTf0EbDl37px5hFqVKlWy9FqvvPKKpAc7FKcM4l64cMG8jmGHDh0eWk9cXJyGDBmiSpUqqX///ub0pFG44eHhFvmTjv39/TN2AwAAAAAAm56KoNr58+dtTt3bvn27JKlq1apZ3oZGjRpJkjZu3Gh17ueff7bIkxWS7nHbtm1WOwIahmHzs/D29la1atW0Y8cObdy4Uc7OzuZpok2bNpUkrVq1SocOHVLDhg0zbbMHf39//eMf/9ClS5dUvHhxNWvWLFPqxdPj119/1ahRo2xOufzpp5/UunVrxcfHq02bNipVqpT53NSpUxUUFKTOnTtnWlv69esnf39/nThxQu+++67u378vSbp+/bq6du2q+Ph4tW7dOs1NPiZPnqzw8HB9+eWX5pGW0oPdTQMCArRu3TodOXJEknT06FGtW7dOhQoVytJgPQAAAAA8zZ6KoFpCQoJGjBhhEUzaunWrNmzYoNKlS+v555/P8jZ06tRJPj4+mj59ukWA78qVK5o6dary5s1rsUNhZitevLiaNGmi48ePa968eRbn5s2bp+PHj6tp06ZWa881adJEt27d0ldffaXq1aubR/WVLl1axYoV0yeffKLExMRMm/qZ5JNPPtHq1au1evXqLN+ZFU+emzdvauzYsSpVqpQKFy6sWrVqqUqVKvL19VXr1q0VFhamWrVqaeHChRbloqKidP78eV29etWqzosXL8rf39/8mjx5siRpyZIlFukpp3l6e3tr+fLl8vDw0BdffKGiRYuqZs2aKl68uHbu3KmgoCCr38mULly4oI8++kidO3e2Cr4nbR4SFxenWrVqqXLlyqpVq5bi4uI0atQofn8AAAAAIIu4pJ3l8Ve5cmWFhoaqTp06atq0qS5fvqzly5fL1dVVs2fPduhL57Vr1zRs2DDz8f3793Xt2jV1797dnLZgwQLze19fX3355Zd64403VL16dXXu3FlOTk7673//q7/++kuLFy9O9+5/tly5csXi2skVL15cY8eO1YwZM1S/fn316dNH69atU4UKFfT777/r+++/V/78+TVjxgyrsk2aNNEnn3yiv//+Wz169LA6l7RLZ2YH1UqUKKESJUpkap14elSpUkXTpk3T5s2bdfz4cYWFhenevXvy8/NT3bp11alTJ3Xr1s1ixFdaEhISdP36dav0uLg4xcXFmY+TRqIl16xZMx04cEDjx4/Xli1bdPToURUtWlQvv/yyPvzwwzR/9999912ZTCZ9+umnNs/37NlTsbGx+vzzzxUWFqbAwEANHTpU/fr1S/f9AQAAAADs81QE1Xx9fbVu3ToNGzZM33zzjWJjY1WnTh1NnDhR9erVc6jOW7duWY1yuX37tkVa8qCa9GCtMH9/f02aNMl8rnr16lq4cKFatmzpUDuSxMTEWLUnSZUqVTR27FiVLVtWBw4c0JgxY/TTTz/phx9+UP78+dW9e3eNGjVKgYGBVmUbNGggFxcXxcfHWwXOkoJqvr6+qly5cobaD2QmX19fDRo0SIMGDbKr3OjRozV69Gib54KCgqymTtujYsWKWrZsmUNlv/vuuzTz9O/f32KtNQAAAABA1jIZGfmWCDxFYmJi5OPjo0k/vywPL9fsbg5sGFLvv9ndBAAAAABANkv6/h4dHW2xIWNmY7EdAAAAAAAAwE4E1QAAAAAAAAA7PRVrqqVHaGioQkND08xXtWpVtW/fPkvbMnXqVEVFRaWZr3v37goKCsrStjhiwYIFOnfuXJr52rdvr6pVq2Z5ewAAAAAAADIbQbX/LzQ0VGPGjEkzX3Bw8CMJqp0/fz7NfI0bN86xQbWtW7emmS8oKIigGgAAAAAAeCyxUQGQTmxUkPOxUQEAAAAAgI0KAAAAAAAAgByKoBoAAAAAAABgJ4JqAAAAAAAAgJ0IqgEAAAAAAAB2IqgGAAAAAAAA2ImgGgAAAAAAAGAngmoAAAAAAACAnQiqAQAAAAAAAHYiqAYAAAAAAADYiaAaAAAAAAAAYCeCagAAAAAAAICdCKoBAAAAAAAAdiKoBgAAAAAAANiJoBoAAAAAAABgJ4JqAAAAAAAAgJ0IqgEAAAAAAAB2IqgGAAAAAAAA2ImgGgAAAAAAAGAngmoAAAAAAACAnQiqAQAAAAAAAHYiqAYAAAAAAADYiaAaAAAAAAAAYCeCagAAAAAAAICdCKoBAAAAAAAAdiKoBgAAAAAAANiJoBoAAAAAAABgJ4JqAAAAAAAAgJ0IqgEAAAAAAAB2IqgGAAAAAAAA2ImgGgAAAAAAAGAngmoAAAAAAACAnQiqAQAAAAAAAHYiqAYAAAAAAADYiaAaAAAAAAAAYCeCagAAAAAAAICdXLK7AcDjpn+dBfL29s7uZgAAAAAAgGzESDUAAAAAAADATgTVAAAAAAAAADsRVAMAAAAAAADsRFANAAAAAAAAsBNBNQAAAAAAAMBOBNUAAAAAAAAAOxFUAwAAAAAAAOxEUA0AAAAAAACwE0E1AAAAAAAAwE4E1QAAAAAAAAA7EVQDAAAAAAAA7OSS3Q0AHheGYUiSYmJisrklAAAAAAAgNUnf25O+x2cVgmpAOl2/fl2SFBAQkM0tAQAAAAAAabl586Z8fHyyrH6CakA65cuXT5J04cKFLP2lBOwVExOjgIAAXbx4Ud7e3tndHMCMvomcir6JnIq+iZyKvomcKrW+aRiGbt68qSJFimTp9QmqAenk5PRgCUIfHx/+R4Icydvbm76JHIm+iZyKvomcir6JnIq+iZzKVt98FINh2KgAAAAAAAAAsBNBNQAAAAAAAMBOBNWAdHJ3d9eoUaPk7u6e3U0BLNA3kVPRN5FT0TeRU9E3kVPRN5FTZXffNBlZvb8oAAAAAAAA8IRhpBoAAAAAAABgJ4JqAAAAAAAAgJ0IqgEAAAAAAAB2IqgGAAAAAAAA2ImgGp5q+/fvV5s2beTr6ysvLy8999xz+vbbb63y/f7772rRooV8fHxUqlQpffzxx0pMTLTKd+3aNfn7++vdd999FM3HEyooKEgmk8nmq1+/fhZ56ZvIbEuWLFHfvn1Vs2ZNubu7y2QyacGCBanmj4mJ0bvvvqvAwEC5u7srMDBQ7777rmJiYqzyxsbGaujQoQoICJCfn586dOigy5cv26z3zTffVPHixXXr1q3MujU85uzpm6NHj071Oerh4WGVn76JjLh06ZKmTp2qF154QcWLF5ebm5sKFSqkDh06aO/evTbL8OzEo2Bv3+TZiUclKipKgwYNUt26dVWoUCG5u7uraNGiatq0qf73v//J1n6aOfW56eJQKeAJEBoaqpYtW8rNzU2dO3eWj4+PVq1apddff13nzp3TBx98IEm6efOmmjdvrvv376tnz54KCwvTe++9J3d3dw0ZMsSiziFDhih37twaN25cNtwRniQ+Pj5W/UuSatasaX5P30RW+PDDD3X+/Hn5+/urcOHCOn/+fKp5b9++rUaNGunQoUNq0aKFunTposOHD+vzzz9XSEiIduzYIS8vL3P+YcOG6auvvlKnTp2UP39+zZ8/X//4xz+0f/9+OTn939/5Nm/erMWLF2vdunXKnTt3lt4vHh/29M0kwcHBCgoKskhzcbH+5y99Exkxffp0ffzxxypVqpRatGihAgUK6PTp01qzZo3WrFmjZcuWqVOnTub8PDvxqNjbN5Pw7ERWu3btmubNm6c6deqoffv2ypcvnyIiIrRu3Tp17NhRffr00axZs8z5c/Rz0wCeQvfv3zdKlSpluLu7GwcPHjSnx8TEGBUrVjRcXFyMU6dOGYZhGN9++60hydi+fbs5X9OmTY2yZcta1Llx40ZDkrFhw4ZHcxN4YgUGBhqBgYFp5qNvIiv88ssvxrlz5wzDMIxJkyYZkoz58+fbzDty5EhDkvHvf//bZvrIkSPNaQkJCYanp6fRq1cvc9qiRYsMScbu3bvNaXfu3DFKlSplvPbaa5l4V3gS2NM3R40aZUgyQkJC0qyXvomM+t///mds27bNKn3btm2Gq6urkS9fPiM2NtaczrMTj4q9fZNnJx6V+Ph44/79+1bpMTExRoUKFQxJxrFjx8zpOfm5yfRPPJW2bNmi8PBwde3aVdWqVTOn58mTR//5z38UHx+v+fPnS5IuXrwoyXKEUM2aNXXhwgXz8d27d9WvXz917dpVrVu3fkR3gacdfRNZoXnz5goMDEwzn2EYmjNnjnLnzq2RI0danHv//ffl6+uruXPnmofvX7t2TXfv3rXqr5Is+uyYMWN048YNTZs2LTNuB0+Q9PZNe9E3kVGvvPKKGjRoYJXeoEEDNWnSRDdu3NDRo0cl8ezEo2VP37QXfRMZ4ezsbHP0Y548edSyZUtJ0pkzZyTl/Ocm0z/xVAoNDZUkvfDCC1bnktK2bt0qSQoICJAkHTp0SHXq1JEk/fbbbypevLi5zKhRoxQVFaWpU6dmYavxNImLi9PChQt16dIl+fr66vnnn1eVKlUs8tA3kZ1Onz6ty5cvq2XLlhbD7SXJw8NDDRs21Nq1a3XmzBmVKVNG/v7+8vDw0KFDh8z5fvvtN0ky99kjR45oypQp+uabb1SwYMFHdi94cm3fvl379u2Ts7OzypUrp+bNm8vd3d0iD30TWcnV1VXS/02d49mJnCJl30yOZyeyS2xsrLZs2SKTyaQKFSpIyvnPTYJqeCqdPn1aklSmTBmrc76+vvL39zfnadu2rQoVKqSXX35ZXbt21cmTJ/XLL79oypQpkmSeyz1nzhzlz5//0d0EnmhXr15V9+7dLdJatWqlxYsXy9/fXxJ9E9nrYc/R5OmnT59WmTJl5OTkpJ49e2rmzJm6efOm8uXLp4ULF6pq1aqqVauWEhMT9dZbb6lBgwbq2bPnI7sPPNlS/kW7cOHCWrhwoVq0aGFOo28iq1y4cEGbNm1SoUKF9Oyzz0ri2YmcwVbfTI5nJx6VpD/+JyYmKiIiQhs2bNDFixc1atQoi+ehlIOfmxmaPAo8plq0aGFIMk6fPm3zfMmSJQ03Nzfz8dGjR42mTZsaefLkMUqUKGFMmDDBiI+PNxISEoxatWoZzZo1MwzDMH7++WejYsWKhrOzs1GiRAljyZIlj+R+8GQZM2aMERoaavz9999GTEyMsWfPHqN169aGJKNu3bpGYmKiOS99E1npYetWLV261JBkjBgxwmbZsWPHGpKMb7/91px2584dY8iQIUbRokUNX19fo3379sbFixcNwzCMadOmGR4eHsapU6eMiIgIo0OHDoanp6eRO3duo1evXsadO3ey5B7xeEprTbXVq1cbCxcuNM6dO2fcvXvXOH36tDFu3DjD09PT8PDwMA4dOmSRn76JzHbv3j2jYcOGhiRj0aJF5nSenchuqfVNw+DZiUfv7NmzhiTzy9XV1fjkk08svu/k9OcmQTU8lewNqqXm888/Nzw9PY0zZ84Y58+fN9zd3Y2OHTsamzZtMvr372+YTCZj3759md18PIUSEhKM+vXrG5KM9evXp5mfvonMkNlBtdRcvHjRyJMnjzFhwgTDMAyjdevWRtGiRY1Vq1YZixYtMnx8fIy33347Q/eCJ0taQbXUzJo1y5BkdOzYMV356ZtwREJCgtGtWzdDktGnTx+Lczw7kZ0e1jcfhmcnslp8fLxx9uxZY9KkSYabm5vx8ssvmzcyyOnPTYJqeCp17NjRkGQcOHDA5nl/f38jf/78D63jwoULRu7cuY2PPvrIMAzDeO+99wxvb2/j1q1bhmEYRmJiolG6dGmjS5cumdt4PLXmzp1rSDLef//9h+ajbyKzPCxwsX79ekOSMWDAAJtlhw0bZkgyfvjhhzSv065dO+PZZ5817t27Z4SFhRmSjMWLF5vPjx071nB1dTX3YcDRoFpcXJzh4uJiFC5cOF356ZuwV2JiotGzZ09DktGtWzcjISHB4jzPTmSXtPrmw/DsxKM0efJkQ5Lx9ddfG4aR85+b7P6Jp1LK+dnJRUZG6tq1a6nO2U7Sv39/lSpVSkOHDpUkhYWFqWzZsubFE00mk6pVq6awsLBMbj2eVklrqd25c+eh+eibeBQe9hxNnp7Ws/S7777T+vXrNXv2bLm6upr7ZfXq1c15atSoofv37ys8PDwzmo6nmJubm/LkyZPmc1Sib8J+iYmJ6tWrl+bNm6cuXbpowYIFcnKy/LrFsxPZIT1982F4duJRSto4MGlzwZz+3CSohqdSo0aNJEkbN260OpeUlpTHlhUrVujHH3/UnDlzLHbMiYuLs8gXGxsrk8mUGU0GtHfvXklSUFBQqnnom3hUypQpoyJFimjnzp26ffu2xbnY2Fht27ZNRYoUUenSpVOtIzo6WoMGDdKAAQNUu3Zti3PJ+2xsbKwk0WeRYadPn1ZkZORDn6MSfRP2S0xMVO/evTV//ny99tprWrx4sZydna3y8ezEo5bevvkwPDvxKF2+fFnS/+1Mm9OfmwTV8FRq1qyZSpYsqW+//dZiq92bN29q3LhxcnFxsdp5MUlUVJQGDx6sQYMGqWbNmub08uXL6/jx4/rjjz8kPfjF3b59u8qXL5+Vt4InzO+//66oqCir9B07duizzz6Tu7u7XnnlFZtl6Zt4lEwmk3r37q1bt25p7NixFucmTZqkyMhI9e7d+6H/KBk+fLhcXFw0YcIEc1pSv1y3bp05bd26dXJzc1PJkiUz+S7wJLp586aOHDlilR4ZGalevXpJkrp06fLQOuibsEfSKKD58+fr1Vdf1ZIlS1INWvDsxKNkT9/k2YlH6dChQ4qOjrZKv3Hjhj744ANJUuvWrSXl/OemyTAMI925gSdISEiIWrZsKXd3d3Xp0kXe3t5atWqVzp49q/Hjx2vEiBE2y7311lvauHGjjh8/bp5OJ0l//vmnypQpo4IFC+qVV17RL7/8ouPHj+vAgQMWQ0qBhxk9erQmT56sZs2aKSgoSO7u7jp27Jg2btwoJycnzZw5U71797ZZlr6JzDBnzhzt2LFDknT06FEdPHhQ9erVM//1r3379mrfvr0k6fbt26pfv74OHTqkFi1aqEaNGjp8+LB+/PFHVa1aVTt27LDoi8nt3LlTDRo00Pfff6+2bdtanGvbtq1++uknBQcH69atW1qxYoUGDx6sqVOnZtl9I+dLb988d+6cSpQooZo1a+rZZ59VgQIFdOnSJf3444+6fv26WrRoofXr18vNzc3mdeibsNfo0aM1ZswY5c6dW4MHD7YYKZ6kffv2qlq1qiSenXh07OmbPDvxKA0ZMkRz5sxRkyZNFBgYKC8vL50/f14//PCDbt26pQ4dOmjFihXmaco5+rmZ7tXXgCfQ3r17jVatWhk+Pj6Gp6enUbNmTWPJkiWp5t++fbthMpmMDRs22Dy/ceNGo3Llyoarq6tRunRpY/ny5VnVdDyhQkNDjU6dOhmlS5c28uTJY7i6uhrFihUzOnfubOzduzfVcvRNZJbg4GCLrc1TvkaNGmWRPyoqynjnnXeMgIAAw9XV1QgICDDeeecdIyoqKtVr3Lt3z6hQoYLRqVMnm+f//vtvo1OnToaXl5eRN29eo2/fvsbdu3cz8zbxGEpv34yOjjbefvtto0aNGoa/v7/h4uJi+Pj4GPXr1zdmzpxpxMfHp3oN+iYckVbflI1NNXh24lGwp2/y7MSjtH37dqN79+5GuXLlDG9vb8PFxcUoUKCA0apVK+Pbb781EhMTrcrk1OcmI9UAAAAAAAAAO7GmGgAAAAAAAGAngmoAAAAAAACAnQiqAQAAAAAAAHYiqAYAAAAAAADYiaAaAAAAAAAAYCeCagAAAAAAAICdCKoBAAAAAAAAdiKoBgAAAAAAANiJoBoAAAAAAABgJ4JqAAAAAJ4qf//9t3x8fGQymWQymRQcHPzI2xAfH6+SJUua21CvXr1H3gYAQMaYDMMwsrsRAADg6dC4cWNt3bo13fl9fHwUFRWVdQ1yUGhoqEJDQ83HefPm1ZAhQ7KtPdnN1s+Vf2I6hr71aLz99tv6+uuvJUlOTk46fvy4ypUrZ5Fn7969+vjjj7Vz507duHFD+fLlU7169TR8+HDVrl071bqDg4O1aNEi5cqVS8eOHVOJEiVSzfvNN9+oX79+5uOVK1eqY8eOGbw7AMCjQlANAAA8Mk9KUG306NEaM2aM+TgwMFDnzp3LvgZlM4JqmYe+lfVOnjypSpUqKT4+XpL06quvasWKFRZ5Vq5cqS5duighIcGqvLOzs5YtW6ZXX33V6tzmzZvVvHlzSdLkyZP1r3/966FtuXfvnkqWLKlLly5JkkqVKqUTJ07I1dXVoXsDADxaTP8EAAAA8NSYMmWKOaAmSX369LE4f+fOHfXr188cUJs+fbouXLigadOmSZISEhL0z3/+U3fu3LEoFxsbax51VrVqVb3zzjtptsXNzU1vvvmm+Tg8PFzfffedYzcGAHjkCKoBAIBsdfbs2VRfR44cye7mAXiCxMTE6NtvvzUfFypUSE2bNrXIkzTdU5Lq1KmjAQMGKCAgQIMGDdJzzz0nSbp+/bp27dplUW7MmDE6c+aMnJycNGvWLLm4uKSrTa+//rrF8YwZM+y+LwBA9iCoBgAAslVQUFCqr+LFi9ssExERofHjx6tRo0YqUKCA3Nzc5Ovrq+rVq2v48OH6888/U73e3r17NXHiRHXs2FFVqlRRsWLFlCtXLnl4eKhgwYJq2LChRo4cqQsXLliV7d69u0wmk8X0PEk6f/68ebHxpNeCBQvM5x92LmXdSa/GjRtb5bFVT0xMjEaMGKEKFSooV65cMplMVtMFY2NjNWfOHLVr104BAQHy9PRU7ty59cwzz6hXr17at29fqp9XZknt/hYtWqTnn39eefLkkZ+fn1q3bq1t27aZy925c0fjx49XhQoV5Onpqfz586t9+/b69ddfbV5nwYIFVp+TJIWFhal79+4qVqyY3N3dVaxYMfXq1SvNqZX379/X0qVL1aFDBwUGBipXrlzy9PRUQECA2rVrp7lz5youLs5m2dDQUKu2nDt3TqdPn1bPnj1VvHhxubm5KSgoyOG+lZH+nCQoKMii/tGjRyshIUHffPONnn/+efn4+MjLy0vVqlXTF198ocTExId+ZlevXtWECRPUtGlTFSpUSO7u7vLx8dEzzzyjV199VbNmzbIYKZbcsWPHNHjwYFWrVk358uWTm5ubChQooCZNmujzzz/X7du3H3rttHz77bcWdXTs2FHOzs4WeSIiIszvg4KCLM6VLFnSZr6jR49qypQpkqSBAweqVq1a6W5TxYoVValSJfPx9u3bFRYWlu7yAIBsZAAAADwijRo1MiRZvOw1d+5cI1euXFb1JH+5u7sbc+bMsVn+pZdeemjZpFeuXLmM5cuXW5QNDg5OV1lJxvz5883lHnYutbobNWpklSdlPePGjTNKlChhlX727Flzmd27dxuBgYFptrdfv37GvXv37P55GEb6fq4p769BgwZGly5dbLbF2dnZWLx4sXH16lWjYsWKqf6MN23aZHWd+fPnW+VduXKl4e7ubrOe3LlzG1u3brV5X8eOHTPKly+f5mdXokQJY//+/VblQ0JCrPLOmjXL8PT0tEgLDAx0uG9lpD8nSdk/Bg0aZNSvXz/VuoKDg1PtC59//nmqn3XyV2RkpEW5uLg4Y+DAgWmWK1q0qLFnz55Ur5+Wdu3aWdS3YsUKqzwbN240n69bt67Fubp165rPJfW/hIQEo06dOoYkIyAgwLh586bd7frnP/9p0a7PP//cofsDADxajFQDAACPjVmzZqlXr15WaxmlFBcXp969e2vx4sUOX+vOnTt68803deLECYfryGpjxozR2bNnUz3/22+/qXnz5jp//nyadc2cOdNiF8KstmPHDi1btszmuYSEBPXv319t27bV8ePHbeZJ+hmnNWpKkrp27ZrqaLJbt26pffv2unLlikX62bNn1aRJk3T9/M+ePavmzZvr999/TzNv//79dffu3TTzZTZ7+vP06dO1Y8eOVM8vXLhQmzdvtkofO3as3nnnnVQ/64fp0aOHpk+fnma+S5cuqUWLFun6rFNKTEzU9u3bLdJs7eJZr149+fn5SZJ2796tWbNm6erVq5o9e7Z2794tSfLz89Pzzz8vSfr666+1Z88eSdJXX32l3Llz2922lO1IvgMsACDnIqgGAACyVcqpbclfU6dONee7cuWKhgwZYlG2VatW+vHHHxUWFqbQ0FC1b9/e4vzAgQMVGRlpkZY3b169+uqrmjNnjjZu3KhDhw7p1KlT2rNnjz777DPlzZvXnPfevXvmxckl6dNPP9XZs2c1ePBgizqLFi1qtR5cx44dM/S5pEd8fLwKFSqk2bNn68SJE9q/f78+/fRT5c6dW4ZhqFevXhZT3cqWLaulS5fq2LFjOnDggN5//33z9EhJmjdvnrZs2ZLl7ZYkwzBUqFAhrVy5UsePH9eIESMszt+8eVMHDhxQ2bJl9fPPP+vw4cNWa0+dO3fOal0rW+Lj4/Xvf/9b+/fvV0hIiNq1a2dxPjIyUuPGjbNIGzhwoP7++2+LtLfeekvbt2/Xnj17rPpidHS0+vfvn6621KpVS+vWrdPJkye1detWDRw40OG+lZH+nBrDMFSqVCmtXbtWR48e1dixYy36iSSLdckk6ciRI1ZTV11dXTV8+HDt2rVLp06d0rZt2zRy5EgVKlTIIt+aNWus6hs4cKB27typsLAwrV69Ws8++6z53M2bNx0KAJ8+fdrieeDj42NzinmuXLk0Y8YM87TQvn37qnDhwnrrrbckPdj9c8aMGfL09NSlS5fMfbdDhw76xz/+YXe7JKlKlSoWx3v37nWoHgDAI5bNI+UAAMBTxNY0wYe9kk+BGjdunMW5Z5991khISLCoPz4+3moq2/Tp0+1q46effmpRvly5clZ5Ro0aZTV972FS3ldmTf90cnIyDh8+bPOa27dvt8jr6upq/Pnnn1b5unXrZpGvQ4cOD70XWxyZ/inJWLNmjfl8YmKi4efnZ5Xn0KFD5jw3btwwnJycLM5/+eWXFtexNf1z0KBBFnkSExONKlWqWOTx9vY296cLFy5Y1dGtWzere+rbt69Vvt9//9183tb0z+LFixu3b99O9bO0t2+lJT39OeXvjJOTk8V9GIZhvPjiixZ5atasaXG+T58+Vve6du1am226efOmER8fbz5u1qyZRbm3337bqsyZM2es6j969Khdn0XKn0fp0qUfmn/37t3Gyy+/bOTPn99wdnY28ufPb7Rv397YvXu3OU/79u0NSYaPj49x+fJlwzAM47vvvjMaNWpkeHt7G+7u7sYzzzxjvPfee0ZUVFSq17p48aJF25ydna2ebwCAnIeRagAA4LGwdetWi+OjR4/K2dnZYmSbi4uL1VTH5IveJ9m+fbv++c9/qkaNGvLz85O7u7u5jmHDhlnkfdimB9ntpZdeUuXKlW2eS/l53b9/X8WKFbMaDbhkyRKLfLY+r6zg6+urtm3bmo9NJpPVovCVK1e2GMHj6+srf39/izwpRyLa0rNnT4tjk8mkHj16WKTFxMSYp0am/OwkmUcpJde3b1+rtLQ+v6FDhypXrlxpttkemd2fmzZtqvLly1uklStXzuI45eceEhJicVy7dm2rEYFJcufObR4FlpCQYDXV9KuvvrLqp6VLl7aqx96+mnLkYb58+R6av06dOlq1apUiIiIUHx+viIgIrV69WnXq1JH0YITdmjVrJEkfffSRChcurBEjRqhjx47aunWrYmJiFBcXp1OnTumjjz5SnTp1Uu2vSdNNkyQkJOj69et23R8A4NFL3z7PAAAAWeRha4Il/9J76dIlh+pPvlZWYmKievbsqYULF6a7/K1btxy67qNQrVq1VM85+nldu3ZN8fHxcnHJ2n8mFi9e3GrXxZTBphIlSliV8/T0tDhObRfJtOqxlXb16lVVrFhRly9ftjpXqlQpq7TkO0EmsVU2uYf9zOyVVf05ZQBNSvtzT3nfNWrUSFd7rl+/7tAabJKs1sFLi2EYFscpp7TaIyYmRgMGDJAkPf/88+rbt6/27duniRMnSpK8vLy0bNkylS1bVr169dKOHTsUFhamDz74QDNmzEjXNTLSPgDAo0FQDQAAZKuUo5MyW/JF4efMmWNXACKrJCQkWKVdu3bN7nqKFCmSGc2xYBiGYmNjHVps3R7J1/pK4uTklGaezJIywCLZH8RwJOiRmT+zrOrPKUdNSbIKgOYE9m74UKBAAYvjjIwEe//993Xp0iW5urpq1qxZMplMWrp0qfl8t27dzOurffLJJ6pbt64k6b///a+++uorq76esi1OTk5pjqQDAGQ/gmoAAOCxUKRIEYudC1u0aKFZs2alWc7d3d38PuVuk76+vpo0aZLq1KkjHx8fSdLSpUv14YcfZlKrH3B2drYIpNnavfTUqVMO1ZualMEbHx8fHTx40OrLvC1eXl52tyUnO3v2rNVC8OfOnbPKV7BgQUm2A1/h4eFW6eHh4Vb5Chcu/NC2ZGZwKrv6sy1FihTRmTNnzMe//vprusr5+fnJzc1N9+7dM6f95z//sZqya0vSPaZXyg0SHAlkS9KePXs0c+ZMSdK///1vVaxYUZLl73DyjRWSv4+MjNS1a9esAnwpp6YWKFAgXb+rAIDsxZMaAAA8Fho3bmxxvGvXLt2/f19BQUE2XwEBAfr1118tgmopp0S+8cYb6tu3r6pUqWIut2fPnjTb4ubmZnGc1oiZlCOukgcHJemXX37R6dOn07yuPVJ+XtHR0dq7d2+qn1dQUJCuXr2qyMjIJ27a2dy5cy2ODcPQ/PnzLdK8vb3N0x4bNmxoVcc333yTrjRbZe1hT9/KrP6cGZo0aWJxvHfvXq1fv95m3lu3bpmnjzo7O6tBgwYW59etW6eCBQum2k/z5cunnTt3ytfX1642PvPMMxaBuKioKF28eNGuOu7fv68+ffooMTFRZcqUsQhYJv+9ST4SMjEx0aIOW79fhw8ftjiuXbu2Xe0CAGQPgmoAAOCx0KNHD4t1nW7fvq3GjRvr888/1969e3X69GkdOHBAS5YsUd++fVWsWDF17NhRMTEx5jL58+e3qHPlypVatWqVTp48qZCQEHXs2DHVQEByKeuJiIjQN998o1OnTuncuXNWo6BSbiYwZ84cffXVVzpx4oSWLVumN954I70fQ7rVq1fPanRWz549NWzYMG3dulWnT5/WkSNHtGbNGr3//vuqWLGi6tata/Xl/knw5Zdf6r333tOBAwe0detWtW/f3uo+u3btah5FVrx4cb344osW55cuXaq+fftq586d2rdvn959912roFqjRo1UoUKFDLXVnr6VWf05M7z99ttWI6s6dOigDz74QHv37tWZM2e0a9cuTZo0SeXLl7dY261///4W5Q4dOqQGDRpo6dKlOnTokE6dOqWdO3dq5syZ6tixo3lDAHs5OTmpfv36Fmn79u2zq45PPvlEx44dkyTNnDlTHh4e5nPPPPOM+X1SnpTv8+XLZ7XZhvQgCJlcyqA4ACCHyta9RwEAwFOlUaNGhiSLlz1mzJhhVT6t19mzZ83lP/300zTzFy5cOM02Hj16NM16kps9e3aa+U0mk8Vxo0aNrK6bssz8+fMf+nkdOHDA8PLysuvzSqtOW9Lzcw0ODk7z/lLWExwcbJUnMDDQIs+oUaMszs+fP9+qLbly5XroPefNm9f4888/LeoJDw83/P390/25+fj4GMePH7eoIyQk5KH90RZ7+lZm9ee0PlPDMIxRo0ZZ5AkMDLTKM3LkyHR/XpGRkRZlO3fubFc/tXX99Pjyyy8t6hk8eHC6y54+fdrw8PAwJBndu3e3Or9v3z5zvV5eXsbatWuN33//3WjQoIE5/e2337ZZd8WKFS3adeLECYfuDwDwaDFSDQAAPDb69eunuXPnpnvNL39/f4vRbQMGDFCjRo1Szd+8eXONGjUqzXorVaqkdu3apasN0oNRdimnxyX30ksvqVOnTumuL71q1KihTZs22dzp0hZ3d3er0U9PglWrVqXaZ7y8vLRq1SoVLVrUIr1kyZIKCQmxuRNmSkFBQdq0aVOGR6lJ9vWtzOrPmWXMmDH65JNPrKawpsfChQs1cODAdE89DggIsPsa0oMNBJLvMrty5Uqr6Zmp6devn2JjY5U/f359+umnVudr1aqlDz74QNKDkbQvvfSSKlSooO3bt0uSKlasqPHjx1uVO3bsmI4fP24+btCgQbr6HQAg+xFUAwAAj5WePXvq3Llz+uijj9SsWTMVKlRI7u7ucnNzU6FChdSgQQO9++67+vHHH3X58mXz4vPSg6DRxo0b9fHHH+vZZ5+Vu7u78uTJo5o1a+qLL77QTz/9ZLEG28OsWLFCY8aM0bPPPmvxJd0WZ2dnbdiwQWPHjlX58uXl7u4uHx8fNWrUSEuWLNGaNWssppFlpjp16ujEiRNauHChXnnlFQUGBipXrlxycXFRvnz5VKNGDfXq1UtLly7VX3/9ZTXt8UnQsmVLHTp0SD169FCxYsXk5uamIkWKqEePHjpy5EiqAc9KlSrpyJEjWrx4sV5++WUFBATIw8ND7u7uKlKkiNq2bavZs2frxIkTqlmzZqa1N719KzP7c2YZNmyYzp49q7Fjx6pRo0YqUKCA3NzclCdPHpUuXVodOnTQzJkzrXaXdXNz0xdffKHjx49r6NCheu6555QvXz65uLgoV65cCgoKUuvWrTVu3Djt37/fHKiyl4+Pj7p06WI+vnz5skJDQ9Mst3DhQm3evFmS9Nlnn9ncIVWSJkyYoJUrV6phw4bKkyeP3NzcVKZMGb333nvatWuXzR1tk+8aKj0I3gEAHg8mw7CxnzgAAADwGFqwYIF69OhhkcY/d5FcWFiYnn32WfNmCZ06ddJ///vfbGlLXFycSpYsqcuXL0uSSpUqpd9//92h0X4AgEePkWoAAAAAnhrlypVTnz59zMffffedTp48mS1tWbBggTmgJkkfffQRATUAeIwQVAMAAADwVBkzZoy8vb0lSYmJiZo4ceIjb0N8fLw+/vhj83HdunXVsWPHR94OAIDjXLK7AQAAAADwKOXPn1/R0dHZ2gYXFxf98ccf2doGAEDGMFINAAAAAAAAsBMbFQAAAAAAAAB2YqQaAAAAAAAAYCeCagAAAAAAAICdCKoBAAAAAAAAdiKoBgAAAAAAANiJoBoAAAAAAABgJ4JqAAAAAAAAgJ0IqgEAAAAAAAB2IqgGAAAAAAAA2On/AUiz6Slic5PWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1300x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 3)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=18, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=18, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 3 Month Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7c48d5c5-d7c5-49d8-b601-312cbc13048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will modify our feature set to add bigger lagging indicators.\n",
    "# Create a new dataframe called 'df_stock_data_6_month' as a copy of 'df_stocks_price_ta'\n",
    "df_stock_data_6_month = df_stocks_price_ta.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1e314a56-c23b-42b9-a9f0-bbf269d95b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1104426286.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>EMA_26_MACD_lag_15</th>\n",
       "      <th>EMA_26_MACD_lag_20</th>\n",
       "      <th>EMA_26_MACD_lag_25</th>\n",
       "      <th>EMA_26_MACD_lag_30</th>\n",
       "      <th>EMA_26_MACD_lag_40</th>\n",
       "      <th>EMA_26_MACD_lag_50</th>\n",
       "      <th>EMA_26_MACD_lag_60</th>\n",
       "      <th>EMA_26_MACD_lag_75</th>\n",
       "      <th>EMA_26_MACD_lag_90</th>\n",
       "      <th>EMA_26_MACD_lag_180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.02</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.97</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.57</td>\n",
       "      <td>10.22</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.39</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close   High    Low     Volume      SMA_5     SMA_20  \\\n",
       "0   AAPL  2022-02-10  10.52  10.73  10.20  1037700.0  10.520000  10.520000   \n",
       "1   AAPL  2022-02-11  10.09  10.49  10.02   480300.0  10.305000  10.305000   \n",
       "2   AAPL  2022-02-14  10.22  10.46   9.97   724400.0  10.276667  10.276667   \n",
       "3   AAPL  2022-02-15  10.56  10.57  10.22   758700.0  10.347500  10.347500   \n",
       "4   AAPL  2022-02-16  10.44  10.55  10.39   685200.0  10.366000  10.366000   \n",
       "\n",
       "      SMA_50      EMA_5  ...  EMA_26_MACD_lag_15  EMA_26_MACD_lag_20  \\\n",
       "0  10.520000  10.520000  ...                 NaN                 NaN   \n",
       "1  10.305000  10.376667  ...                 NaN                 NaN   \n",
       "2  10.276667  10.324445  ...                 NaN                 NaN   \n",
       "3  10.347500  10.402963  ...                 NaN                 NaN   \n",
       "4  10.366000  10.415309  ...                 NaN                 NaN   \n",
       "\n",
       "   EMA_26_MACD_lag_25  EMA_26_MACD_lag_30  EMA_26_MACD_lag_40  \\\n",
       "0                 NaN                 NaN                 NaN   \n",
       "1                 NaN                 NaN                 NaN   \n",
       "2                 NaN                 NaN                 NaN   \n",
       "3                 NaN                 NaN                 NaN   \n",
       "4                 NaN                 NaN                 NaN   \n",
       "\n",
       "   EMA_26_MACD_lag_50  EMA_26_MACD_lag_60  EMA_26_MACD_lag_75  \\\n",
       "0                 NaN                 NaN                 NaN   \n",
       "1                 NaN                 NaN                 NaN   \n",
       "2                 NaN                 NaN                 NaN   \n",
       "3                 NaN                 NaN                 NaN   \n",
       "4                 NaN                 NaN                 NaN   \n",
       "\n",
       "   EMA_26_MACD_lag_90  EMA_26_MACD_lag_180  \n",
       "0                 NaN                  NaN  \n",
       "1                 NaN                  NaN  \n",
       "2                 NaN                  NaN  \n",
       "3                 NaN                  NaN  \n",
       "4                 NaN                  NaN  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of columns to create lags for (focusing on mid-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_5', 'EMA_5', 'Volume', 'SMA_20',\n",
    "       'SMA_50', 'EMA_5', 'EMA_20', 'EMA_50',  'EMA_12_MACD',\n",
    "       'EMA_26_MACD']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 12, 15, 20, 30, 60, 90, 180, 360] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7, 10, 15, 20, 25, 30, 40, 50, 60, 75, 90, 180]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_stock_data_6_month[f'{col}_lag_{lag}'] = df_stock_data_6_month[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_stock_data_6_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f49f34d1-1235-42aa-8716-fe89165ce334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3320551559.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-120)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3320551559.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (406528, 205)\n",
      "Testing data shape: (270615, 205)\n",
      "X_train shape: (406528, 201), y_train shape: (406528,)\n",
      "X_test shape: (270615, 201), y_test shape: (270615,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 39556.315823205616\n"
     ]
    }
   ],
   "source": [
    "# now we're going to move onto our next model: 6 month prediction\n",
    "# we'll start at our baseline model and then do the same as we just did\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_6_month.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a09aee03-9fcd-4fdb-88d5-0c9e0e73b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 39556.315823205616\n",
      "Mean Absolute Error on unseen data: 39.416918393972786\n",
      "Root Mean Squared Error on unseen data: 198.8876965103815\n",
      "R-squared on unseen data: 0.8381612251405623\n",
      "Median Absolute Error on unseen data: 9.123995780944824\n",
      "Durbin-Watson Statistic on unseen data: 0.028605736941907666\n",
      "MAPE on unseen data: 20.33%\n",
      "Fib_30_High_Max: 32.51%\n",
      "Middle_Band: 13.50%\n",
      "EMA_12_MACD: 8.31%\n",
      "Fib_5_Low_Min: 6.21%\n",
      "5_day-Fib_38: 6.09%\n",
      "Volume: 5.72%\n",
      "EMA_5: 4.77%\n",
      "Fib_5_High_Max: 4.28%\n",
      "5_day-Fib_23: 3.67%\n",
      "Fib_10_High_Max: 2.76%\n",
      "Low: 2.16%\n",
      "VWAP: 1.29%\n",
      "10_day_Fib_50: 1.26%\n",
      "EMA_50: 0.87%\n",
      "SMA_5_lag_1: 0.59%\n",
      "High: 0.54%\n",
      "Cumulative_Price_Volume: 0.50%\n",
      "Fib_30_Low_Min: 0.30%\n",
      "EMA_12_MACD_lag_3: 0.30%\n",
      "SMA_20: 0.29%\n",
      "Fib_10_Low_Min: 0.28%\n",
      "EMA_26_MACD_lag_40: 0.26%\n",
      "5_day-Fib_61: 0.24%\n",
      "30_day_Fib_61: 0.23%\n",
      "Lower_Band: 0.22%\n",
      "10_day_Fib_38: 0.21%\n",
      "SMA_20_lag_30: 0.18%\n",
      "ATR_Prev_Close: 0.17%\n",
      "SMA_50: 0.13%\n",
      "ATR_High_Low: 0.13%\n",
      "EMA_50_lag_30: 0.13%\n",
      "SMA_5: 0.12%\n",
      "Close_lag_60: 0.10%\n",
      "EMA_20_lag_20: 0.10%\n",
      "SMA_50_lag_25: 0.08%\n",
      "EMA_26_MACD_lag_60: 0.07%\n",
      "EMA_50_lag_90: 0.07%\n",
      "EMA_50_lag_25: 0.07%\n",
      "EMA_20: 0.05%\n",
      "Cumulative_Volume: 0.04%\n",
      "Std_Dev: 0.04%\n",
      "EMA_50_lag_75: 0.03%\n",
      "EMA_26_MACD_lag_180: 0.03%\n",
      "Close_lag_50: 0.03%\n",
      "SMA_50_lag_90: 0.03%\n",
      "EMA_50_lag_60: 0.03%\n",
      "Close_lag_30: 0.03%\n",
      "EMA_26_MACD: 0.03%\n",
      "EMA_5_lag_60: 0.02%\n",
      "SMA_50_lag_50: 0.02%\n",
      "Volume_lag_1: 0.02%\n",
      "EMA_50_lag_180: 0.02%\n",
      "10_day_Fib_61: 0.02%\n",
      "EMA_12_MACD_lag_180: 0.02%\n",
      "Upper_Band: 0.02%\n",
      "Close_lag_25: 0.02%\n",
      "EMA_12_MACD_lag_60: 0.02%\n",
      "SMA_50_lag_75: 0.02%\n",
      "EMA_20_lag_60: 0.02%\n",
      "SMA_20_lag_60: 0.02%\n",
      "Close_lag_90: 0.02%\n",
      "Volume_lag_10: 0.01%\n",
      "SMA_5_lag_60: 0.01%\n",
      "EMA_26_MACD_lag_75: 0.01%\n",
      "SMA_50_lag_60: 0.01%\n",
      "ATR: 0.01%\n",
      "EMA_5_lag_40: 0.01%\n",
      "SMA_50_lag_180: 0.01%\n",
      "SMA_20_lag_75: 0.01%\n",
      "EMA_20_lag_180: 0.01%\n",
      "EMA_12_MACD_lag_90: 0.01%\n",
      "SMA_50_lag_20: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "EMA_20_lag_30: 0.01%\n",
      "Volume_lag_15: 0.01%\n",
      "Volume_lag_180: 0.01%\n",
      "Volume_lag_7: 0.01%\n",
      "EMA_26_MACD_lag_25: 0.01%\n",
      "SMA_50_lag_3: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "EMA_5_lag_180: 0.01%\n",
      "EMA_5_lag_50: 0.01%\n",
      "SMA_20_lag_180: 0.01%\n",
      "Volume_lag_20: 0.01%\n",
      "EMA_20_lag_10: 0.01%\n",
      "EMA_12_MACD_lag_50: 0.01%\n",
      "Close_lag_75: 0.01%\n",
      "EMA_5_lag_90: 0.01%\n",
      "SMA_20_lag_90: 0.01%\n",
      "EMA_50_lag_20: 0.01%\n",
      "Volume_lag_3: 0.01%\n",
      "SMA_5_lag_90: 0.01%\n",
      "ATR_True_Range: 0.01%\n",
      "SMA_20_lag_15: 0.01%\n",
      "Close_lag_180: 0.01%\n",
      "Close_lag_40: 0.01%\n",
      "Close_lag_15: 0.01%\n",
      "SMA_50_lag_30: 0.01%\n",
      "SMA_50_lag_1: 0.01%\n",
      "EMA_5_lag_75: 0.01%\n",
      "30_day_Fib_23: 0.01%\n",
      "SMA_50_lag_40: 0.01%\n",
      "Volume_lag_25: 0.01%\n",
      "MACD_Histogram: 0.01%\n",
      "EMA_50_lag_40: 0.01%\n",
      "EMA_20_lag_40: 0.01%\n",
      "MACD: 0.01%\n",
      "30_day_Fib_38: 0.01%\n",
      "EMA_26_MACD_lag_15: 0.01%\n",
      "Close_lag_20: 0.01%\n",
      "EMA_20_lag_90: 0.01%\n",
      "EMA_12_MACD_lag_7: 0.01%\n",
      "EMA_12_MACD_lag_10: 0.01%\n",
      "EMA_50_lag_50: 0.01%\n",
      "SMA_50_lag_15: 0.01%\n",
      "SMA_50_lag_7: 0.01%\n",
      "SMA_20_lag_3: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "SMA_20_lag_25: 0.00%\n",
      "EMA_50_lag_1: 0.00%\n",
      "30_day_Fib_50: 0.00%\n",
      "EMA_26_MACD_lag_30: 0.00%\n",
      "SMA_50_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_75: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "EMA_50_lag_15: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "SMA_20_lag_40: 0.00%\n",
      "Volume_lag_60: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "EMA_50_lag_10: 0.00%\n",
      "SMA_5_lag_180: 0.00%\n",
      "EMA_12_MACD_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_90: 0.00%\n",
      "SMA_50_lag_10: 0.00%\n",
      "EMA_50_lag_5: 0.00%\n",
      "SMA_20_lag_50: 0.00%\n",
      "5_day-Fib_50: 0.00%\n",
      "Volume_lag_90: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_50_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_7: 0.00%\n",
      "EMA_26_MACD_lag_3: 0.00%\n",
      "EMA_50_lag_7: 0.00%\n",
      "Volume_lag_75: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "EMA_12_MACD_lag_30: 0.00%\n",
      "SMA_5_lag_75: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_40: 0.00%\n",
      "SMA_5_lag_50: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "SMA_20_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_40: 0.00%\n",
      "Volume_lag_30: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "EMA_20_lag_50: 0.00%\n",
      "SMA_5_lag_30: 0.00%\n",
      "Volume_lag_50: 0.00%\n",
      "EMA_20_lag_25: 0.00%\n",
      "Volume_lag_40: 0.00%\n",
      "EMA_26_MACD_lag_50: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "EMA_20_lag_75: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_25: 0.00%\n",
      "RSI: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "%K: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "%D: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "SMA_5_lag_25: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_1: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_6_month.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8b28bc7d-0e99-4862-9908-d092c1b0408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3398565816.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-120)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/3398565816.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (406528, 205)\n",
      "Testing data shape: (270615, 205)\n",
      "X_train shape: (406528, 201), y_train shape: (406528,)\n",
      "X_test shape: (270615, 201), y_test shape: (270615,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 39709.225423030846\n"
     ]
    }
   ],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_lr_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_lr_1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_6_month_lr_1.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f0079e37-5c7f-40ee-ba59-50bde2f72d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 39709.225423030846\n",
      "Mean Absolute Error on unseen data: 39.93220039169745\n",
      "Root Mean Squared Error on unseen data: 199.27173764242346\n",
      "R-squared on unseen data: 0.8375356182865124\n",
      "Median Absolute Error on unseen data: 9.373844146728516\n",
      "Durbin-Watson Statistic on unseen data: 0.02865197681840183\n",
      "MAPE on unseen data: 21.11%\n",
      "Fib_30_High_Max: 32.47%\n",
      "EMA_12_MACD: 15.43%\n",
      "Middle_Band: 8.60%\n",
      "Fib_5_Low_Min: 7.65%\n",
      "Volume: 5.66%\n",
      "5_day-Fib_38: 5.29%\n",
      "Fib_5_High_Max: 4.40%\n",
      "EMA_5: 3.09%\n",
      "5_day-Fib_23: 2.91%\n",
      "Fib_10_High_Max: 2.21%\n",
      "Low: 2.11%\n",
      "EMA_50: 2.04%\n",
      "10_day_Fib_50: 1.37%\n",
      "VWAP: 1.29%\n",
      "Cumulative_Price_Volume: 0.55%\n",
      "SMA_20: 0.45%\n",
      "30_day_Fib_61: 0.41%\n",
      "5_day-Fib_50: 0.28%\n",
      "Fib_10_Low_Min: 0.26%\n",
      "5_day-Fib_61: 0.24%\n",
      "10_day_Fib_38: 0.22%\n",
      "ATR_Prev_Close: 0.19%\n",
      "Fib_30_Low_Min: 0.18%\n",
      "SMA_20_lag_30: 0.18%\n",
      "EMA_26_MACD_lag_40: 0.16%\n",
      "SMA_5: 0.15%\n",
      "Lower_Band: 0.13%\n",
      "10_day_Fib_61: 0.13%\n",
      "EMA_50_lag_30: 0.11%\n",
      "High: 0.11%\n",
      "ATR_High_Low: 0.10%\n",
      "SMA_50_lag_25: 0.10%\n",
      "SMA_50: 0.09%\n",
      "EMA_20: 0.07%\n",
      "EMA_20_lag_20: 0.06%\n",
      "EMA_50_lag_90: 0.06%\n",
      "Std_Dev: 0.05%\n",
      "Close_lag_60: 0.05%\n",
      "EMA_50_lag_75: 0.04%\n",
      "EMA_26_MACD_lag_180: 0.04%\n",
      "EMA_26_MACD_lag_60: 0.04%\n",
      "Cumulative_Volume: 0.04%\n",
      "EMA_50_lag_60: 0.04%\n",
      "Volume_lag_1: 0.03%\n",
      "Upper_Band: 0.02%\n",
      "EMA_50_lag_180: 0.02%\n",
      "Close_lag_30: 0.02%\n",
      "10_day_Fib_23: 0.02%\n",
      "EMA_12_MACD_lag_180: 0.02%\n",
      "SMA_50_lag_1: 0.02%\n",
      "EMA_20_lag_60: 0.02%\n",
      "EMA_12_MACD_lag_60: 0.02%\n",
      "SMA_20_lag_75: 0.02%\n",
      "EMA_5_lag_60: 0.01%\n",
      "ATR: 0.01%\n",
      "EMA_12_MACD_lag_90: 0.01%\n",
      "SMA_50_lag_20: 0.01%\n",
      "SMA_50_lag_60: 0.01%\n",
      "EMA_26_MACD: 0.01%\n",
      "Volume_lag_7: 0.01%\n",
      "EMA_5_lag_50: 0.01%\n",
      "Close_lag_25: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "SMA_50_lag_50: 0.01%\n",
      "SMA_50_lag_3: 0.01%\n",
      "SMA_50_lag_180: 0.01%\n",
      "SMA_50_lag_90: 0.01%\n",
      "EMA_5_lag_40: 0.01%\n",
      "EMA_50_lag_15: 0.01%\n",
      "EMA_26_MACD_lag_5: 0.01%\n",
      "EMA_26_MACD_lag_1: 0.01%\n",
      "SMA_20_lag_60: 0.01%\n",
      "EMA_20_lag_1: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "Volume_lag_15: 0.01%\n",
      "30_day_Fib_23: 0.01%\n",
      "Volume_lag_20: 0.01%\n",
      "EMA_20_lag_30: 0.01%\n",
      "EMA_50_lag_40: 0.01%\n",
      "ATR_True_Range: 0.01%\n",
      "EMA_12_MACD_lag_20: 0.01%\n",
      "SMA_50_lag_40: 0.01%\n",
      "SMA_50_lag_30: 0.01%\n",
      "EMA_5_lag_180: 0.01%\n",
      "Close_lag_90: 0.01%\n",
      "SMA_5_lag_40: 0.01%\n",
      "EMA_50_lag_25: 0.01%\n",
      "EMA_5_lag_90: 0.01%\n",
      "EMA_20_lag_15: 0.01%\n",
      "Volume_lag_180: 0.01%\n",
      "EMA_50_lag_1: 0.01%\n",
      "SMA_20_lag_1: 0.01%\n",
      "EMA_20_lag_90: 0.01%\n",
      "Close_lag_15: 0.01%\n",
      "EMA_20_lag_180: 0.01%\n",
      "SMA_20_lag_180: 0.01%\n",
      "SMA_50_lag_7: 0.01%\n",
      "EMA_50_lag_50: 0.01%\n",
      "EMA_26_MACD_lag_20: 0.01%\n",
      "MACD_Histogram: 0.01%\n",
      "SMA_5_lag_60: 0.01%\n",
      "EMA_5_lag_75: 0.01%\n",
      "SMA_20_lag_40: 0.01%\n",
      "EMA_26_MACD_lag_25: 0.01%\n",
      "Close_lag_180: 0.01%\n",
      "EMA_26_MACD_lag_75: 0.01%\n",
      "SMA_20_lag_7: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "EMA_12_MACD_lag_15: 0.01%\n",
      "EMA_12_MACD_lag_50: 0.01%\n",
      "30_day_Fib_50: 0.01%\n",
      "EMA_50_lag_3: 0.01%\n",
      "EMA_50_lag_20: 0.01%\n",
      "SMA_50_lag_5: 0.01%\n",
      "SMA_5_lag_7: 0.01%\n",
      "SMA_50_lag_75: 0.01%\n",
      "SMA_5_lag_90: 0.01%\n",
      "Volume_lag_75: 0.00%\n",
      "MACD: 0.00%\n",
      "Volume_lag_3: 0.00%\n",
      "Volume_lag_25: 0.00%\n",
      "Close_lag_75: 0.00%\n",
      "EMA_50_lag_10: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_25: 0.00%\n",
      "EMA_20_lag_50: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "SMA_5_lag_75: 0.00%\n",
      "Volume_lag_60: 0.00%\n",
      "30_day_Fib_38: 0.00%\n",
      "SMA_20_lag_50: 0.00%\n",
      "EMA_20_lag_75: 0.00%\n",
      "Volume_lag_90: 0.00%\n",
      "SMA_5_lag_50: 0.00%\n",
      "SMA_20_lag_25: 0.00%\n",
      "SMA_20_lag_15: 0.00%\n",
      "Volume_lag_30: 0.00%\n",
      "EMA_50_lag_5: 0.00%\n",
      "EMA_20_lag_40: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "EMA_26_MACD_lag_90: 0.00%\n",
      "SMA_50_lag_15: 0.00%\n",
      "SMA_50_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_75: 0.00%\n",
      "EMA_26_MACD_lag_7: 0.00%\n",
      "EMA_50_lag_7: 0.00%\n",
      "EMA_26_MACD_lag_30: 0.00%\n",
      "SMA_20_lag_20: 0.00%\n",
      "EMA_20_lag_10: 0.00%\n",
      "SMA_20_lag_90: 0.00%\n",
      "Close_lag_50: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "SMA_5_lag_180: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "SMA_5_lag_25: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "EMA_20_lag_25: 0.00%\n",
      "Volume_lag_50: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "SMA_5_lag_30: 0.00%\n",
      "EMA_26_MACD_lag_3: 0.00%\n",
      "%D: 0.00%\n",
      "EMA_12_MACD_lag_40: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "EMA_26_MACD_lag_15: 0.00%\n",
      "RSI: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_50: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "%K: 0.00%\n",
      "Volume_lag_40: 0.00%\n",
      "EMA_12_MACD_lag_30: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "Close_lag_40: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_6_month_lr_1.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3368cfb0-0392-4be4-a586-29b4ce0bc69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1469120769.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-120)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1469120769.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (406528, 205)\n",
      "Testing data shape: (270615, 205)\n",
      "X_train shape: (406528, 201), y_train shape: (406528,)\n",
      "X_test shape: (270615, 201), y_test shape: (270615,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 39179.243873570806\n"
     ]
    }
   ],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.01\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_lr_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_lr_01.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_6_month_lr_01.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bac22b48-ec53-40ad-bbab-4b9a39f7abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 39179.243873570806\n",
      "Mean Absolute Error on unseen data: 39.05236319493131\n",
      "Root Mean Squared Error on unseen data: 197.9374746569502\n",
      "R-squared on unseen data: 0.8397039588631745\n",
      "Median Absolute Error on unseen data: 8.80865478515625\n",
      "Durbin-Watson Statistic on unseen data: 0.02858915370029223\n",
      "MAPE on unseen data: 20.07%\n",
      "Fib_30_High_Max: 48.93%\n",
      "Middle_Band: 5.77%\n",
      "EMA_12_MACD: 5.00%\n",
      "Volume: 4.90%\n",
      "Fib_5_Low_Min: 4.31%\n",
      "Fib_5_High_Max: 4.11%\n",
      "Fib_10_High_Max: 3.50%\n",
      "5_day-Fib_38: 3.11%\n",
      "EMA_5: 3.09%\n",
      "5_day-Fib_23: 1.99%\n",
      "VWAP: 1.85%\n",
      "Low: 1.19%\n",
      "High: 1.17%\n",
      "10_day_Fib_50: 1.08%\n",
      "EMA_50: 1.04%\n",
      "5_day-Fib_61: 0.64%\n",
      "SMA_5_lag_1: 0.54%\n",
      "Fib_10_Low_Min: 0.49%\n",
      "Lower_Band: 0.46%\n",
      "Fib_30_Low_Min: 0.45%\n",
      "10_day_Fib_38: 0.45%\n",
      "SMA_20: 0.41%\n",
      "Cumulative_Price_Volume: 0.41%\n",
      "EMA_12_MACD_lag_3: 0.40%\n",
      "30_day_Fib_61: 0.37%\n",
      "EMA_50_lag_90: 0.31%\n",
      "SMA_20_lag_30: 0.28%\n",
      "EMA_50_lag_30: 0.24%\n",
      "EMA_26_MACD_lag_40: 0.20%\n",
      "ATR_High_Low: 0.19%\n",
      "10_day_Fib_23: 0.19%\n",
      "SMA_50: 0.17%\n",
      "EMA_20: 0.16%\n",
      "SMA_5: 0.16%\n",
      "Close_lag_60: 0.13%\n",
      "ATR_Prev_Close: 0.13%\n",
      "EMA_26_MACD_lag_60: 0.10%\n",
      "SMA_50_lag_25: 0.09%\n",
      "EMA_50_lag_75: 0.09%\n",
      "EMA_20_lag_20: 0.09%\n",
      "EMA_50_lag_25: 0.08%\n",
      "EMA_26_MACD_lag_180: 0.07%\n",
      "Std_Dev: 0.06%\n",
      "EMA_26_MACD: 0.06%\n",
      "SMA_50_lag_90: 0.05%\n",
      "Cumulative_Volume: 0.04%\n",
      "SMA_50_lag_3: 0.04%\n",
      "EMA_50_lag_60: 0.04%\n",
      "SMA_50_lag_1: 0.04%\n",
      "EMA_50_lag_180: 0.04%\n",
      "Upper_Band: 0.03%\n",
      "SMA_50_lag_50: 0.03%\n",
      "ATR_True_Range: 0.03%\n",
      "EMA_12_MACD_lag_90: 0.03%\n",
      "EMA_12_MACD_lag_180: 0.03%\n",
      "Close_lag_90: 0.03%\n",
      "Close_lag_30: 0.02%\n",
      "EMA_5_lag_40: 0.02%\n",
      "Volume_lag_1: 0.02%\n",
      "Close_lag_50: 0.02%\n",
      "SMA_20_lag_180: 0.02%\n",
      "10_day_Fib_61: 0.02%\n",
      "EMA_26_MACD_lag_75: 0.02%\n",
      "Close_lag_25: 0.02%\n",
      "ATR: 0.02%\n",
      "SMA_50_lag_180: 0.02%\n",
      "SMA_20_lag_75: 0.02%\n",
      "EMA_20_lag_180: 0.02%\n",
      "EMA_20_lag_90: 0.02%\n",
      "Signal_Line: 0.02%\n",
      "SMA_20_lag_60: 0.02%\n",
      "EMA_20_lag_60: 0.02%\n",
      "EMA_20_lag_75: 0.02%\n",
      "Close_lag_40: 0.02%\n",
      "SMA_5_lag_60: 0.01%\n",
      "Close_lag_75: 0.01%\n",
      "EMA_5_lag_60: 0.01%\n",
      "SMA_50_lag_7: 0.01%\n",
      "SMA_50_lag_75: 0.01%\n",
      "SMA_50_lag_60: 0.01%\n",
      "5_day-Fib_50: 0.01%\n",
      "SMA_50_lag_20: 0.01%\n",
      "MACD_Histogram: 0.01%\n",
      "SMA_20_lag_90: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "Close_lag_180: 0.01%\n",
      "Volume_lag_20: 0.01%\n",
      "SMA_50_lag_40: 0.01%\n",
      "Volume_lag_15: 0.01%\n",
      "EMA_12_MACD_lag_60: 0.01%\n",
      "EMA_50_lag_40: 0.01%\n",
      "MACD: 0.01%\n",
      "EMA_12_MACD_lag_1: 0.01%\n",
      "EMA_5_lag_75: 0.01%\n",
      "Volume_lag_3: 0.01%\n",
      "EMA_50_lag_50: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "Volume_lag_180: 0.01%\n",
      "EMA_26_MACD_lag_90: 0.01%\n",
      "SMA_50_lag_30: 0.01%\n",
      "EMA_20_lag_1: 0.01%\n",
      "Volume_lag_7: 0.01%\n",
      "SMA_50_lag_15: 0.01%\n",
      "EMA_26_MACD_lag_15: 0.01%\n",
      "EMA_20_lag_30: 0.01%\n",
      "EMA_26_MACD_lag_25: 0.01%\n",
      "SMA_5_lag_20: 0.01%\n",
      "SMA_20_lag_40: 0.01%\n",
      "SMA_5_lag_40: 0.01%\n",
      "SMA_5_lag_180: 0.01%\n",
      "EMA_20_lag_5: 0.01%\n",
      "Volume_lag_25: 0.01%\n",
      "EMA_20_lag_25: 0.01%\n",
      "SMA_50_lag_5: 0.01%\n",
      "EMA_12_MACD_lag_30: 0.01%\n",
      "SMA_20_lag_1: 0.01%\n",
      "EMA_12_MACD_lag_75: 0.01%\n",
      "EMA_5_lag_180: 0.01%\n",
      "EMA_5_lag_10: 0.01%\n",
      "SMA_50_lag_10: 0.01%\n",
      "EMA_26_MACD_lag_50: 0.01%\n",
      "Volume_lag_75: 0.01%\n",
      "EMA_5_lag_90: 0.01%\n",
      "EMA_20_lag_40: 0.01%\n",
      "SMA_20_lag_15: 0.01%\n",
      "SMA_5_lag_7: 0.01%\n",
      "EMA_26_MACD_lag_5: 0.01%\n",
      "EMA_26_MACD_lag_30: 0.01%\n",
      "EMA_50_lag_20: 0.01%\n",
      "EMA_50_lag_10: 0.01%\n",
      "ATR_High_Close: 0.01%\n",
      "EMA_50_lag_15: 0.01%\n",
      "EMA_5_lag_15: 0.01%\n",
      "SMA_20_lag_3: 0.01%\n",
      "SMA_5_lag_50: 0.01%\n",
      "30_day_Fib_23: 0.01%\n",
      "EMA_50_lag_5: 0.01%\n",
      "EMA_20_lag_10: 0.01%\n",
      "EMA_5_lag_25: 0.01%\n",
      "EMA_26_MACD_lag_7: 0.01%\n",
      "EMA_26_MACD_lag_3: 0.01%\n",
      "Close_lag_1: 0.01%\n",
      "EMA_26_MACD_lag_10: 0.01%\n",
      "30_day_Fib_50: 0.01%\n",
      "SMA_20_lag_25: 0.01%\n",
      "SMA_5_lag_90: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "Volume_lag_40: 0.00%\n",
      "Volume_lag_90: 0.00%\n",
      "EMA_26_MACD_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "SMA_20_lag_50: 0.00%\n",
      "EMA_26_MACD_lag_20: 0.00%\n",
      "Volume_lag_60: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "Volume_lag_30: 0.00%\n",
      "EMA_50_lag_7: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "SMA_5_lag_75: 0.00%\n",
      "EMA_12_MACD_lag_50: 0.00%\n",
      "EMA_12_MACD_lag_20: 0.00%\n",
      "EMA_20_lag_50: 0.00%\n",
      "30_day_Fib_38: 0.00%\n",
      "Volume_lag_50: 0.00%\n",
      "EMA_50_lag_1: 0.00%\n",
      "EMA_5_lag_50: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_50_lag_3: 0.00%\n",
      "%D: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "RSI: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "%K: 0.00%\n",
      "SMA_5_lag_30: 0.00%\n",
      "SMA_20_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_25: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "SMA_5_lag_25: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_40: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_6_month_lr_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be07e445-4b9a-4531-bf37-ff3d8725a3d3",
   "metadata": {},
   "source": [
    "model with learning_rate 0.01 performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6447c125-546d-49b9-bf7f-43ba6018f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/180156107.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-120)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/180156107.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (406528, 205)\n",
      "Testing data shape: (270615, 205)\n",
      "X_train shape: (406528, 201), y_train shape: (406528,)\n",
      "X_test shape: (270615, 201), y_test shape: (270615,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 31411.174267588787\n"
     ]
    }
   ],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.01\n",
    "# max_depth = 3\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_md_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_md_3.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_6_month_md_3.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d12ac369-2efa-460f-b7d2-733d5302933c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 31411.174267588787\n",
      "Mean Absolute Error on unseen data: 34.84146322021573\n",
      "Root Mean Squared Error on unseen data: 177.23197868214638\n",
      "R-squared on unseen data: 0.871485858716382\n",
      "Median Absolute Error on unseen data: 8.994926452636719\n",
      "Durbin-Watson Statistic on unseen data: 0.020845241129731375\n",
      "MAPE on unseen data: 21.51%\n",
      "Fib_10_High_Max: 17.93%\n",
      "Fib_30_High_Max: 13.03%\n",
      "Fib_5_Low_Min: 12.74%\n",
      "ATR_High_Low: 5.86%\n",
      "10_day_Fib_50: 4.75%\n",
      "EMA_50_lag_25: 4.42%\n",
      "EMA_5: 2.79%\n",
      "Middle_Band: 2.77%\n",
      "Volume: 2.74%\n",
      "EMA_12_MACD: 2.23%\n",
      "EMA_50: 2.11%\n",
      "Fib_5_High_Max: 2.10%\n",
      "5_day-Fib_61: 1.93%\n",
      "Low: 1.78%\n",
      "High: 1.60%\n",
      "30_day_Fib_61: 1.47%\n",
      "VWAP: 1.45%\n",
      "Fib_30_Low_Min: 1.18%\n",
      "EMA_26_MACD_lag_40: 1.09%\n",
      "SMA_50: 1.05%\n",
      "Fib_10_Low_Min: 1.02%\n",
      "SMA_50_lag_90: 1.01%\n",
      "SMA_5: 0.98%\n",
      "EMA_50_lag_30: 0.97%\n",
      "ATR: 0.92%\n",
      "SMA_50_lag_20: 0.76%\n",
      "EMA_20: 0.69%\n",
      "Close_lag_90: 0.65%\n",
      "5_day-Fib_23: 0.58%\n",
      "SMA_50_lag_25: 0.55%\n",
      "ATR_Prev_Close: 0.54%\n",
      "Lower_Band: 0.53%\n",
      "Std_Dev: 0.43%\n",
      "ATR_True_Range: 0.38%\n",
      "Volume_lag_75: 0.35%\n",
      "SMA_5_lag_60: 0.31%\n",
      "30_day_Fib_23: 0.29%\n",
      "EMA_12_MACD_lag_3: 0.21%\n",
      "10_day_Fib_38: 0.21%\n",
      "Volume_lag_60: 0.20%\n",
      "SMA_50_lag_75: 0.20%\n",
      "Volume_lag_1: 0.18%\n",
      "EMA_5_lag_75: 0.17%\n",
      "SMA_50_lag_3: 0.17%\n",
      "Close_lag_180: 0.17%\n",
      "Cumulative_Price_Volume: 0.16%\n",
      "Volume_lag_50: 0.15%\n",
      "5_day-Fib_50: 0.15%\n",
      "EMA_50_lag_1: 0.14%\n",
      "EMA_50_lag_90: 0.13%\n",
      "Volume_lag_30: 0.13%\n",
      "10_day_Fib_61: 0.12%\n",
      "EMA_50_lag_40: 0.09%\n",
      "EMA_20_lag_180: 0.08%\n",
      "SMA_50_lag_40: 0.08%\n",
      "Volume_lag_25: 0.08%\n",
      "EMA_50_lag_75: 0.06%\n",
      "Volume_lag_3: 0.05%\n",
      "Cumulative_Volume: 0.04%\n",
      "Volume_lag_5: 0.04%\n",
      "EMA_20_lag_90: 0.04%\n",
      "Signal_Line: 0.04%\n",
      "EMA_50_lag_180: 0.04%\n",
      "Volume_lag_40: 0.04%\n",
      "EMA_26_MACD: 0.03%\n",
      "EMA_26_MACD_lag_180: 0.03%\n",
      "Upper_Band: 0.03%\n",
      "EMA_12_MACD_lag_180: 0.03%\n",
      "EMA_20_lag_3: 0.03%\n",
      "EMA_5_lag_40: 0.03%\n",
      "EMA_12_MACD_lag_40: 0.03%\n",
      "SMA_50_lag_180: 0.02%\n",
      "SMA_20_lag_15: 0.02%\n",
      "Volume_lag_20: 0.02%\n",
      "SMA_20_lag_30: 0.02%\n",
      "EMA_12_MACD_lag_25: 0.02%\n",
      "EMA_5_lag_60: 0.02%\n",
      "SMA_5_lag_7: 0.02%\n",
      "Volume_lag_180: 0.02%\n",
      "EMA_5_lag_50: 0.02%\n",
      "EMA_50_lag_15: 0.02%\n",
      "SMA_50_lag_50: 0.02%\n",
      "SMA_20_lag_60: 0.02%\n",
      "SMA_20_lag_40: 0.02%\n",
      "EMA_26_MACD_lag_10: 0.01%\n",
      "Close_lag_20: 0.01%\n",
      "5_day-Fib_38: 0.01%\n",
      "Close_lag_50: 0.01%\n",
      "MACD: 0.01%\n",
      "EMA_20_lag_40: 0.01%\n",
      "Close_lag_10: 0.01%\n",
      "EMA_20_lag_50: 0.01%\n",
      "SMA_5_lag_30: 0.01%\n",
      "SMA_50_lag_10: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "MACD_Histogram: 0.01%\n",
      "EMA_50_lag_20: 0.01%\n",
      "RSI: 0.01%\n",
      "10_day_Fib_23: 0.01%\n",
      "%D: 0.01%\n",
      "Close_lag_5: 0.01%\n",
      "Volume_lag_90: 0.01%\n",
      "Volume_lag_15: 0.01%\n",
      "SMA_20_lag_3: 0.01%\n",
      "EMA_20_lag_15: 0.01%\n",
      "SMA_5_lag_90: 0.01%\n",
      "EMA_12_MACD_lag_50: 0.01%\n",
      "SMA_20_lag_75: 0.01%\n",
      "EMA_20_lag_75: 0.01%\n",
      "EMA_50_lag_60: 0.01%\n",
      "SMA_50_lag_1: 0.01%\n",
      "SMA_50_lag_60: 0.01%\n",
      "EMA_26_MACD_lag_60: 0.01%\n",
      "Close_lag_1: 0.01%\n",
      "Volume_lag_7: 0.01%\n",
      "SMA_20_lag_90: 0.01%\n",
      "Close_lag_75: 0.01%\n",
      "SMA_50_lag_15: 0.00%\n",
      "SMA_20: 0.00%\n",
      "EMA_26_MACD_lag_20: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "EMA_20_lag_60: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "Close_lag_30: 0.00%\n",
      "SMA_50_lag_30: 0.00%\n",
      "EMA_26_MACD_lag_1: 0.00%\n",
      "Close_lag_40: 0.00%\n",
      "SMA_5_lag_75: 0.00%\n",
      "EMA_26_MACD_lag_90: 0.00%\n",
      "EMA_12_MACD_lag_60: 0.00%\n",
      "EMA_26_MACD_lag_75: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "%K: 0.00%\n",
      "EMA_12_MACD_lag_75: 0.00%\n",
      "SMA_20_lag_180: 0.00%\n",
      "SMA_5_lag_180: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "30_day_Fib_38: 0.00%\n",
      "30_day_Fib_50: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "Close_lag_25: 0.00%\n",
      "Close_lag_60: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "SMA_5_lag_25: 0.00%\n",
      "SMA_5_lag_40: 0.00%\n",
      "SMA_5_lag_50: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "EMA_5_lag_90: 0.00%\n",
      "EMA_5_lag_180: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "SMA_20_lag_20: 0.00%\n",
      "SMA_20_lag_25: 0.00%\n",
      "SMA_20_lag_50: 0.00%\n",
      "SMA_50_lag_5: 0.00%\n",
      "SMA_50_lag_7: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "EMA_20_lag_10: 0.00%\n",
      "EMA_20_lag_20: 0.00%\n",
      "EMA_20_lag_25: 0.00%\n",
      "EMA_20_lag_30: 0.00%\n",
      "EMA_50_lag_3: 0.00%\n",
      "EMA_50_lag_5: 0.00%\n",
      "EMA_50_lag_7: 0.00%\n",
      "EMA_50_lag_10: 0.00%\n",
      "EMA_50_lag_50: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "EMA_12_MACD_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_30: 0.00%\n",
      "EMA_12_MACD_lag_90: 0.00%\n",
      "EMA_26_MACD_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_7: 0.00%\n",
      "EMA_26_MACD_lag_15: 0.00%\n",
      "EMA_26_MACD_lag_25: 0.00%\n",
      "EMA_26_MACD_lag_30: 0.00%\n",
      "EMA_26_MACD_lag_50: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_6_month_md_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "faba1716-99b9-4fad-a2bc-74cbb5e5f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/12738486.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-120)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/12738486.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (406528, 205)\n",
      "Testing data shape: (270615, 205)\n",
      "X_train shape: (406528, 201), y_train shape: (406528,)\n",
      "X_test shape: (270615, 201), y_test shape: (270615,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 43856.30757644138\n"
     ]
    }
   ],
   "source": [
    "# 6 month prediction model\n",
    "# learning rate = 0.01\n",
    "# max_depth = 7\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_6_month = df_stock_data_6_month.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_6_month[df_stock_data_6_month['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_6_month[df_stock_data_6_month['Date'] > '2024-01-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-120)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-120)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_6_month_md_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_6_month_md_7.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_6_month_md_7.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6f3b5029-bb9b-4f8d-8e6b-111edd9a4733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 43856.30757644138\n",
      "Mean Absolute Error on unseen data: 40.75621874158073\n",
      "Root Mean Squared Error on unseen data: 209.41897616128625\n",
      "R-squared on unseen data: 0.8205684493026995\n",
      "Median Absolute Error on unseen data: 9.070610046386719\n",
      "Durbin-Watson Statistic on unseen data: 0.02741931648833066\n",
      "MAPE on unseen data: 20.17%\n",
      "Fib_30_High_Max: 45.49%\n",
      "EMA_12_MACD: 10.16%\n",
      "Fib_5_High_Max: 6.96%\n",
      "Middle_Band: 6.71%\n",
      "EMA_5: 6.36%\n",
      "Fib_5_Low_Min: 4.69%\n",
      "Fib_10_High_Max: 3.66%\n",
      "Volume: 3.49%\n",
      "VWAP: 1.42%\n",
      "EMA_50: 1.20%\n",
      "30_day_Fib_61: 1.15%\n",
      "High: 1.00%\n",
      "Low: 0.95%\n",
      "5_day-Fib_23: 0.78%\n",
      "10_day_Fib_50: 0.59%\n",
      "10_day_Fib_23: 0.56%\n",
      "Cumulative_Price_Volume: 0.45%\n",
      "SMA_20_lag_30: 0.40%\n",
      "Fib_10_Low_Min: 0.31%\n",
      "EMA_26_MACD: 0.30%\n",
      "5_day-Fib_61: 0.26%\n",
      "EMA_12_MACD_lag_3: 0.25%\n",
      "SMA_50: 0.24%\n",
      "Fib_30_Low_Min: 0.23%\n",
      "EMA_50_lag_25: 0.22%\n",
      "EMA_50_lag_90: 0.20%\n",
      "5_day-Fib_38: 0.15%\n",
      "EMA_50_lag_75: 0.09%\n",
      "Lower_Band: 0.07%\n",
      "SMA_50_lag_25: 0.07%\n",
      "EMA_26_MACD_lag_60: 0.07%\n",
      "SMA_5: 0.06%\n",
      "EMA_26_MACD_lag_180: 0.06%\n",
      "EMA_50_lag_30: 0.05%\n",
      "EMA_26_MACD_lag_75: 0.05%\n",
      "10_day_Fib_38: 0.04%\n",
      "SMA_50_lag_1: 0.04%\n",
      "EMA_50_lag_60: 0.04%\n",
      "Cumulative_Volume: 0.04%\n",
      "EMA_20: 0.04%\n",
      "SMA_20: 0.03%\n",
      "SMA_50_lag_50: 0.03%\n",
      "EMA_12_MACD_lag_15: 0.03%\n",
      "SMA_50_lag_60: 0.02%\n",
      "EMA_12_MACD_lag_50: 0.02%\n",
      "EMA_12_MACD_lag_180: 0.02%\n",
      "EMA_26_MACD_lag_40: 0.02%\n",
      "EMA_20_lag_180: 0.02%\n",
      "Close_lag_60: 0.02%\n",
      "EMA_20_lag_60: 0.02%\n",
      "EMA_5_lag_60: 0.02%\n",
      "EMA_5_lag_180: 0.02%\n",
      "EMA_50_lag_180: 0.02%\n",
      "Close_lag_75: 0.02%\n",
      "SMA_5_lag_60: 0.02%\n",
      "EMA_5_lag_90: 0.02%\n",
      "SMA_5_lag_180: 0.02%\n",
      "5_day-Fib_50: 0.01%\n",
      "Volume_lag_1: 0.01%\n",
      "EMA_12_MACD_lag_60: 0.01%\n",
      "30_day_Fib_23: 0.01%\n",
      "10_day_Fib_61: 0.01%\n",
      "Volume_lag_20: 0.01%\n",
      "SMA_20_lag_60: 0.01%\n",
      "SMA_50_lag_180: 0.01%\n",
      "Close_lag_90: 0.01%\n",
      "Close_lag_15: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "SMA_50_lag_3: 0.01%\n",
      "SMA_20_lag_3: 0.01%\n",
      "EMA_26_MACD_lag_20: 0.01%\n",
      "EMA_50_lag_10: 0.01%\n",
      "ATR: 0.01%\n",
      "EMA_50_lag_50: 0.01%\n",
      "Volume_lag_180: 0.01%\n",
      "SMA_50_lag_20: 0.01%\n",
      "SMA_20_lag_75: 0.01%\n",
      "SMA_50_lag_7: 0.01%\n",
      "SMA_50_lag_30: 0.01%\n",
      "30_day_Fib_50: 0.01%\n",
      "Std_Dev: 0.01%\n",
      "SMA_50_lag_75: 0.01%\n",
      "Close_lag_180: 0.01%\n",
      "EMA_26_MACD_lag_50: 0.01%\n",
      "Volume_lag_25: 0.01%\n",
      "EMA_20_lag_75: 0.01%\n",
      "SMA_20_lag_180: 0.01%\n",
      "EMA_12_MACD_lag_75: 0.01%\n",
      "SMA_50_lag_90: 0.01%\n",
      "EMA_5_lag_50: 0.01%\n",
      "EMA_50_lag_15: 0.01%\n",
      "SMA_5_lag_1: 0.01%\n",
      "SMA_50_lag_40: 0.01%\n",
      "EMA_20_lag_1: 0.01%\n",
      "EMA_50_lag_40: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "EMA_20_lag_30: 0.01%\n",
      "Close_lag_50: 0.01%\n",
      "EMA_50_lag_20: 0.01%\n",
      "SMA_20_lag_90: 0.01%\n",
      "MACD: 0.01%\n",
      "Upper_Band: 0.01%\n",
      "SMA_5_lag_90: 0.01%\n",
      "Volume_lag_90: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "30_day_Fib_38: 0.01%\n",
      "EMA_12_MACD_lag_1: 0.01%\n",
      "Volume_lag_15: 0.01%\n",
      "SMA_20_lag_1: 0.01%\n",
      "Volume_lag_60: 0.01%\n",
      "EMA_26_MACD_lag_3: 0.01%\n",
      "ATR_True_Range: 0.01%\n",
      "EMA_12_MACD_lag_20: 0.01%\n",
      "EMA_26_MACD_lag_90: 0.01%\n",
      "EMA_26_MACD_lag_1: 0.01%\n",
      "SMA_50_lag_15: 0.01%\n",
      "SMA_5_lag_40: 0.00%\n",
      "EMA_12_MACD_lag_90: 0.00%\n",
      "EMA_20_lag_90: 0.00%\n",
      "EMA_20_lag_50: 0.00%\n",
      "Volume_lag_7: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "SMA_20_lag_15: 0.00%\n",
      "EMA_20_lag_20: 0.00%\n",
      "Volume_lag_3: 0.00%\n",
      "SMA_20_lag_50: 0.00%\n",
      "EMA_12_MACD_lag_25: 0.00%\n",
      "EMA_5_lag_75: 0.00%\n",
      "SMA_50_lag_5: 0.00%\n",
      "SMA_5_lag_75: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "EMA_26_MACD_lag_30: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "EMA_50_lag_3: 0.00%\n",
      "SMA_50_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_40: 0.00%\n",
      "EMA_26_MACD_lag_25: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "Close_lag_25: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "EMA_50_lag_1: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n",
      "EMA_20_lag_25: 0.00%\n",
      "EMA_50_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_30: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "SMA_20_lag_25: 0.00%\n",
      "SMA_20_lag_40: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_7: 0.00%\n",
      "SMA_5_lag_30: 0.00%\n",
      "SMA_20_lag_20: 0.00%\n",
      "EMA_20_lag_40: 0.00%\n",
      "Volume_lag_30: 0.00%\n",
      "Volume_lag_40: 0.00%\n",
      "ATR_High_Low: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "Volume_lag_50: 0.00%\n",
      "Volume_lag_75: 0.00%\n",
      "ATR_Prev_Close: 0.00%\n",
      "EMA_50_lag_7: 0.00%\n",
      "%K: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "SMA_5_lag_50: 0.00%\n",
      "EMA_5_lag_40: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "EMA_26_MACD_lag_15: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "Close_lag_40: 0.00%\n",
      "EMA_20_lag_10: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "%D: 0.00%\n",
      "Close_lag_30: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "RSI: 0.00%\n",
      "SMA_5_lag_25: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_6_month_md_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed1f03-4d19-4618-812d-a4795292834d",
   "metadata": {},
   "source": [
    "best model: learning_rate = 0.01 and max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "521210ee-88e2-428b-baf3-8a851df6cdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert to NumPy arrays (ensuring correct types)\n",
    "features = np.array([feature for feature, importance in sorted_features[:5]])  # Extract feature names\n",
    "importances = np.array([importance for feature, importance in sorted_features[:5]])  # Extract importances\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(13, 8))\n",
    "ax = sns.barplot(x=importances * 100, y=features, palette=\"viridis\")\n",
    "\n",
    "# Add text labels to the bars (feature importance values)\n",
    "for i, v in enumerate(importances * 100):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}%\", va=\"center\", fontsize=16)  # Adjust position & format\n",
    "\n",
    "# Format x-axis labels to include % sign\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "\n",
    "# Extend x-axis limits for more space\n",
    "plt.xlim(0, max(importances * 100) + 3)  # Extend to provide more space on the right\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance (%)\", fontsize=18, fontweight='bold')  # Bigger x-axis title\n",
    "plt.ylabel(\"Important TA Indicators\", fontsize=18, fontweight='bold')  # Bigger y-axis title\n",
    "plt.title(\"Best 6 Month Prediction Model: Top 5 Most Important Features\", fontsize=18, fontweight='bold')  # Bigger title\n",
    "\n",
    "# Increase font size for y-axis and x-axis tick labels (feature names)\n",
    "ax.set_yticklabels(features, fontsize=14)\n",
    "plt.xticks(fontsize=14)  # Increase font size for x-axis labels\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45138d-22a4-4053-9b16-5c5b4c8486ca",
   "metadata": {},
   "source": [
    "now we create our final model: 1 year price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0e44f246-6f86-4237-86ff-411f0f201e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will modify our feature set to add bigger lagging indicators.\n",
    "# Create a new dataframe called 'df_stock_data_1_year' as a copy of 'df_stocks_price_ta'\n",
    "df_stock_data_1_year = df_stocks_price_ta.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bdee4b54-352a-4cc6-9738-5f3a06553828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/454538710.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_5</th>\n",
       "      <th>...</th>\n",
       "      <th>EMA_26_MACD_lag_20</th>\n",
       "      <th>EMA_26_MACD_lag_25</th>\n",
       "      <th>EMA_26_MACD_lag_30</th>\n",
       "      <th>EMA_26_MACD_lag_40</th>\n",
       "      <th>EMA_26_MACD_lag_50</th>\n",
       "      <th>EMA_26_MACD_lag_60</th>\n",
       "      <th>EMA_26_MACD_lag_75</th>\n",
       "      <th>EMA_26_MACD_lag_90</th>\n",
       "      <th>EMA_26_MACD_lag_180</th>\n",
       "      <th>EMA_26_MACD_lag_360</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>10.52</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1037700.0</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>10.09</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.02</td>\n",
       "      <td>480300.0</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.305000</td>\n",
       "      <td>10.376667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.46</td>\n",
       "      <td>9.97</td>\n",
       "      <td>724400.0</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.276667</td>\n",
       "      <td>10.324445</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.57</td>\n",
       "      <td>10.22</td>\n",
       "      <td>758700.0</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.347500</td>\n",
       "      <td>10.402963</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-02-16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>10.55</td>\n",
       "      <td>10.39</td>\n",
       "      <td>685200.0</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.366000</td>\n",
       "      <td>10.415309</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol        Date  Close   High    Low     Volume      SMA_5     SMA_20  \\\n",
       "0   AAPL  2022-02-10  10.52  10.73  10.20  1037700.0  10.520000  10.520000   \n",
       "1   AAPL  2022-02-11  10.09  10.49  10.02   480300.0  10.305000  10.305000   \n",
       "2   AAPL  2022-02-14  10.22  10.46   9.97   724400.0  10.276667  10.276667   \n",
       "3   AAPL  2022-02-15  10.56  10.57  10.22   758700.0  10.347500  10.347500   \n",
       "4   AAPL  2022-02-16  10.44  10.55  10.39   685200.0  10.366000  10.366000   \n",
       "\n",
       "      SMA_50      EMA_5  ...  EMA_26_MACD_lag_20  EMA_26_MACD_lag_25  \\\n",
       "0  10.520000  10.520000  ...                 NaN                 NaN   \n",
       "1  10.305000  10.376667  ...                 NaN                 NaN   \n",
       "2  10.276667  10.324445  ...                 NaN                 NaN   \n",
       "3  10.347500  10.402963  ...                 NaN                 NaN   \n",
       "4  10.366000  10.415309  ...                 NaN                 NaN   \n",
       "\n",
       "   EMA_26_MACD_lag_30  EMA_26_MACD_lag_40  EMA_26_MACD_lag_50  \\\n",
       "0                 NaN                 NaN                 NaN   \n",
       "1                 NaN                 NaN                 NaN   \n",
       "2                 NaN                 NaN                 NaN   \n",
       "3                 NaN                 NaN                 NaN   \n",
       "4                 NaN                 NaN                 NaN   \n",
       "\n",
       "   EMA_26_MACD_lag_60  EMA_26_MACD_lag_75  EMA_26_MACD_lag_90  \\\n",
       "0                 NaN                 NaN                 NaN   \n",
       "1                 NaN                 NaN                 NaN   \n",
       "2                 NaN                 NaN                 NaN   \n",
       "3                 NaN                 NaN                 NaN   \n",
       "4                 NaN                 NaN                 NaN   \n",
       "\n",
       "   EMA_26_MACD_lag_180  EMA_26_MACD_lag_360  \n",
       "0                  NaN                  NaN  \n",
       "1                  NaN                  NaN  \n",
       "2                  NaN                  NaN  \n",
       "3                  NaN                  NaN  \n",
       "4                  NaN                  NaN  \n",
       "\n",
       "[5 rows x 214 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of columns to create lags for (focusing on mid-term indicators)\n",
    "columns_to_lag = ['Close', 'SMA_5', 'EMA_5', 'Volume', 'SMA_20',\n",
    "       'SMA_50', 'EMA_5', 'EMA_20', 'EMA_50',  'EMA_12_MACD',\n",
    "       'EMA_26_MACD']\n",
    "\n",
    "# Creating lag features for each column\n",
    "# [1, 3, 5, 7, 10, 12, 15, 20, 30, 60, 90, 180, 360] are the lags we will use\n",
    "# but to save space, we will only use necessary lags per the timeline goal of the model\n",
    "# this first model will be predicting price 1 week ahead (5 trading days)\n",
    "lags = [1, 3, 5, 7, 10, 15, 20, 25, 30, 40, 50, 60, 75, 90, 180, 360]\n",
    "for col in columns_to_lag:\n",
    "    for lag in lags:\n",
    "        df_stock_data_1_year[f'{col}_lag_{lag}'] = df_stock_data_1_year[col].shift(lag)\n",
    "\n",
    "# Do not drop NaN values to maintain continuity (XGBoost can handle NaNs)\n",
    "# You can handle missing values in your model later, if needed\n",
    "df_stock_data_1_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a72037a4-8045-4466-9a8d-a51ad6811af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1348732154.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-240)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1348732154.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-240)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1348732154.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-240)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/1348732154.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (200032, 215)\n",
      "Testing data shape: (282929, 215)\n",
      "X_train shape: (200032, 211), y_train shape: (200032,)\n",
      "X_test shape: (282929, 211), y_test shape: (282929,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 39254.946575320864\n"
     ]
    }
   ],
   "source": [
    "# now we're going to move onto our next model: 1 year prediction\n",
    "# we'll start at our baseline model and then do the same as we just did\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_year = df_stock_data_1_year.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_1_year[df_stock_data_1_year['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_year[df_stock_data_1_year['Date'] > '2023-07-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-240)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-240)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_year = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_year.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_1_year.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0e494a7a-a645-4733-8498-bafa7b572f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 39254.946575320864\n",
      "Mean Absolute Error on unseen data: 51.15177988265191\n",
      "Root Mean Squared Error on unseen data: 198.12861119818325\n",
      "R-squared on unseen data: 0.8384774960622475\n",
      "Median Absolute Error on unseen data: 14.205612182617188\n",
      "Durbin-Watson Statistic on unseen data: 0.013377636191698287\n",
      "MAPE on unseen data: 28.78%\n",
      "Fib_30_High_Max: 60.76%\n",
      "Volume: 10.58%\n",
      "Fib_5_High_Max: 6.22%\n",
      "SMA_50: 5.11%\n",
      "VWAP: 3.20%\n",
      "EMA_5: 2.75%\n",
      "EMA_50: 1.20%\n",
      "Low: 1.06%\n",
      "Fib_10_Low_Min: 0.90%\n",
      "Fib_30_Low_Min: 0.85%\n",
      "EMA_20: 0.68%\n",
      "Cumulative_Price_Volume: 0.67%\n",
      "Middle_Band: 0.65%\n",
      "Fib_5_Low_Min: 0.37%\n",
      "EMA_26_MACD_lag_1: 0.34%\n",
      "Lower_Band: 0.27%\n",
      "30_day_Fib_61: 0.26%\n",
      "Volume_lag_1: 0.25%\n",
      "EMA_12_MACD_lag_30: 0.21%\n",
      "Volume_lag_3: 0.16%\n",
      "EMA_50_lag_360: 0.15%\n",
      "High: 0.12%\n",
      "EMA_20_lag_360: 0.11%\n",
      "Std_Dev: 0.11%\n",
      "EMA_50_lag_60: 0.09%\n",
      "5_day-Fib_61: 0.08%\n",
      "SMA_5_lag_360: 0.08%\n",
      "EMA_26_MACD_lag_25: 0.08%\n",
      "EMA_26_MACD_lag_360: 0.07%\n",
      "SMA_50_lag_25: 0.06%\n",
      "Volume_lag_25: 0.06%\n",
      "10_day_Fib_61: 0.06%\n",
      "ATR: 0.06%\n",
      "EMA_12_MACD_lag_60: 0.06%\n",
      "EMA_5_lag_360: 0.05%\n",
      "Upper_Band: 0.05%\n",
      "Volume_lag_360: 0.05%\n",
      "SMA_50_lag_360: 0.05%\n",
      "Volume_lag_40: 0.05%\n",
      "SMA_20_lag_50: 0.05%\n",
      "Volume_lag_50: 0.05%\n",
      "ATR_High_Low: 0.04%\n",
      "EMA_5_lag_60: 0.04%\n",
      "5_day-Fib_50: 0.04%\n",
      "SMA_50_lag_60: 0.04%\n",
      "EMA_12_MACD: 0.03%\n",
      "EMA_12_MACD_lag_360: 0.03%\n",
      "EMA_50_lag_5: 0.03%\n",
      "Close_lag_360: 0.03%\n",
      "EMA_50_lag_30: 0.03%\n",
      "Volume_lag_60: 0.03%\n",
      "Close_lag_25: 0.03%\n",
      "EMA_50_lag_90: 0.03%\n",
      "30_day_Fib_50: 0.02%\n",
      "EMA_50_lag_50: 0.02%\n",
      "EMA_20_lag_60: 0.02%\n",
      "EMA_20_lag_180: 0.02%\n",
      "EMA_12_MACD_lag_180: 0.02%\n",
      "EMA_50_lag_20: 0.02%\n",
      "EMA_20_lag_3: 0.02%\n",
      "Volume_lag_30: 0.02%\n",
      "SMA_20_lag_360: 0.02%\n",
      "SMA_50_lag_75: 0.02%\n",
      "SMA_50_lag_180: 0.02%\n",
      "SMA_20_lag_25: 0.02%\n",
      "EMA_50_lag_180: 0.02%\n",
      "EMA_26_MACD_lag_180: 0.02%\n",
      "EMA_50_lag_40: 0.02%\n",
      "MACD: 0.02%\n",
      "Volume_lag_7: 0.02%\n",
      "Fib_10_High_Max: 0.02%\n",
      "30_day_Fib_23: 0.02%\n",
      "Close_lag_50: 0.02%\n",
      "Close_lag_75: 0.02%\n",
      "EMA_26_MACD_lag_7: 0.02%\n",
      "SMA_5_lag_50: 0.02%\n",
      "EMA_20_lag_20: 0.02%\n",
      "SMA_20_lag_180: 0.02%\n",
      "Volume_lag_15: 0.02%\n",
      "ATR_Prev_Close: 0.02%\n",
      "SMA_20_lag_15: 0.01%\n",
      "SMA_50_lag_1: 0.01%\n",
      "EMA_5_lag_75: 0.01%\n",
      "Cumulative_Volume: 0.01%\n",
      "SMA_50_lag_30: 0.01%\n",
      "ATR_True_Range: 0.01%\n",
      "EMA_26_MACD: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "EMA_26_MACD_lag_90: 0.01%\n",
      "Close_lag_15: 0.01%\n",
      "SMA_20_lag_75: 0.01%\n",
      "SMA_5_lag_7: 0.01%\n",
      "EMA_12_MACD_lag_40: 0.01%\n",
      "Volume_lag_20: 0.01%\n",
      "SMA_20_lag_30: 0.01%\n",
      "EMA_26_MACD_lag_40: 0.01%\n",
      "EMA_50_lag_3: 0.01%\n",
      "SMA_50_lag_90: 0.01%\n",
      "EMA_50_lag_15: 0.01%\n",
      "Volume_lag_90: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "EMA_12_MACD_lag_50: 0.01%\n",
      "SMA_20_lag_60: 0.01%\n",
      "SMA_5_lag_180: 0.01%\n",
      "EMA_5_lag_30: 0.01%\n",
      "SMA_50_lag_50: 0.01%\n",
      "SMA_5_lag_75: 0.01%\n",
      "EMA_26_MACD_lag_20: 0.01%\n",
      "EMA_12_MACD_lag_90: 0.01%\n",
      "EMA_20_lag_50: 0.01%\n",
      "EMA_26_MACD_lag_60: 0.01%\n",
      "Close_lag_60: 0.01%\n",
      "EMA_50_lag_75: 0.01%\n",
      "EMA_5_lag_50: 0.01%\n",
      "Close_lag_90: 0.01%\n",
      "SMA_50_lag_15: 0.01%\n",
      "SMA_20_lag_40: 0.01%\n",
      "EMA_50_lag_1: 0.01%\n",
      "EMA_12_MACD_lag_7: 0.01%\n",
      "EMA_20_lag_75: 0.01%\n",
      "SMA_20_lag_90: 0.01%\n",
      "EMA_26_MACD_lag_5: 0.01%\n",
      "Volume_lag_180: 0.01%\n",
      "SMA_50_lag_40: 0.01%\n",
      "SMA_50_lag_10: 0.01%\n",
      "EMA_20_lag_40: 0.01%\n",
      "30_day_Fib_38: 0.01%\n",
      "SMA_5_lag_15: 0.01%\n",
      "EMA_5_lag_180: 0.01%\n",
      "EMA_5_lag_40: 0.01%\n",
      "EMA_26_MACD_lag_75: 0.01%\n",
      "Close_lag_180: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "EMA_20_lag_25: 0.01%\n",
      "SMA_20_lag_20: 0.01%\n",
      "Close_lag_40: 0.01%\n",
      "%D: 0.01%\n",
      "SMA_50_lag_3: 0.01%\n",
      "5_day-Fib_23: 0.01%\n",
      "EMA_5_lag_10: 0.01%\n",
      "EMA_12_MACD_lag_10: 0.01%\n",
      "EMA_20_lag_10: 0.01%\n",
      "%K: 0.01%\n",
      "EMA_12_MACD_lag_20: 0.01%\n",
      "SMA_50_lag_20: 0.01%\n",
      "EMA_5_lag_7: 0.01%\n",
      "SMA_20_lag_7: 0.01%\n",
      "SMA_5_lag_90: 0.01%\n",
      "EMA_20_lag_90: 0.01%\n",
      "SMA_5_lag_60: 0.01%\n",
      "EMA_12_MACD_lag_75: 0.01%\n",
      "EMA_5_lag_1: 0.01%\n",
      "5_day-Fib_38: 0.01%\n",
      "SMA_20_lag_1: 0.01%\n",
      "EMA_26_MACD_lag_30: 0.01%\n",
      "EMA_50_lag_7: 0.01%\n",
      "SMA_50_lag_5: 0.01%\n",
      "10_day_Fib_23: 0.01%\n",
      "SMA_20_lag_5: 0.01%\n",
      "SMA_5_lag_20: 0.01%\n",
      "EMA_26_MACD_lag_50: 0.01%\n",
      "EMA_26_MACD_lag_15: 0.01%\n",
      "SMA_5: 0.01%\n",
      "EMA_5_lag_5: 0.01%\n",
      "EMA_12_MACD_lag_15: 0.01%\n",
      "MACD_Histogram: 0.01%\n",
      "Volume_lag_75: 0.01%\n",
      "SMA_50_lag_7: 0.01%\n",
      "EMA_20_lag_1: 0.01%\n",
      "Close_lag_30: 0.01%\n",
      "SMA_5_lag_5: 0.01%\n",
      "EMA_12_MACD_lag_1: 0.01%\n",
      "EMA_5_lag_15: 0.01%\n",
      "SMA_5_lag_10: 0.01%\n",
      "EMA_20_lag_30: 0.01%\n",
      "EMA_50_lag_25: 0.01%\n",
      "EMA_12_MACD_lag_25: 0.01%\n",
      "10_day_Fib_50: 0.01%\n",
      "EMA_5_lag_90: 0.01%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "SMA_5_lag_30: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n",
      "SMA_20: 0.00%\n",
      "SMA_5_lag_25: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "SMA_5_lag_40: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_3: 0.00%\n",
      "10_day_Fib_38: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "EMA_50_lag_10: 0.00%\n",
      "RSI: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_1_year.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d1d4747e-4e28-41cf-8d44-deb55c433333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2075373070.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-240)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2075373070.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (200032, 215)\n",
      "Testing data shape: (282929, 215)\n",
      "X_train shape: (200032, 211), y_train shape: (200032,)\n",
      "X_test shape: (282929, 211), y_test shape: (282929,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 38378.36210765123\n"
     ]
    }
   ],
   "source": [
    "# 1 year prediction\n",
    "# learning_rate = 0.1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_year = df_stock_data_1_year.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_1_year[df_stock_data_1_year['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_year[df_stock_data_1_year['Date'] > '2023-07-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-240)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-240)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_year_lr_1 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_year_lr_1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_1_year_lr_1.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2dec4a6b-39f8-4056-8018-9a1faaba6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 38378.36210765123\n",
      "Mean Absolute Error on unseen data: 50.46652074173195\n",
      "Root Mean Squared Error on unseen data: 195.90396143940333\n",
      "R-squared on unseen data: 0.8420843820851152\n",
      "Median Absolute Error on unseen data: 14.063949584960938\n",
      "Durbin-Watson Statistic on unseen data: 0.013791500419471102\n",
      "MAPE on unseen data: 28.88%\n",
      "Fib_30_High_Max: 63.71%\n",
      "Volume: 9.72%\n",
      "Fib_5_High_Max: 8.05%\n",
      "SMA_50: 5.77%\n",
      "VWAP: 2.90%\n",
      "Low: 1.16%\n",
      "EMA_5: 0.87%\n",
      "Fib_30_Low_Min: 0.83%\n",
      "Fib_10_Low_Min: 0.74%\n",
      "Cumulative_Price_Volume: 0.63%\n",
      "EMA_20: 0.62%\n",
      "EMA_50: 0.61%\n",
      "Fib_5_Low_Min: 0.31%\n",
      "Lower_Band: 0.29%\n",
      "EMA_12_MACD_lag_30: 0.25%\n",
      "EMA_26_MACD_lag_25: 0.18%\n",
      "Volume_lag_1: 0.17%\n",
      "EMA_50_lag_360: 0.16%\n",
      "30_day_Fib_61: 0.15%\n",
      "Std_Dev: 0.13%\n",
      "Volume_lag_3: 0.12%\n",
      "EMA_5_lag_360: 0.10%\n",
      "EMA_50_lag_60: 0.08%\n",
      "5_day-Fib_61: 0.07%\n",
      "Volume_lag_360: 0.06%\n",
      "EMA_12_MACD_lag_360: 0.06%\n",
      "High: 0.06%\n",
      "Volume_lag_40: 0.06%\n",
      "ATR: 0.05%\n",
      "SMA_50_lag_360: 0.05%\n",
      "EMA_20_lag_360: 0.05%\n",
      "SMA_20: 0.05%\n",
      "SMA_5_lag_360: 0.04%\n",
      "EMA_5_lag_60: 0.04%\n",
      "EMA_26_MACD_lag_360: 0.04%\n",
      "SMA_50_lag_25: 0.04%\n",
      "SMA_50_lag_60: 0.04%\n",
      "5_day-Fib_50: 0.03%\n",
      "Volume_lag_50: 0.03%\n",
      "EMA_12_MACD_lag_180: 0.03%\n",
      "EMA_50_lag_5: 0.03%\n",
      "EMA_50_lag_50: 0.03%\n",
      "SMA_20_lag_50: 0.03%\n",
      "Upper_Band: 0.03%\n",
      "SMA_20_lag_25: 0.03%\n",
      "SMA_20_lag_360: 0.03%\n",
      "Volume_lag_25: 0.03%\n",
      "ATR_High_Low: 0.02%\n",
      "EMA_20_lag_3: 0.02%\n",
      "SMA_50_lag_50: 0.02%\n",
      "SMA_50_lag_180: 0.02%\n",
      "SMA_20_lag_10: 0.02%\n",
      "SMA_50_lag_75: 0.02%\n",
      "EMA_50_lag_20: 0.02%\n",
      "Volume_lag_60: 0.02%\n",
      "SMA_20_lag_30: 0.02%\n",
      "Close_lag_25: 0.02%\n",
      "Volume_lag_15: 0.02%\n",
      "EMA_26_MACD_lag_90: 0.02%\n",
      "EMA_5_lag_180: 0.02%\n",
      "EMA_50_lag_180: 0.02%\n",
      "EMA_50_lag_15: 0.02%\n",
      "EMA_12_MACD_lag_60: 0.02%\n",
      "30_day_Fib_23: 0.02%\n",
      "EMA_50_lag_30: 0.02%\n",
      "MACD: 0.02%\n",
      "SMA_50_lag_30: 0.02%\n",
      "Volume_lag_5: 0.02%\n",
      "Close_lag_60: 0.02%\n",
      "SMA_20_lag_75: 0.01%\n",
      "Volume_lag_30: 0.01%\n",
      "EMA_20_lag_90: 0.01%\n",
      "Fib_10_High_Max: 0.01%\n",
      "Volume_lag_90: 0.01%\n",
      "EMA_20_lag_50: 0.01%\n",
      "Close_lag_360: 0.01%\n",
      "EMA_12_MACD_lag_1: 0.01%\n",
      "EMA_20_lag_180: 0.01%\n",
      "5_day-Fib_23: 0.01%\n",
      "EMA_12_MACD_lag_50: 0.01%\n",
      "Volume_lag_7: 0.01%\n",
      "Middle_Band: 0.01%\n",
      "EMA_50_lag_75: 0.01%\n",
      "Close_lag_15: 0.01%\n",
      "Cumulative_Volume: 0.01%\n",
      "EMA_12_MACD_lag_90: 0.01%\n",
      "EMA_50_lag_90: 0.01%\n",
      "30_day_Fib_50: 0.01%\n",
      "EMA_12_MACD: 0.01%\n",
      "EMA_5_lag_75: 0.01%\n",
      "EMA_12_MACD_lag_10: 0.01%\n",
      "SMA_20_lag_180: 0.01%\n",
      "EMA_50_lag_40: 0.01%\n",
      "EMA_26_MACD_lag_1: 0.01%\n",
      "SMA_50_lag_1: 0.01%\n",
      "SMA_5_lag_40: 0.01%\n",
      "ATR_Prev_Close: 0.01%\n",
      "EMA_50_lag_7: 0.01%\n",
      "SMA_5_lag_180: 0.01%\n",
      "SMA_50_lag_90: 0.01%\n",
      "EMA_26_MACD_lag_7: 0.01%\n",
      "EMA_26_MACD_lag_60: 0.01%\n",
      "SMA_5_lag_7: 0.01%\n",
      "SMA_20_lag_60: 0.01%\n",
      "EMA_50_lag_1: 0.01%\n",
      "EMA_12_MACD_lag_40: 0.01%\n",
      "Close_lag_7: 0.01%\n",
      "EMA_20_lag_10: 0.01%\n",
      "Close_lag_75: 0.01%\n",
      "SMA_50_lag_10: 0.01%\n",
      "EMA_26_MACD_lag_3: 0.01%\n",
      "SMA_5_lag_15: 0.01%\n",
      "EMA_26_MACD_lag_40: 0.01%\n",
      "SMA_5_lag_60: 0.01%\n",
      "SMA_50_lag_40: 0.01%\n",
      "EMA_20_lag_40: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "EMA_5_lag_15: 0.01%\n",
      "EMA_12_MACD_lag_15: 0.01%\n",
      "Volume_lag_20: 0.01%\n",
      "EMA_26_MACD: 0.01%\n",
      "SMA_20_lag_15: 0.01%\n",
      "SMA_50_lag_15: 0.01%\n",
      "EMA_50_lag_25: 0.01%\n",
      "Close_lag_50: 0.01%\n",
      "EMA_26_MACD_lag_180: 0.01%\n",
      "EMA_20_lag_60: 0.01%\n",
      "30_day_Fib_38: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "EMA_26_MACD_lag_20: 0.01%\n",
      "%D: 0.01%\n",
      "EMA_5_lag_90: 0.01%\n",
      "EMA_5_lag_40: 0.01%\n",
      "SMA_50_lag_5: 0.01%\n",
      "Close_lag_180: 0.01%\n",
      "Close_lag_90: 0.01%\n",
      "EMA_26_MACD_lag_15: 0.01%\n",
      "EMA_20_lag_75: 0.01%\n",
      "EMA_20_lag_1: 0.01%\n",
      "SMA_20_lag_90: 0.01%\n",
      "Volume_lag_75: 0.01%\n",
      "SMA_20_lag_20: 0.01%\n",
      "SMA_5_lag_90: 0.01%\n",
      "SMA_20_lag_40: 0.01%\n",
      "10_day_Fib_23: 0.01%\n",
      "Volume_lag_180: 0.01%\n",
      "SMA_5_lag_10: 0.01%\n",
      "SMA_5_lag_25: 0.01%\n",
      "SMA_5_lag_50: 0.01%\n",
      "SMA_5_lag_75: 0.01%\n",
      "EMA_26_MACD_lag_50: 0.01%\n",
      "SMA_50_lag_3: 0.01%\n",
      "EMA_50_lag_3: 0.01%\n",
      "EMA_5_lag_50: 0.01%\n",
      "%K: 0.01%\n",
      "EMA_5_lag_10: 0.01%\n",
      "EMA_20_lag_30: 0.01%\n",
      "EMA_5_lag_5: 0.01%\n",
      "EMA_50_lag_10: 0.01%\n",
      "MACD_Histogram: 0.01%\n",
      "SMA_20_lag_7: 0.01%\n",
      "ATR_True_Range: 0.01%\n",
      "Close_lag_30: 0.01%\n",
      "SMA_5_lag_5: 0.01%\n",
      "EMA_20_lag_15: 0.01%\n",
      "10_day_Fib_61: 0.01%\n",
      "SMA_50_lag_7: 0.00%\n",
      "Close_lag_40: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_20: 0.00%\n",
      "SMA_50_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_30: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "EMA_12_MACD_lag_25: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_12_MACD_lag_75: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "EMA_20_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_75: 0.00%\n",
      "EMA_20_lag_25: 0.00%\n",
      "SMA_5: 0.00%\n",
      "RSI: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "10_day_Fib_38: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "SMA_5_lag_30: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_1_year_lr_1.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1d3dda7b-36de-45c8-a18f-64af22b21e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/101592945.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-240)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/101592945.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (200032, 215)\n",
      "Testing data shape: (282929, 215)\n",
      "X_train shape: (200032, 211), y_train shape: (200032,)\n",
      "X_test shape: (282929, 211), y_test shape: (282929,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 39511.952154202365\n"
     ]
    }
   ],
   "source": [
    "# 1 year prediction\n",
    "# learning_rate = 0.01\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_year = df_stock_data_1_year.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_1_year[df_stock_data_1_year['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_year[df_stock_data_1_year['Date'] > '2023-07-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-240)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-240)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_baseline_1_year_lr_01 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_baseline_1_year_lr_01.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_baseline_1_year_lr_01.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b875f41d-5248-4358-9a67-88765378677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 39511.952154202365\n",
      "Mean Absolute Error on unseen data: 50.77260147063136\n",
      "Root Mean Squared Error on unseen data: 198.77613577641145\n",
      "R-squared on unseen data: 0.8374199940593532\n",
      "Median Absolute Error on unseen data: 13.946044921875\n",
      "Durbin-Watson Statistic on unseen data: 0.013171991251702311\n",
      "MAPE on unseen data: 28.66%\n",
      "Fib_30_High_Max: 55.01%\n",
      "Fib_5_High_Max: 9.67%\n",
      "Volume: 7.30%\n",
      "SMA_50: 6.24%\n",
      "EMA_5: 4.06%\n",
      "VWAP: 3.34%\n",
      "Fib_10_Low_Min: 1.38%\n",
      "EMA_20: 1.35%\n",
      "Fib_5_Low_Min: 1.09%\n",
      "EMA_50: 1.06%\n",
      "Fib_30_Low_Min: 0.97%\n",
      "EMA_26_MACD_lag_1: 0.70%\n",
      "Low: 0.68%\n",
      "30_day_Fib_61: 0.52%\n",
      "Cumulative_Price_Volume: 0.51%\n",
      "Lower_Band: 0.38%\n",
      "High: 0.29%\n",
      "EMA_5_lag_360: 0.27%\n",
      "EMA_12_MACD_lag_30: 0.23%\n",
      "EMA_12_MACD_lag_7: 0.23%\n",
      "SMA_5_lag_360: 0.21%\n",
      "Volume_lag_1: 0.20%\n",
      "Volume_lag_3: 0.18%\n",
      "EMA_50_lag_360: 0.16%\n",
      "EMA_12_MACD: 0.14%\n",
      "EMA_26_MACD_lag_25: 0.14%\n",
      "Std_Dev: 0.12%\n",
      "EMA_20_lag_360: 0.12%\n",
      "EMA_50_lag_60: 0.11%\n",
      "5_day-Fib_50: 0.11%\n",
      "SMA_20_lag_25: 0.11%\n",
      "Upper_Band: 0.10%\n",
      "EMA_5_lag_60: 0.09%\n",
      "EMA_26_MACD_lag_360: 0.08%\n",
      "Volume_lag_360: 0.08%\n",
      "SMA_50_lag_25: 0.07%\n",
      "EMA_50_lag_5: 0.07%\n",
      "SMA_20_lag_50: 0.07%\n",
      "SMA_50_lag_360: 0.06%\n",
      "10_day_Fib_61: 0.06%\n",
      "EMA_12_MACD_lag_60: 0.06%\n",
      "EMA_12_MACD_lag_360: 0.06%\n",
      "ATR: 0.06%\n",
      "ATR_Prev_Close: 0.06%\n",
      "SMA_50_lag_60: 0.06%\n",
      "Volume_lag_50: 0.06%\n",
      "5_day-Fib_61: 0.05%\n",
      "Fib_10_High_Max: 0.05%\n",
      "30_day_Fib_50: 0.05%\n",
      "Close_lag_1: 0.05%\n",
      "EMA_26_MACD: 0.05%\n",
      "Volume_lag_25: 0.05%\n",
      "SMA_20_lag_360: 0.04%\n",
      "EMA_20_lag_25: 0.04%\n",
      "EMA_50_lag_50: 0.04%\n",
      "Volume_lag_40: 0.03%\n",
      "EMA_5_lag_75: 0.03%\n",
      "EMA_20_lag_180: 0.03%\n",
      "SMA_50_lag_180: 0.03%\n",
      "EMA_12_MACD_lag_180: 0.03%\n",
      "SMA_20: 0.03%\n",
      "5_day-Fib_38: 0.03%\n",
      "SMA_50_lag_20: 0.03%\n",
      "Volume_lag_60: 0.03%\n",
      "ATR_High_Low: 0.03%\n",
      "Volume_lag_30: 0.02%\n",
      "ATR_True_Range: 0.02%\n",
      "EMA_50_lag_20: 0.02%\n",
      "EMA_50_lag_90: 0.02%\n",
      "EMA_5_lag_50: 0.02%\n",
      "Volume_lag_90: 0.02%\n",
      "SMA_50_lag_50: 0.02%\n",
      "EMA_50_lag_180: 0.02%\n",
      "SMA_50_lag_75: 0.02%\n",
      "EMA_26_MACD_lag_90: 0.02%\n",
      "30_day_Fib_23: 0.02%\n",
      "EMA_12_MACD_lag_75: 0.02%\n",
      "EMA_26_MACD_lag_40: 0.02%\n",
      "EMA_12_MACD_lag_1: 0.02%\n",
      "Close_lag_25: 0.02%\n",
      "EMA_20_lag_20: 0.02%\n",
      "Volume_lag_7: 0.02%\n",
      "EMA_12_MACD_lag_5: 0.02%\n",
      "SMA_50_lag_30: 0.02%\n",
      "EMA_50_lag_30: 0.02%\n",
      "SMA_50_lag_1: 0.02%\n",
      "EMA_50_lag_40: 0.02%\n",
      "SMA_5_lag_50: 0.02%\n",
      "EMA_20_lag_50: 0.02%\n",
      "Close_lag_360: 0.02%\n",
      "SMA_50_lag_90: 0.02%\n",
      "EMA_5_lag_10: 0.02%\n",
      "Close_lag_40: 0.02%\n",
      "SMA_5_lag_180: 0.02%\n",
      "MACD: 0.02%\n",
      "SMA_20_lag_75: 0.02%\n",
      "EMA_12_MACD_lag_90: 0.01%\n",
      "Volume_lag_15: 0.01%\n",
      "Close_lag_75: 0.01%\n",
      "SMA_5_lag_75: 0.01%\n",
      "Close_lag_50: 0.01%\n",
      "EMA_20_lag_90: 0.01%\n",
      "EMA_26_MACD_lag_30: 0.01%\n",
      "Cumulative_Volume: 0.01%\n",
      "EMA_26_MACD_lag_3: 0.01%\n",
      "EMA_50_lag_1: 0.01%\n",
      "EMA_50_lag_75: 0.01%\n",
      "SMA_20_lag_180: 0.01%\n",
      "SMA_50_lag_40: 0.01%\n",
      "SMA_50_lag_15: 0.01%\n",
      "SMA_5_lag_10: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "Close_lag_180: 0.01%\n",
      "EMA_5_lag_180: 0.01%\n",
      "SMA_20_lag_60: 0.01%\n",
      "EMA_26_MACD_lag_75: 0.01%\n",
      "Volume_lag_20: 0.01%\n",
      "SMA_5_lag_15: 0.01%\n",
      "EMA_20_lag_10: 0.01%\n",
      "EMA_26_MACD_lag_7: 0.01%\n",
      "Volume_lag_75: 0.01%\n",
      "Close_lag_60: 0.01%\n",
      "Volume_lag_10: 0.01%\n",
      "EMA_12_MACD_lag_40: 0.01%\n",
      "EMA_20_lag_60: 0.01%\n",
      "SMA_50_lag_10: 0.01%\n",
      "EMA_50_lag_3: 0.01%\n",
      "SMA_20_lag_40: 0.01%\n",
      "Close_lag_7: 0.01%\n",
      "EMA_12_MACD_lag_50: 0.01%\n",
      "EMA_20_lag_75: 0.01%\n",
      "EMA_12_MACD_lag_25: 0.01%\n",
      "EMA_20_lag_3: 0.01%\n",
      "MACD_Histogram: 0.01%\n",
      "EMA_26_MACD_lag_180: 0.01%\n",
      "Close_lag_15: 0.01%\n",
      "SMA_20_lag_30: 0.01%\n",
      "EMA_26_MACD_lag_60: 0.01%\n",
      "EMA_5_lag_40: 0.01%\n",
      "SMA_20_lag_15: 0.01%\n",
      "Volume_lag_180: 0.01%\n",
      "Close_lag_90: 0.01%\n",
      "SMA_20_lag_20: 0.01%\n",
      "SMA_20_lag_90: 0.01%\n",
      "30_day_Fib_38: 0.01%\n",
      "SMA_50_lag_3: 0.01%\n",
      "EMA_5_lag_30: 0.01%\n",
      "EMA_12_MACD_lag_10: 0.01%\n",
      "SMA_5_lag_40: 0.01%\n",
      "SMA_5_lag_1: 0.01%\n",
      "EMA_50_lag_7: 0.01%\n",
      "EMA_50_lag_25: 0.01%\n",
      "SMA_20_lag_1: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "EMA_5_lag_90: 0.01%\n",
      "5_day-Fib_23: 0.01%\n",
      "EMA_20_lag_7: 0.01%\n",
      "EMA_20_lag_40: 0.01%\n",
      "SMA_20_lag_10: 0.01%\n",
      "SMA_50_lag_5: 0.01%\n",
      "SMA_5_lag_7: 0.01%\n",
      "%K: 0.01%\n",
      "EMA_26_MACD_lag_50: 0.01%\n",
      "SMA_5_lag_25: 0.01%\n",
      "EMA_50_lag_15: 0.01%\n",
      "EMA_5_lag_15: 0.01%\n",
      "%D: 0.01%\n",
      "SMA_5_lag_60: 0.01%\n",
      "EMA_50_lag_10: 0.00%\n",
      "Close_lag_30: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "EMA_26_MACD_lag_20: 0.00%\n",
      "SMA_5_lag_90: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_15: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_20: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "SMA_5_lag_30: 0.00%\n",
      "10_day_Fib_38: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "RSI: 0.00%\n",
      "SMA_50_lag_7: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "SMA_5: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "EMA_20_lag_30: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_baseline_1_year_lr_01.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a28eb-8063-48d1-9900-0c3111460873",
   "metadata": {},
   "source": [
    "best model so far: learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7cbac17b-ec8b-4285-ba39-7aae160f68b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2936963303.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-240)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/2936963303.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (200032, 215)\n",
      "Testing data shape: (282929, 215)\n",
      "X_train shape: (200032, 211), y_train shape: (200032,)\n",
      "X_test shape: (282929, 211), y_test shape: (282929,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 23328.955456417185\n"
     ]
    }
   ],
   "source": [
    "# 1 year prediction\n",
    "# learning_rate = 0.1\n",
    "# max_depth = 3\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_year = df_stock_data_1_year.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_1_year[df_stock_data_1_year['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_year[df_stock_data_1_year['Date'] > '2023-07-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-240)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-240)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_year_md_3 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_year_md_3.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_1_year_md_3.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7af1b4f5-49d1-432c-9b20-fc45f018e2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 23328.955456417185\n",
      "Mean Absolute Error on unseen data: 41.786851251224135\n",
      "Root Mean Squared Error on unseen data: 152.7381925270074\n",
      "R-squared on unseen data: 0.9040082428250764\n",
      "Median Absolute Error on unseen data: 14.172431945800781\n",
      "Durbin-Watson Statistic on unseen data: 0.013576647768583278\n",
      "MAPE on unseen data: 27.70%\n",
      "Fib_30_High_Max: 31.09%\n",
      "Fib_30_Low_Min: 21.73%\n",
      "Volume: 13.27%\n",
      "Low: 8.63%\n",
      "5_day-Fib_23: 3.94%\n",
      "SMA_50: 3.26%\n",
      "VWAP: 2.78%\n",
      "EMA_50: 1.80%\n",
      "Fib_10_Low_Min: 1.46%\n",
      "Fib_5_High_Max: 1.23%\n",
      "EMA_12_MACD_lag_30: 1.12%\n",
      "High: 1.09%\n",
      "30_day_Fib_61: 0.85%\n",
      "Lower_Band: 0.61%\n",
      "EMA_5: 0.41%\n",
      "Std_Dev: 0.39%\n",
      "SMA_20_lag_25: 0.38%\n",
      "Signal_Line: 0.34%\n",
      "ATR_High_Low: 0.33%\n",
      "SMA_50_lag_50: 0.32%\n",
      "10_day_Fib_61: 0.28%\n",
      "Volume_lag_1: 0.22%\n",
      "EMA_5_lag_180: 0.19%\n",
      "Fib_5_Low_Min: 0.16%\n",
      "EMA_12_MACD_lag_180: 0.14%\n",
      "MACD: 0.14%\n",
      "Close_lag_25: 0.13%\n",
      "SMA_50_lag_1: 0.13%\n",
      "Volume_lag_3: 0.12%\n",
      "EMA_50_lag_90: 0.12%\n",
      "Cumulative_Price_Volume: 0.10%\n",
      "Volume_lag_25: 0.10%\n",
      "5_day-Fib_61: 0.10%\n",
      "EMA_26_MACD_lag_20: 0.10%\n",
      "EMA_50_lag_180: 0.09%\n",
      "EMA_50_lag_5: 0.08%\n",
      "EMA_50_lag_360: 0.08%\n",
      "Volume_lag_40: 0.07%\n",
      "SMA_20_lag_50: 0.07%\n",
      "Close_lag_180: 0.06%\n",
      "EMA_50_lag_25: 0.06%\n",
      "Cumulative_Volume: 0.06%\n",
      "EMA_5_lag_50: 0.05%\n",
      "EMA_12_MACD_lag_75: 0.05%\n",
      "EMA_50_lag_75: 0.05%\n",
      "EMA_26_MACD_lag_3: 0.05%\n",
      "Close_lag_10: 0.05%\n",
      "SMA_5_lag_50: 0.05%\n",
      "Close_lag_15: 0.05%\n",
      "EMA_26_MACD_lag_7: 0.04%\n",
      "SMA_5_lag_360: 0.04%\n",
      "ATR: 0.04%\n",
      "Close_lag_60: 0.04%\n",
      "EMA_12_MACD_lag_10: 0.04%\n",
      "Volume_lag_30: 0.04%\n",
      "10_day_Fib_50: 0.03%\n",
      "EMA_12_MACD_lag_20: 0.03%\n",
      "SMA_50_lag_40: 0.03%\n",
      "10_day_Fib_38: 0.03%\n",
      "EMA_12_MACD_lag_90: 0.03%\n",
      "Close_lag_90: 0.03%\n",
      "EMA_50_lag_50: 0.03%\n",
      "Volume_lag_75: 0.03%\n",
      "EMA_12_MACD_lag_1: 0.03%\n",
      "SMA_20_lag_90: 0.03%\n",
      "SMA_50_lag_60: 0.02%\n",
      "MACD_Histogram: 0.02%\n",
      "Close_lag_360: 0.02%\n",
      "Fib_10_High_Max: 0.02%\n",
      "EMA_5_lag_60: 0.02%\n",
      "SMA_20_lag_30: 0.02%\n",
      "SMA_50_lag_75: 0.02%\n",
      "Volume_lag_10: 0.02%\n",
      "Close_lag_50: 0.02%\n",
      "EMA_20_lag_180: 0.02%\n",
      "SMA_5_lag_1: 0.02%\n",
      "SMA_50_lag_90: 0.02%\n",
      "SMA_50_lag_180: 0.02%\n",
      "EMA_26_MACD_lag_90: 0.02%\n",
      "SMA_20_lag_180: 0.02%\n",
      "Close_lag_75: 0.02%\n",
      "Upper_Band: 0.02%\n",
      "EMA_12_MACD_lag_360: 0.02%\n",
      "EMA_20_lag_50: 0.02%\n",
      "30_day_Fib_23: 0.02%\n",
      "EMA_12_MACD_lag_50: 0.02%\n",
      "EMA_50_lag_30: 0.02%\n",
      "SMA_50_lag_360: 0.02%\n",
      "Close_lag_20: 0.02%\n",
      "EMA_12_MACD_lag_5: 0.02%\n",
      "EMA_20: 0.02%\n",
      "EMA_5_lag_10: 0.02%\n",
      "EMA_20_lag_40: 0.02%\n",
      "EMA_5_lag_40: 0.02%\n",
      "SMA_20_lag_1: 0.02%\n",
      "EMA_20_lag_10: 0.01%\n",
      "SMA_5_lag_90: 0.01%\n",
      "ATR_Prev_Close: 0.01%\n",
      "EMA_26_MACD_lag_1: 0.01%\n",
      "EMA_20_lag_7: 0.01%\n",
      "EMA_26_MACD_lag_50: 0.01%\n",
      "Volume_lag_60: 0.01%\n",
      "SMA_20: 0.01%\n",
      "EMA_12_MACD_lag_7: 0.01%\n",
      "EMA_20_lag_360: 0.01%\n",
      "SMA_20_lag_5: 0.01%\n",
      "EMA_5_lag_5: 0.01%\n",
      "SMA_20_lag_20: 0.01%\n",
      "EMA_26_MACD_lag_15: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "SMA_5_lag_10: 0.01%\n",
      "Volume_lag_7: 0.01%\n",
      "EMA_26_MACD_lag_60: 0.01%\n",
      "Close_lag_40: 0.01%\n",
      "EMA_20_lag_1: 0.01%\n",
      "EMA_20_lag_90: 0.01%\n",
      "EMA_50_lag_20: 0.01%\n",
      "EMA_50_lag_10: 0.01%\n",
      "EMA_50_lag_1: 0.01%\n",
      "Volume_lag_180: 0.01%\n",
      "EMA_26_MACD_lag_360: 0.01%\n",
      "SMA_50_lag_30: 0.01%\n",
      "EMA_50_lag_60: 0.01%\n",
      "SMA_5_lag_75: 0.01%\n",
      "SMA_20_lag_360: 0.01%\n",
      "EMA_50_lag_40: 0.01%\n",
      "EMA_26_MACD_lag_180: 0.01%\n",
      "EMA_12_MACD_lag_15: 0.01%\n",
      "EMA_12_MACD_lag_40: 0.01%\n",
      "SMA_20_lag_75: 0.01%\n",
      "EMA_50_lag_15: 0.01%\n",
      "EMA_12_MACD_lag_25: 0.01%\n",
      "Close_lag_30: 0.01%\n",
      "SMA_20_lag_15: 0.01%\n",
      "SMA_5_lag_30: 0.01%\n",
      "SMA_5_lag_40: 0.01%\n",
      "SMA_5: 0.01%\n",
      "EMA_5_lag_25: 0.01%\n",
      "SMA_5_lag_15: 0.01%\n",
      "Volume_lag_360: 0.01%\n",
      "SMA_5_lag_25: 0.01%\n",
      "SMA_50_lag_5: 0.01%\n",
      "SMA_50_lag_3: 0.01%\n",
      "SMA_50_lag_25: 0.01%\n",
      "SMA_5_lag_180: 0.01%\n",
      "Volume_lag_90: 0.01%\n",
      "EMA_12_MACD: 0.01%\n",
      "RSI: 0.01%\n",
      "%K: 0.01%\n",
      "Close_lag_7: 0.01%\n",
      "EMA_20_lag_60: 0.01%\n",
      "EMA_26_MACD_lag_40: 0.01%\n",
      "SMA_5_lag_5: 0.01%\n",
      "SMA_20_lag_60: 0.01%\n",
      "30_day_Fib_38: 0.01%\n",
      "Volume_lag_20: 0.01%\n",
      "30_day_Fib_50: 0.01%\n",
      "SMA_50_lag_20: 0.01%\n",
      "%D: 0.01%\n",
      "SMA_20_lag_3: 0.01%\n",
      "SMA_20_lag_40: 0.01%\n",
      "EMA_12_MACD_lag_60: 0.01%\n",
      "EMA_26_MACD_lag_25: 0.01%\n",
      "5_day-Fib_50: 0.01%\n",
      "EMA_26_MACD_lag_75: 0.01%\n",
      "EMA_5_lag_30: 0.01%\n",
      "EMA_20_lag_5: 0.01%\n",
      "EMA_5_lag_360: 0.01%\n",
      "SMA_50_lag_15: 0.01%\n",
      "SMA_50_lag_7: 0.01%\n",
      "EMA_26_MACD: 0.01%\n",
      "Close_lag_3: 0.01%\n",
      "EMA_20_lag_30: 0.01%\n",
      "EMA_26_MACD_lag_30: 0.01%\n",
      "EMA_20_lag_75: 0.01%\n",
      "SMA_50_lag_10: 0.01%\n",
      "EMA_50_lag_7: 0.01%\n",
      "Volume_lag_15: 0.01%\n",
      "EMA_5_lag_90: 0.01%\n",
      "SMA_20_lag_10: 0.01%\n",
      "EMA_5_lag_15: 0.01%\n",
      "Volume_lag_50: 0.01%\n",
      "SMA_5_lag_60: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "EMA_50_lag_3: 0.00%\n",
      "10_day_Fib_23: 0.00%\n",
      "EMA_5_lag_75: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "EMA_20_lag_25: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "SMA_5_lag_7: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "EMA_20_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "EMA_20_lag_20: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "ATR_True_Range: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "Middle_Band: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "5_day-Fib_100: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_1_year_md_3.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6640fe3d-ee8a-49f0-9756-7985f42de8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/4262343893.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-240)\n",
      "/var/folders/wv/49phbq0x3bj2l3xqln6zr3x40000gn/T/ipykernel_90942/4262343893.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (200032, 215)\n",
      "Testing data shape: (282929, 215)\n",
      "X_train shape: (200032, 211), y_train shape: (200032,)\n",
      "X_test shape: (282929, 211), y_test shape: (282929,)\n",
      "Mean Squared Error on unseen data (post-February 10, 2024): 47990.785699917986\n"
     ]
    }
   ],
   "source": [
    "# 1 year prediction\n",
    "# learning_rate = 0.1\n",
    "# max_depth = 7\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort values by 'Symbol' and 'Date' to maintain time order\n",
    "df_stock_data_1_year = df_stock_data_1_year.sort_values(by=['Symbol', 'Date'])\n",
    "\n",
    "# Filter data to only include rows with Date before or equal to November 10, 2023 for training\n",
    "df_stock_data_train = df_stock_data_1_year[df_stock_data_1_year['Date'] <= '2023-07-10']\n",
    "\n",
    "# Filter data to only include rows with Date after February 10, 2024 for testing\n",
    "df_stock_data_test = df_stock_data_1_year[df_stock_data_1_year['Date'] > '2023-07-10']\n",
    "\n",
    "# Check if the test set is empty\n",
    "if df_stock_data_test.empty:\n",
    "    raise ValueError(\"No data available in the testing set for the given date range.\")\n",
    "\n",
    "# Shift 'Close' to predict 20 trading days ahead (1 month ahead)\n",
    "df_stock_data_train['Close_Target'] = df_stock_data_train.groupby('Symbol')['Close'].shift(-240)\n",
    "df_stock_data_test['Close_Target'] = df_stock_data_test.groupby('Symbol')['Close'].shift(-240)\n",
    "\n",
    "# Drop rows where 'Close_Target' is NaN (caused by shifting)\n",
    "df_stock_data_train = df_stock_data_train.dropna(subset=['Close_Target'])\n",
    "df_stock_data_test = df_stock_data_test.dropna(subset=['Close_Target'])\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df_stock_data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_stock_data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median (or mean) of each numeric column\n",
    "numeric_cols_train = df_stock_data_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_test = df_stock_data_test.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "df_stock_data_train[numeric_cols_train] = df_stock_data_train[numeric_cols_train].fillna(df_stock_data_train[numeric_cols_train].median())\n",
    "df_stock_data_test[numeric_cols_test] = df_stock_data_test[numeric_cols_test].fillna(df_stock_data_test[numeric_cols_test].median())\n",
    "\n",
    "# Check for the shapes of the data\n",
    "print(f\"Training data shape: {df_stock_data_train.shape}\")\n",
    "print(f\"Testing data shape: {df_stock_data_test.shape}\")\n",
    "\n",
    "# Create X (features) and y (target) for training\n",
    "X_train = df_stock_data_train.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_train = df_stock_data_train['Close_Target']\n",
    "\n",
    "# Create X and y for testing\n",
    "X_test = df_stock_data_test.drop(columns=['Close', 'Close_Target', 'Symbol', 'Date'])\n",
    "y_test = df_stock_data_test['Close_Target']\n",
    "\n",
    "# Check the shapes of the training and testing data\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Ensure there are samples in both training and testing sets\n",
    "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Insufficient data in either training or testing set.\")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "model_1_year_md_7 = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    alpha=0,\n",
    "    reg_lambda=1,  # Fixed parameter name\n",
    "    objective='reg:squarederror',\n",
    "    missing=np.nan  # Ensure missing values are handled correctly\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1_year_md_7.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the unseen test data (post-February 10, 2024)\n",
    "y_pred = model_1_year_md_7.predict(X_test)\n",
    "\n",
    "# Calculate performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on unseen data (post-February 10, 2024): {mse_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9dfbb1e9-9a6f-4e87-aba2-2443b0194930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on unseen data: 47990.785699917986\n",
      "Mean Absolute Error on unseen data: 53.95243372693579\n",
      "Root Mean Squared Error on unseen data: 219.06799332608583\n",
      "R-squared on unseen data: 0.8025320998127615\n",
      "Median Absolute Error on unseen data: 13.919929504394531\n",
      "Durbin-Watson Statistic on unseen data: 0.013846786474875502\n",
      "MAPE on unseen data: 29.73%\n",
      "Fib_30_High_Max: 75.72%\n",
      "SMA_50: 5.40%\n",
      "Volume: 4.28%\n",
      "Fib_5_High_Max: 3.91%\n",
      "VWAP: 3.11%\n",
      "EMA_50: 2.22%\n",
      "Fib_30_Low_Min: 0.69%\n",
      "Cumulative_Price_Volume: 0.60%\n",
      "EMA_5: 0.44%\n",
      "Fib_5_Low_Min: 0.40%\n",
      "Low: 0.38%\n",
      "EMA_26_MACD_lag_1: 0.32%\n",
      "Fib_10_Low_Min: 0.24%\n",
      "30_day_Fib_61: 0.21%\n",
      "EMA_50_lag_360: 0.15%\n",
      "Volume_lag_1: 0.10%\n",
      "EMA_50_lag_60: 0.10%\n",
      "EMA_20: 0.09%\n",
      "SMA_50_lag_360: 0.07%\n",
      "EMA_12_MACD_lag_30: 0.06%\n",
      "Lower_Band: 0.05%\n",
      "SMA_5_lag_7: 0.04%\n",
      "Volume_lag_3: 0.04%\n",
      "EMA_5_lag_360: 0.04%\n",
      "EMA_5_lag_180: 0.04%\n",
      "EMA_20_lag_360: 0.03%\n",
      "EMA_50_lag_90: 0.03%\n",
      "ATR: 0.03%\n",
      "Volume_lag_50: 0.03%\n",
      "Volume_lag_25: 0.03%\n",
      "EMA_26_MACD_lag_180: 0.03%\n",
      "High: 0.03%\n",
      "EMA_20_lag_180: 0.03%\n",
      "30_day_Fib_38: 0.02%\n",
      "Upper_Band: 0.02%\n",
      "30_day_Fib_23: 0.02%\n",
      "SMA_50_lag_75: 0.02%\n",
      "EMA_50_lag_1: 0.02%\n",
      "EMA_50_lag_180: 0.02%\n",
      "SMA_50_lag_40: 0.02%\n",
      "EMA_50_lag_40: 0.02%\n",
      "EMA_26_MACD_lag_360: 0.02%\n",
      "SMA_5_lag_360: 0.02%\n",
      "SMA_50_lag_60: 0.02%\n",
      "EMA_26_MACD_lag_60: 0.02%\n",
      "EMA_12_MACD_lag_360: 0.02%\n",
      "Close_lag_360: 0.02%\n",
      "SMA_50_lag_15: 0.02%\n",
      "Fib_10_High_Max: 0.02%\n",
      "SMA_50_lag_180: 0.02%\n",
      "Volume_lag_20: 0.01%\n",
      "EMA_26_MACD: 0.01%\n",
      "EMA_12_MACD_lag_180: 0.01%\n",
      "SMA_50_lag_20: 0.01%\n",
      "SMA_50_lag_50: 0.01%\n",
      "EMA_50_lag_50: 0.01%\n",
      "EMA_50_lag_30: 0.01%\n",
      "EMA_12_MACD_lag_90: 0.01%\n",
      "Middle_Band: 0.01%\n",
      "EMA_26_MACD_lag_90: 0.01%\n",
      "SMA_50_lag_30: 0.01%\n",
      "EMA_50_lag_75: 0.01%\n",
      "30_day_Fib_50: 0.01%\n",
      "SMA_50_lag_90: 0.01%\n",
      "SMA_5_lag_180: 0.01%\n",
      "EMA_20_lag_3: 0.01%\n",
      "EMA_12_MACD_lag_75: 0.01%\n",
      "Close_lag_25: 0.01%\n",
      "MACD: 0.01%\n",
      "EMA_12_MACD: 0.01%\n",
      "SMA_20_lag_360: 0.01%\n",
      "EMA_50_lag_25: 0.01%\n",
      "EMA_12_MACD_lag_60: 0.01%\n",
      "Cumulative_Volume: 0.01%\n",
      "EMA_50_lag_10: 0.01%\n",
      "SMA_50_lag_25: 0.01%\n",
      "EMA_12_MACD_lag_20: 0.01%\n",
      "EMA_5_lag_60: 0.01%\n",
      "EMA_50_lag_5: 0.01%\n",
      "Volume_lag_5: 0.01%\n",
      "Volume_lag_30: 0.01%\n",
      "5_day-Fib_61: 0.01%\n",
      "SMA_20_lag_40: 0.01%\n",
      "Volume_lag_40: 0.01%\n",
      "EMA_26_MACD_lag_75: 0.01%\n",
      "SMA_20_lag_180: 0.01%\n",
      "SMA_20_lag_60: 0.01%\n",
      "EMA_50_lag_15: 0.01%\n",
      "EMA_20_lag_40: 0.01%\n",
      "EMA_26_MACD_lag_7: 0.01%\n",
      "EMA_20_lag_60: 0.01%\n",
      "EMA_5_lag_75: 0.01%\n",
      "EMA_5_lag_50: 0.01%\n",
      "SMA_20_lag_75: 0.01%\n",
      "Volume_lag_180: 0.01%\n",
      "EMA_5_lag_90: 0.01%\n",
      "SMA_5_lag_90: 0.01%\n",
      "SMA_20_lag_30: 0.01%\n",
      "Close_lag_75: 0.01%\n",
      "EMA_26_MACD_lag_50: 0.01%\n",
      "Signal_Line: 0.01%\n",
      "EMA_20_lag_75: 0.01%\n",
      "EMA_26_MACD_lag_15: 0.01%\n",
      "10_day_Fib_23: 0.00%\n",
      "EMA_20_lag_30: 0.00%\n",
      "EMA_50_lag_20: 0.00%\n",
      "SMA_20_lag_1: 0.00%\n",
      "SMA_20: 0.00%\n",
      "SMA_20_lag_5: 0.00%\n",
      "SMA_5_lag_75: 0.00%\n",
      "Volume_lag_75: 0.00%\n",
      "EMA_26_MACD_lag_3: 0.00%\n",
      "EMA_26_MACD_lag_40: 0.00%\n",
      "SMA_20_lag_20: 0.00%\n",
      "Volume_lag_360: 0.00%\n",
      "EMA_20_lag_90: 0.00%\n",
      "EMA_26_MACD_lag_30: 0.00%\n",
      "10_day_Fib_61: 0.00%\n",
      "SMA_20_lag_50: 0.00%\n",
      "SMA_20_lag_15: 0.00%\n",
      "EMA_20_lag_10: 0.00%\n",
      "SMA_50_lag_5: 0.00%\n",
      "SMA_20_lag_25: 0.00%\n",
      "SMA_50_lag_7: 0.00%\n",
      "Volume_lag_90: 0.00%\n",
      "%D: 0.00%\n",
      "Close_lag_40: 0.00%\n",
      "Close_lag_90: 0.00%\n",
      "EMA_20_lag_15: 0.00%\n",
      "SMA_50_lag_10: 0.00%\n",
      "SMA_5_lag_60: 0.00%\n",
      "SMA_50_lag_1: 0.00%\n",
      "EMA_50_lag_3: 0.00%\n",
      "Close_lag_180: 0.00%\n",
      "SMA_20_lag_90: 0.00%\n",
      "Volume_lag_15: 0.00%\n",
      "SMA_20_lag_3: 0.00%\n",
      "EMA_20_lag_7: 0.00%\n",
      "EMA_5_lag_40: 0.00%\n",
      "EMA_12_MACD_lag_50: 0.00%\n",
      "SMA_5_lag_40: 0.00%\n",
      "Std_Dev: 0.00%\n",
      "Volume_lag_60: 0.00%\n",
      "EMA_20_lag_50: 0.00%\n",
      "EMA_12_MACD_lag_40: 0.00%\n",
      "Volume_lag_10: 0.00%\n",
      "EMA_20_lag_20: 0.00%\n",
      "SMA_50_lag_3: 0.00%\n",
      "Volume_lag_7: 0.00%\n",
      "EMA_5_lag_30: 0.00%\n",
      "EMA_12_MACD_lag_25: 0.00%\n",
      "SMA_20_lag_10: 0.00%\n",
      "SMA_5_lag_20: 0.00%\n",
      "EMA_20_lag_1: 0.00%\n",
      "SMA_5_lag_30: 0.00%\n",
      "EMA_12_MACD_lag_15: 0.00%\n",
      "SMA_20_lag_7: 0.00%\n",
      "EMA_5_lag_5: 0.00%\n",
      "%K: 0.00%\n",
      "Close_lag_50: 0.00%\n",
      "Close_lag_60: 0.00%\n",
      "EMA_12_MACD_lag_1: 0.00%\n",
      "SMA_5_lag_15: 0.00%\n",
      "EMA_50_lag_7: 0.00%\n",
      "MACD_Histogram: 0.00%\n",
      "EMA_26_MACD_lag_25: 0.00%\n",
      "EMA_20_lag_25: 0.00%\n",
      "EMA_5_lag_10: 0.00%\n",
      "SMA_5_lag_10: 0.00%\n",
      "Close_lag_30: 0.00%\n",
      "EMA_5_lag_15: 0.00%\n",
      "5_day-Fib_38: 0.00%\n",
      "SMA_5_lag_50: 0.00%\n",
      "SMA_5: 0.00%\n",
      "SMA_5_lag_25: 0.00%\n",
      "EMA_12_MACD_lag_7: 0.00%\n",
      "EMA_5_lag_1: 0.00%\n",
      "EMA_5_lag_25: 0.00%\n",
      "EMA_26_MACD_lag_20: 0.00%\n",
      "EMA_26_MACD_lag_10: 0.00%\n",
      "SMA_5_lag_5: 0.00%\n",
      "ATR_Prev_Close: 0.00%\n",
      "Close_lag_20: 0.00%\n",
      "Close_lag_15: 0.00%\n",
      "5_day-Fib_23: 0.00%\n",
      "Close_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_5: 0.00%\n",
      "EMA_5_lag_20: 0.00%\n",
      "5_day-Fib_50: 0.00%\n",
      "EMA_12_MACD_lag_10: 0.00%\n",
      "EMA_5_lag_7: 0.00%\n",
      "RSI: 0.00%\n",
      "SMA_5_lag_1: 0.00%\n",
      "10_day_Fib_50: 0.00%\n",
      "Close_lag_10: 0.00%\n",
      "10_day_Fib_38: 0.00%\n",
      "EMA_20_lag_5: 0.00%\n",
      "ATR_High_Low: 0.00%\n",
      "Close_lag_7: 0.00%\n",
      "EMA_26_MACD_lag_5: 0.00%\n",
      "EMA_12_MACD_lag_3: 0.00%\n",
      "EMA_5_lag_3: 0.00%\n",
      "SMA_5_lag_3: 0.00%\n",
      "Close_lag_3: 0.00%\n",
      "Close_lag_1: 0.00%\n",
      "ATR_True_Range: 0.00%\n",
      "ATR_High_Close: 0.00%\n",
      "ATR_Low_Close: 0.00%\n",
      "30_day_Fib_100: 0.00%\n",
      "10_day_Fib_100: 0.00%\n",
      "5_day-Fib_100: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# metrics and feature importance on unseen data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `y_pred` are your predictions for the test data and `y_test` are the true values for the test data\n",
    "\n",
    "# Calculate performance metrics on unseen test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print out the metrics for unseen data\n",
    "print(f'Mean Squared Error on unseen data: {mse}')\n",
    "print(f'Mean Absolute Error on unseen data: {mae}')\n",
    "print(f'Root Mean Squared Error on unseen data: {rmse}')\n",
    "print(f'R-squared on unseen data: {r2}')\n",
    "\n",
    "# Additional metrics\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f'Median Absolute Error on unseen data: {medae}')\n",
    "\n",
    "dw_stat = durbin_watson(y_test - y_pred)\n",
    "print(f'Durbin-Watson Statistic on unseen data: {dw_stat}')\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error) on unseen data\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE on unseen data: {mape:.2f}%')\n",
    "# Get feature importance as a dictionary\n",
    "feature_importance = dict(zip(X_train.columns, model_1_year_md_7.feature_importances_))\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature importance values\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb81866-a363-40d5-91ad-49e4cbabe51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
